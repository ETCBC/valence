{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/etcbc.png\"/>\n",
    "\n",
    "# Corrections and enrichment\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://github.com/ETCBC/valence/blob/master/notebooks/flowchart.ipynb)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "We also need to enrich constituents surrounding the \n",
    "verb occurrences with higher level features, that can be used\n",
    "as input for the flow chart decisions.\n",
    "\n",
    "Read more in the [wiki](https://github.com/ETCBC/valence/wiki/Workflows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "See [operation](https://github.com/ETCBC/pipeline/blob/master/README.md#operation) \n",
    "for how to run this script in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook processes excel sheets with manual corrections and enrichments.\n",
    "These have been entered against the `4b` version.\n",
    "However, the `4b` version in this repository has been regenerated from scratch,\n",
    "and in that process the node numbers have changed.\n",
    "As the sheets rely on node numbers to let the entered data flow back to the right nodes,\n",
    "these sheets no longer work on this version.\n",
    "It should be possible to identify the meterial in those sheets on the basis of\n",
    "book, chapter and verse info.\n",
    "But we leave that as an exercise to posterity.\n",
    "\n",
    "\n",
    "For all other versions, we keep the mechanism in place, but for now we work with zero manual input\n",
    "for those versions.\n",
    "\n",
    "As far as *corrections* are concerned: we expect to see them turn up in the continuous version `c`\n",
    "of the core [BHSA](https://github.com/ETCBC/bhsa) data.\n",
    "\n",
    "As far as *enrichments* are concerned: there are very few manual enrichments.\n",
    "Most of the cases are handled by the algorithm in the notebook.\n",
    "\n",
    "We recommend to harvest exceptions in the notebook itself, it has already a mechanism to apply\n",
    "verb specific logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'SCRIPT' not in locals():\n",
    "    SCRIPT = False\n",
    "    FORCE = True\n",
    "    CORE_NAME = 'bhsa'\n",
    "    NAME = 'valence'\n",
    "    VERSION= 'c'\n",
    "\n",
    "def stop(good=False):\n",
    "    if SCRIPT: sys.exit(0 if good else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Janet Dyk and Dirk Roorda](https://github.com/ETCBC/valence/wiki/Authors)\n",
    "\n",
    "Last modified 2017-09-13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[References](https://github.com/ETCBC/valence/wiki/References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We have carried out the valence project against the Hebrew Text Database of the ETCBC, version 4b.\n",
    "See the description of the [sources](https://github.com/ETCBC/valence/wiki/Sources).\n",
    "\n",
    "However, we can run our stuff also against the newer versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Start the engines. We use the Python package \n",
    "[text-fabric](https://github.com/Dans-labs/text-fabric)\n",
    "to process the data of the Hebrew Text Database smoothly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, collections\n",
    "from copy import deepcopy\n",
    "import utils\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the context: source file and target directories\n",
    "\n",
    "The conversion is executed in an environment of directories, so that sources, temp files and\n",
    "results are in convenient places and do not have to be shifted around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repoBase = os.path.expanduser('~/github/etcbc')\n",
    "coreRepo = '{}/{}'.format(repoBase, CORE_NAME)\n",
    "thisRepo = '{}/{}'.format(repoBase, NAME)\n",
    "\n",
    "coreTf = '{}/tf/{}'.format(coreRepo, VERSION)\n",
    "\n",
    "thisSource = '{}/source/{}'.format(thisRepo, VERSION)\n",
    "thisTemp = '{}/_temp/{}'.format(thisRepo, VERSION)\n",
    "thisTempTf = '{}/tf'.format(thisTemp)\n",
    "\n",
    "thisTf = '{}/tf/{}'.format(thisRepo, VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Check whether this conversion is needed in the first place.\n",
    "Only when run as a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    (good, work) = utils.mustRun(None, '{}/.tf/{}.tfx'.format(thisTf, 'valence'), force=FORCE)\n",
    "    print(good, work)\n",
    "    if not good: stop(good=False)\n",
    "    if not work: stop(good=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the feature data\n",
    "\n",
    "We load the features we need from the BHSA core database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".       0.00s Load the existing TF dataset                                                   .\n",
      "..............................................................................................\n",
      "This is Text-Fabric 3.0.2\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "107 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Load the existing TF dataset')\n",
    "TF = Fabric(locations=coreTf, modules=[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.20s B lex_utf8             from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.15s B lex                  from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B gloss                from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.16s B sp                   from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.13s B vs                   from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.13s B uvf                  from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.14s B prs                  from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B nametype             from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.14s B ls                   from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.08s B function             from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.26s B rela                 from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.25s B typ                  from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.25s B mother               from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 102 for nodes; 4 for edges; 1 configs; 7 computed\n",
      "  7.12s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    lex gloss lex_utf8\n",
    "    sp vs lex uvf prs nametype ls\n",
    "    function rela typ\n",
    "    mother\n",
    "''')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linkShebanq = 'https://shebanq.ancient-data.org/hebrew/text'\n",
    "linkPassage = '?book={}&chapter={}&verse={}'\n",
    "linkAppearance = '&version={}&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x'.format(VERSION)\n",
    "\n",
    "resultDir = '{}/results'.format(thisTemp)\n",
    "allResults = '{}/all.csv'.format(resultDir)\n",
    "selectedResults = '{}/selected.csv'.format(resultDir)\n",
    "kinds = ('corr_blank', 'corr_filled', 'enrich_blank', 'enrich_filled')\n",
    "kdir = {}\n",
    "for k in kinds:\n",
    "    kd = '{}/{}'.format(thisSource, k)\n",
    "    kdir[k] = kd\n",
    "    if not os.path.exists(kd):\n",
    "        os.makedirs(kd)\n",
    "if not os.path.exists(resultDir):\n",
    "    os.makedirs(resultDir)\n",
    "\n",
    "def vfile(verb, kind):\n",
    "    if kind not in kinds:\n",
    "        utils.caption(0, 'ERROR: Unknown kind `{}`'.format(kind))\n",
    "        return None\n",
    "    baseName = verb.replace('>','a').replace('<', 'o')\n",
    "    return (baseName, '{}/{}.csv'.format(kdir[kind], baseName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain\n",
    "Here is a subset of verbs that interest us.\n",
    "In fact, we are interested in all verbs, but we have subjected the occurrences of these verbs to closer inspection, \n",
    "together with the contexts they occur in.\n",
    "\n",
    "Manual additions in the correction and enrichment workflow can only happen for selected verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs_initial = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())\n",
    "\n",
    "motion_verbs = set('''\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "'''.strip().split())\n",
    "\n",
    "double_object_verbs = set('''\n",
    "    NTN\n",
    "    <FH\n",
    "    FJM\n",
    "'''.strip().split())\n",
    "\n",
    "complex_qal_verbs = set('''\n",
    "    NF>\n",
    "    PQD\n",
    "'''.strip().split())\n",
    "\n",
    "verbs = verbs_initial | motion_verbs | double_object_verbs | complex_qal_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Correction workflow\n",
    "\n",
    "## 1.1 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicate_functions = {\n",
    "    'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_values = dict(\n",
    "    function={F.function.v(p) for p in F.otype.s('phrase')},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_values = dict(\n",
    "    function=dict(\n",
    "        BoundErr='this constituent is part of another constituent and does not merit its own function/type/rela value',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the error_values to the legal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       7.42s {'function': {'Voct', 'Cmpl', 'Time', 'Exst', 'Pred', 'Loca', 'PreC', 'NCoS', 'Rela', 'Ques', 'Frnt', 'PrcS', 'PrAd', 'BoundErr', 'PreS', 'PtcO', 'Conj', 'ModS', 'Adju', 'PreO', 'Intj', 'EPPr', 'Modi', 'Nega', 'Objc', 'Subj', 'IntS', 'ExsS', 'Supp', 'NCop'}}\n"
     ]
    }
   ],
   "source": [
    "for feature in set(legal_values.keys()) | set(error_values.keys()):\n",
    "    ev = error_values.get(feature, {})\n",
    "    if ev:\n",
    "        lv = legal_values.setdefault(feature, set())\n",
    "        lv |= set(ev.keys())\n",
    "if not SCRIPT:\n",
    "    utils.caption(0, '{}'.format(legal_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".       7.53s Finding occurrences ...                                                        .\n",
      "..............................................................................................\n",
      "|       9.84s \tDone\n",
      "|       9.84s \tAll:      1380 verbs with  73710 verb occurrences in 70159 clauses\n",
      "|       9.84s \tSelected:   18 verbs with  16209 verb occurrences in 16052 clauses\n",
      "|       9.84s \t<BR   556 occurrences of which   33 outside a predicate phrase\n",
      "|       9.84s \t<FH  2629 occurrences of which   59 outside a predicate phrase\n",
      "|       9.84s \t<LH   890 occurrences of which   10 outside a predicate phrase\n",
      "|       9.85s \tBR>    54 occurrences of which    3 outside a predicate phrase\n",
      "|       9.85s \tBW>  2570 occurrences of which   27 outside a predicate phrase\n",
      "|       9.85s \tCJT    85 occurrences of which    1 outside a predicate phrase\n",
      "|       9.85s \tCWB  1056 occurrences of which   22 outside a predicate phrase\n",
      "|       9.85s \tFJM   609 occurrences of which    3 outside a predicate phrase\n",
      "|       9.85s \tHLK  1554 occurrences of which   31 outside a predicate phrase\n",
      "|       9.85s \tJRD   377 occurrences of which   16 outside a predicate phrase\n",
      "|       9.85s \tJY>  1069 occurrences of which   32 outside a predicate phrase\n",
      "|       9.85s \tNF>   656 occurrences of which   53 outside a predicate phrase\n",
      "|       9.85s \tNPL   445 occurrences of which   11 outside a predicate phrase\n",
      "|       9.85s \tNTN  2017 occurrences of which   10 outside a predicate phrase\n",
      "|       9.85s \tNWS   159 occurrences of which    4 outside a predicate phrase\n",
      "|       9.85s \tPQD   303 occurrences of which   72 outside a predicate phrase\n",
      "|       9.85s \tQR>   883 occurrences of which   12 outside a predicate phrase\n",
      "|       9.85s \tSWR   297 occurrences of which    1 outside a predicate phrase\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Finding occurrences ...')\n",
    "occs = collections.defaultdict(list)   # dictionary of verb occurrence nodes per verb lexeme\n",
    "npoccs = collections.defaultdict(list) # same, but those not occurring in a \"predicate\"\n",
    "clause_verb = collections.defaultdict(list)    # dictionary of verb occurrence nodes per clause node\n",
    "sel_clause_verb = collections.defaultdict(list)    # dictionary of selected verb occurrence nodes per clause node\n",
    "clause_verb_index = collections.defaultdict(set) # mapping from clauses to its main verb(s)\n",
    "sel_clause_verb_index = collections.defaultdict(set) # mapping from clauses to its main verb(s), for selected verbs\n",
    "verb_clause_index = collections.defaultdict(list) # mapping from verbs to the clauses of which it is main verb\n",
    "sel_verb_clause_index = collections.defaultdict(list) # mapping from selected verbs to the clauses of which it is main verb\n",
    "\n",
    "nw = 0\n",
    "sel_nw = 0\n",
    "for w in F.otype.s('word'):\n",
    "    if F.sp.v(w) != 'verb': continue\n",
    "    lex = F.lex.v(w).rstrip('[=')\n",
    "    nw += 1\n",
    "    pf = F.function.v(L.u(w, 'phrase')[0])\n",
    "    if pf not in predicate_functions:\n",
    "        npoccs[lex].append(w)\n",
    "    occs[lex].append(w)\n",
    "    cn = L.u(w, 'clause')[0]\n",
    "    clause_verb[cn].append(w)\n",
    "    clause_verb_index[cn].add(lex)\n",
    "    verb_clause_index[lex].append(cn)\n",
    "    if lex in verbs:\n",
    "        sel_nw += 1\n",
    "        sel_clause_verb[cn].append(w)\n",
    "        sel_clause_verb_index[cn].add(lex)\n",
    "\n",
    "sel_verb_clause_index = dict((lex, cns) for (lex, cns) in verb_clause_index.items() if lex in verbs)\n",
    "sel_clause_verb\n",
    "\n",
    "utils.caption(0, '\\tDone')\n",
    "utils.caption(0, '\\tAll:      {:>4} verbs with {:>6} verb occurrences in {} clauses'.format(\n",
    "    len(verb_clause_index), nw, len(clause_verb)))\n",
    "utils.caption(0, '\\tSelected: {:>4} verbs with {:>6} verb occurrences in {} clauses'.format(\n",
    "    len(sel_verb_clause_index), sel_nw, len(sel_clause_verb)))\n",
    "\n",
    "for verb in sorted(verbs):\n",
    "    utils.caption(0, '\\t{} {:>5} occurrences of which {:>4} outside a predicate phrase'.format(\n",
    "        verb, \n",
    "        len(occs[verb]),\n",
    "        len(npoccs[verb]),\n",
    "        continuation=True,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         10s Generating blank correction sheets ...                                         .\n",
      "..............................................................................................\n",
      "|         10s \tas /Users/dirk/github/etcbc/valence/source/c/corr_blank/{verb}.csv\n",
      "|         10s \t\tfor verb NFa\n",
      "|         10s \t\tfor verb QRa\n",
      "|         11s \t\tfor verb NPL\n",
      "|         11s \t\tfor verb BWa\n",
      "|         11s \t\tfor verb CJT\n",
      "|         11s \t\tfor verb NWS\n",
      "|         11s \t\tfor verb NTN\n",
      "|         12s \t\tfor verb CWB\n",
      "|         12s \t\tfor verb PQD\n",
      "|         12s \t\tfor verb JYa\n",
      "|         12s \t\tfor verb BRa\n",
      "|         12s \t\tfor verb SWR\n",
      "|         12s \t\tfor verb HLK\n",
      "|         12s \t\tfor verb oBR\n",
      "|         13s \t\tfor verb oLH\n",
      "|         13s \t\tfor verb JRD\n",
      "|         13s \t\tfor verb oFH\n",
      "|         13s \t\tfor verb FJM\n",
      "|         13s \t52109  phrases seen 1  time(s)\n",
      "|         13s \t185    phrases seen 2  time(s)\n",
      "|         13s \t9      phrases seen 3  time(s)\n",
      "|         13s \tTotal phrases seen: 52303\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Generating blank correction sheets ...')\n",
    "sheetKind = 'corr_blank'\n",
    "utils.caption(0, '\\tas {}'.format(vfile('{verb}', sheetKind)[1]))\n",
    "\n",
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u(wn, 'clause')[0]\n",
    "        if cln in clauses_seen: continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u(wn, 'verse')[0]\n",
    "        bn = L.u(wn, 'book')[0]\n",
    "        (bookName, ch, vs) = T.sectionFromNode(vn, lang='la')\n",
    "        passage_label = '{} {}:{}'.format(*T.sectionFromNode(vn))\n",
    "        ln = linkShebanq+(linkPassage.format(bookName, ch, vs))+linkAppearance\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.text([wn], fmt='text-trans-plain')\n",
    "        vstem = F.vs.v(wn)\n",
    "        np = '* ' if wn in npoccs[verb] else ''\n",
    "        row = [cln, wn, passage_label, lnx, np+vt, vstem]\n",
    "        phrases = L.d(cln, 'phrase')\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            phrases_seen[pn] += 1\n",
    "            pt = T.text(L.d(pn, 'word'), fmt='text-trans-plain')\n",
    "            pf = F.function.v(pn)\n",
    "            pnp = np if pf in predicate_functions else ''\n",
    "            row.extend((pn, pnp+pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    location = vfile(verb, sheetKind)\n",
    "    if location == None: return\n",
    "    (baseName, fileName) = location\n",
    "    row_file = open(fileName, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    utils.caption(0, '\\t\\tfor verb {}'.format(baseName))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)\n",
    "    \n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    utils.caption(0, '\\t{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "utils.caption(0, '\\tTotal phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 1.3 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         14s Processing filled correction sheets ...                                        .\n",
      "..............................................................................................\n",
      "|         14s \tas /Users/dirk/github/etcbc/valence/source/c/corr_filled/{verb}.csv\n",
      "|         14s \t\tNO file for oBR\n",
      "|         14s \t\tNO file for oFH\n",
      "|         14s \t\tNO file for oLH\n",
      "|         14s \t\tNO file for BRa\n",
      "|         14s \t\tNO file for BWa\n",
      "|         14s \t\tNO file for CJT\n",
      "|         14s \t\tNO file for CWB\n",
      "|         14s \t\tNO file for FJM\n",
      "|         14s \t\tNO file for HLK\n",
      "|         14s \t\tNO file for JRD\n",
      "|         14s \t\tNO file for JYa\n",
      "|         14s \t\tNO file for NFa\n",
      "|         14s \t\tNO file for NPL\n",
      "|         14s \t\tNO file for NTN\n",
      "|         14s \t\tNO file for NWS\n",
      "|         14s \t\tNO file for PQD\n",
      "|         14s \t\tNO file for QRa\n",
      "|         14s \t\tNO file for SWR\n",
      "|         14s \tFound 0 corrections in the phrase function\n",
      "|         14s \tTotal phrases seen: 0\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Processing filled correction sheets ...')\n",
    "sheetKind = 'corr_filled'\n",
    "utils.caption(0, '\\tas {}'.format(vfile('{verb}', sheetKind)[1]))\n",
    "\n",
    "phrases_seen = collections.Counter()\n",
    "pf_corr = {}\n",
    "\n",
    "def read_corr():\n",
    "    function_values = legal_values['function']\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        repeated = collections.defaultdict(list)\n",
    "        non_phrase = set()\n",
    "        illegal_fvalue = set()\n",
    "        nodeNumberErrors = []\n",
    "\n",
    "        location = vfile(verb, sheetKind)\n",
    "        if location == None: continue\n",
    "        (baseName, fileName) = location\n",
    "        if not os.path.exists(fileName):\n",
    "            utils.caption(0, '\\t\\tNO file for {}'.format(baseName))\n",
    "            continue\n",
    "        else:\n",
    "            utils.caption(0, '\\t\\tverb {}'.format(baseName))\n",
    "        with open(fileName) as f:\n",
    "            header = f.__next__()\n",
    "            for (i, line) in enumerate(f):\n",
    "                fields = line.rstrip().split(';')\n",
    "                cn = int(fields[0])\n",
    "                wn = int(fields[1])\n",
    "                if F.otype.v(cn) != 'clause':\n",
    "                    nodeNumberErrors.append([i, '{} is not a clause node'.format(cn)])\n",
    "                if F.otype.v(wn) != 'word':\n",
    "                    nodeNumberErrors.append([i, '{} is not a word node'.format(wn)])\n",
    "                words = set(L.d(cn, 'word'))\n",
    "                phrases = set(L.d(cn, 'phrase'))\n",
    "                if wn not in words:\n",
    "                    nodeNumberErrors.append([i, '{} is not a word of clause {}'.format(wn, cn)])\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[2+4*i], fields[2+4*i+3])\n",
    "                    if pn != '':\n",
    "                        pn = int(pn)\n",
    "                        if F.otype.v(pn) != 'phrase':\n",
    "                            nodeNumberErrors.append([i, '{} is not a phrase node'.format(pn)])\n",
    "                        if pn not in phrases:\n",
    "                            nodeNumberErrors.append([i, '{} is not a phrase of clause {}'.format(pn, cn)])\n",
    "                        pc = pc.strip()\n",
    "                        phrases_seen[pn] += 1\n",
    "                        if pc != '':\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        utils.caption(0, '\\t{}: Found {:>5} corrections in {}'.format(verb, len(pf_corr), fileName))\n",
    "        if len(nodeNumberErrors):\n",
    "            for (i, msg) in nodeNumberErrors:\n",
    "                utils.caption(0, 'ERROR: Line {:>3}: {}'.format(i+1, msg))\n",
    "        else:\n",
    "            utils.caption(0, '\\tOK: node numbers in sheet are consistent')\n",
    "        if len(repeated):\n",
    "            utils.caption(0, 'ERROR: Some phrases have been corrected multiple times!')\n",
    "            for x in sorted(repeated):\n",
    "                utils.caption(0, '\\t{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "        else:\n",
    "            utils.caption(0, '\\tOK: Corrected phrases did not receive multiple corrections')\n",
    "        if len(non_phrase):\n",
    "            utils.caption(0, 'ERROR: Corrections have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "        else:\n",
    "            utils.caption(0, '\\tOK: all corrected nodes where phrase nodes')\n",
    "        if len(illegal_fvalue):\n",
    "            utils.caption(0, 'ERROR: Some corrections supply illegal values for phrase function!')\n",
    "            utils.caption(0, '\\t`{}`'.format('`, `'.join(illegal_fvalue)))\n",
    "        else:\n",
    "            utils.caption(0, '\\tOK: all corrected values are legal')\n",
    "    utils.caption(0, '\\tFound {} corrections in the phrase function'.format(len(pf_corr)))\n",
    "        \n",
    "read_corr()\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    utils.caption(0, '\\t{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "utils.caption(0, '\\tTotal phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Enrichment workflow\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         14s 6 Enrich field specifications OK                                               .\n",
      "..............................................................................................\n",
      "|         14s \tgrammatical has possible values\n",
      "|         14s \t\t*\n",
      "|         14s \t\tL_object\n",
      "|         14s \t\tinfinitive_object\n",
      "|         14s \t\tNA\n",
      "|         14s \t\tsubject\n",
      "|         14s \t\tdirect_object\n",
      "|         14s \t\tindirect_object\n",
      "|         14s \t\tprincipal_direct_object\n",
      "|         14s \t\tNP_direct_object\n",
      "|         14s \t\tK_object\n",
      "|         14s \tlexical has possible values\n",
      "|         14s \t\tlocation\n",
      "|         14s \t\ttime\n",
      "|         14s \toriginal has possible values\n",
      "|         14s \t\t*\n",
      "|         14s \t\tL_object\n",
      "|         14s \t\tinfinitive_object\n",
      "|         14s \t\tNA\n",
      "|         14s \t\tsubject\n",
      "|         14s \t\tdirect_object\n",
      "|         14s \t\tindirect_object\n",
      "|         14s \t\tprincipal_direct_object\n",
      "|         14s \t\tNP_direct_object\n",
      "|         14s \t\tK_object\n",
      "|         14s \tpredication has possible values\n",
      "|         14s \t\tNA\n",
      "|         14s \t\tregular\n",
      "|         14s \t\tcopula\n",
      "|         14s \tsemantic has possible values\n",
      "|         14s \t\tlocation\n",
      "|         14s \t\tmanner\n",
      "|         14s \t\tinstrument\n",
      "|         14s \t\tbenefactive\n",
      "|         14s \t\ttime\n",
      "|         14s \tvalence has possible values\n",
      "|         14s \t\tcomplement\n",
      "|         14s \t\tcore\n",
      "|         14s \t\tadjunct\n"
     ]
    }
   ],
   "source": [
    "enrich_field_spec = '''\n",
    "valence\n",
    "    adjunct\n",
    "    complement\n",
    "    core\n",
    "\n",
    "predication\n",
    "    NA\n",
    "    regular\n",
    "    copula\n",
    "\n",
    "grammatical\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "original\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "lexical\n",
    "    location\n",
    "    time\n",
    "\n",
    "semantic\n",
    "    benefactive\n",
    "    time\n",
    "    location\n",
    "    instrument\n",
    "    manner\n",
    "'''\n",
    "enrich_fields = collections.OrderedDict()\n",
    "cur_e = None\n",
    "for line in enrich_field_spec.strip().split('\\n'):\n",
    "    if line.startswith(' '):\n",
    "        enrich_fields.setdefault(cur_e, set()).add(line.strip())\n",
    "    else:\n",
    "        cur_e = line.strip()\n",
    "nef = len(enrich_fields)\n",
    "if None in enrich_fields:\n",
    "    utils.caption(0, 'ERROR: Invalid enrich field specification')\n",
    "else:\n",
    "    utils.caption(4, '{} Enrich field specifications OK'.format(nef))\n",
    "for (ef, fields) in sorted(enrich_fields.items()):\n",
    "    utils.caption(0, '\\t{} has possible values'.format(ef))\n",
    "    for field in fields:\n",
    "        utils.caption(0, '\\t\\t{}'.format(field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enrich_baseline_rules = dict(\n",
    "    phrase='''Adju\tAdjunct\tadjunct\tNA\tNA\t\t\t\n",
    "Cmpl\tComplement\tcomplement\tNA\t*\t\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\t\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\tNA\t\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Exst\tExistence\tcore\tcopula\tNA\t\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\t\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\t\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tNA\t\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\t\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\tNA\t\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\t\tNA\tNA\n",
    "Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\tNA\t\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PreC\tPredicate complement\tcore\tregular\tNA\t\t\t\n",
    "Pred\tPredicate\tcore\tregular\tNA\t\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\t\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\t\tNA\tNA\n",
    "Subj\tSubject\tcore\tNA\tsubject\t\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\tNA\t\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\tNA\t\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\t\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\t\tNA\tNA''',\n",
    "    clause='''Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "InfC\tInfinitive Construct clause\tNA\tNA\t\t\t\t''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         14s \tChecking enrich baseline rules                                                .\n",
      "..............................................................................................\n",
      "|         14s \tEnrich baseline rules are OK (204 good)\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, '\\tChecking enrich baseline rules')\n",
    "transform = collections.OrderedDict((('phrase', {}), ('clause', {})))\n",
    "errors = 0\n",
    "good = 0\n",
    "\n",
    "for kind in ('phrase', 'clause'):\n",
    "    for line in enrich_baseline_rules[kind].split('\\n'):\n",
    "        x = line.split('\\t')\n",
    "        nefields = len(x) - 2\n",
    "        if len(x) - 2 != nef:\n",
    "            utils.caption(0, 'ERROR: Wrong number of fields ({} must be {}) in {}:\\n{}'.format(nefields, nef, kind, line))\n",
    "            errors += 1\n",
    "        transform[kind][x[0]] = dict(zip(enrich_fields, x[2:]))\n",
    "    for e in error_values['function']:\n",
    "        transform[kind][e] = dict(zip(enrich_fields, ['']*nef))\n",
    "\n",
    "    for f in transform[kind]:\n",
    "        for e in enrich_fields:\n",
    "            val = transform[kind][f][e]\n",
    "            if val != '' and val != 'NA' and val not in enrich_fields[e]:\n",
    "                utils.caption(0, 'ERROR: Defaults for `{}` ({}): wrong `{}` value: \"{}\"'.format(f, kind, e, val))\n",
    "                errors += 1\n",
    "            else: good += 1\n",
    "if errors:\n",
    "    utils.caption(0, 'ERROR: There were {} errors ({} good)'.format(errors, good))\n",
    "else:\n",
    "    utils.caption(0, '\\tEnrich baseline rules are OK ({} good)'.format(good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prettyprint the baseline rules of enrichment for easier reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func    : valence        predication    grammatical    original       lexical        semantic       \n",
      "[phrase]\n",
      "Adju    : adjunct        NA             NA                                                          \n",
      "BoundErr:                                                                                           \n",
      "Cmpl    : complement     NA             *                                                           \n",
      "Conj    : NA             NA             NA                            NA             NA             \n",
      "EPPr    : NA             copula         NA                                                          \n",
      "ExsS    : core           copula         subject                                                     \n",
      "Exst    : core           copula         NA                                                          \n",
      "Frnt    : NA             NA             NA                            NA             NA             \n",
      "IntS    : core           NA             subject                                                     \n",
      "Intj    : NA             NA             NA                            NA             NA             \n",
      "Loca    : adjunct        NA             NA                            location       location       \n",
      "ModS    : core           NA             subject                                                     \n",
      "Modi    : NA             NA             NA                            NA             NA             \n",
      "NCoS    : core           copula         subject                                                     \n",
      "NCop    : core           copula         NA                                                          \n",
      "Nega    : NA             NA             NA                            NA             NA             \n",
      "Objc    : complement     NA             direct_object                                               \n",
      "PrAd    : adjunct        NA             NA                                                          \n",
      "PrcS    : core           regular        subject                                                     \n",
      "PreC    : core           regular        NA                                                          \n",
      "PreO    : core           regular        direct_object                                               \n",
      "PreS    : core           regular        subject                                                     \n",
      "Pred    : core           regular        NA                                                          \n",
      "PtcO    : core           regular        direct_object                                               \n",
      "Ques    : NA             NA             NA                            NA             NA             \n",
      "Rela    : NA             NA             NA                            NA             NA             \n",
      "Subj    : core           NA             subject                                                     \n",
      "Supp    : adjunct        NA             NA                                           benefactive    \n",
      "Time    : adjunct        NA             NA                            time           time           \n",
      "Unkn    : NA             NA             NA                            NA             NA             \n",
      "Voct    : NA             NA             NA                            NA             NA             \n",
      "[clause]\n",
      "BoundErr:                                                                                           \n",
      "InfC    : NA             NA                                                                         \n",
      "Objc    : complement     NA             direct_object                                               \n"
     ]
    }
   ],
   "source": [
    "if not SCRIPT:\n",
    "    ltpl = '{:<8}: '+('{:<15}' * nef)\n",
    "    utils.caption(0, ltpl.format('func', *enrich_fields), continuation=True)\n",
    "    for kind in transform:\n",
    "        utils.caption(0, '[{}]'.format(kind), continuation=True)\n",
    "        for f in sorted(transform[kind]):\n",
    "            sfs = transform[kind][f]\n",
    "            utils.caption(0, ltpl.format(f, *[sfs[sf] for sf in enrich_fields]), continuation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Enrichment logic\n",
    "\n",
    "We apply enrichment logic to *all* verbs, not only to selected verbs.\n",
    "But only selected verbs can receive manual enrichment enhancements.\n",
    "\n",
    "For some verbs, selected or not, additional logic specific to that verb can be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Direct objects\n",
    "\n",
    "We have to do some work to identify (multiple) direct objects and indirect objects.\n",
    "\n",
    "[More on direct objects](https://github.com/ETCBC/valence/wiki/Discussion#direct-objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objectfuncs = set('''\n",
    "Objc PreO PtcO\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_obj_preps = set('''\n",
    "K L\n",
    "'''.strip().split())\n",
    "\n",
    "no_prs = set('''\n",
    "absent n/a\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_parts = set('''\n",
    ">NP/ >P/ >PSJM/ >YB</ >ZN/\n",
    "<JN/ <NQ/ <RP/ <YM/ <YM==/\n",
    "BHN/ BHWN/ BVN/\n",
    "CD=/ CD===/ CKM/ CN/\n",
    "DD/\n",
    "GRGRT/ GRM/ GRWN/ GW/ GW=/ GWJH/ GWPH/ GXWN/\n",
    "FPH/\n",
    "JD/ JRK/ JRKH/\n",
    "KRF/ KSL=/ KTP/\n",
    "L</ LCN/ LCWN/ LXJ/\n",
    "M<H/ MPRQT/ MTL<WT/ MTNJM/ MYX/\n",
    "NBLH=/\n",
    "P<M/ PGR/ PH/ PM/ PNH/ PT=/\n",
    "QRSL/\n",
    "R>C/ RGL/\n",
    "XDH/ XLY/ XMC=/ XRY/\n",
    "YW>R/\n",
    "ZRW</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         14s Finding direct objects and determining the principal one                       .\n",
      "..............................................................................................\n",
      "|         18s \tDone\n",
      "|         18s \t 3658 clauses with  1  principal object\n",
      "|         18s \t66501 clauses with  0  principal objects\n",
      "|         18s \t 3658 clauses with  a  principal object\n",
      "|         18s \t    8 clauses with  2     direct objects\n",
      "|         18s \t23689 clauses with  1     direct object\n",
      "|         18s \t46462 clauses with  0     direct objects\n",
      "|         18s \t23697 clauses with  a     direct object\n",
      "|         18s \t 1009 clauses with  1         NP object\n",
      "|         18s \t69150 clauses with  0         NP objects\n",
      "|         18s \t 1009 clauses with  a         NP object\n",
      "|         18s \t   33 clauses with  2          L objects\n",
      "|         18s \t 3827 clauses with  1          L object\n",
      "|         18s \t66299 clauses with  0          L objects\n",
      "|         18s \t 3860 clauses with  a          L object\n",
      "|         18s \t  115 clauses with  1          K object\n",
      "|         18s \t70044 clauses with  0          K objects\n",
      "|         18s \t  115 clauses with  a          K object\n",
      "|         18s \t    8 clauses with  2     clause objects\n",
      "|         18s \t 1294 clauses with  1     clause object\n",
      "|         18s \t68857 clauses with  0     clause objects\n",
      "|         18s \t 1302 clauses with  a     clause object\n",
      "|         18s \t    1 clauses with  3 infinitive objects\n",
      "|         18s \t   18 clauses with  2 infinitive objects\n",
      "|         18s \t 1192 clauses with  1 infinitive object\n",
      "|         18s \t68948 clauses with  0 infinitive objects\n",
      "|         18s \t 1211 clauses with  a infinitive object\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Finding direct objects and determining the principal one')\n",
    "clause_objects = collections.defaultdict(set)\n",
    "objects = collections.defaultdict(set)\n",
    "objects_count = collections.defaultdict(collections.Counter)\n",
    "object_kinds = (\n",
    "    'principal',\n",
    "    'direct',\n",
    "    'NP',\n",
    "    'L',\n",
    "    'K',\n",
    "    'clause',\n",
    "    'infinitive',\n",
    ")\n",
    "\n",
    "def is_marked(phr):\n",
    "    # simple criterion for determining whether a direct object is marked:\n",
    "    # has it the object marker somewhere?\n",
    "    words = L.d(p, 'word')\n",
    "    has_et = False\n",
    "    for w in words:\n",
    "        if F.lex.v(w) == '>T':\n",
    "            has_et = True\n",
    "            break\n",
    "    return has_et\n",
    "\n",
    "for c in clause_verb:\n",
    "    these_objects = collections.defaultdict(set)\n",
    "    direct_objects_cat = collections.defaultdict(set)\n",
    "\n",
    "    for p in L.d(c, 'phrase'):\n",
    "        pf = pf_corr.get(p, F.function.v(p))  # NB we take the corrected value for phrase function if there is one\n",
    "        if pf in objectfuncs:\n",
    "            direct_objects_cat['p_'+pf].add(p)\n",
    "            these_objects['direct'].add(p)\n",
    "        elif pf == 'Cmpl':\n",
    "            pwords = L.d(p, 'word')\n",
    "            w1 = pwords[0]\n",
    "            w1l = F.lex.v(w1)\n",
    "            w2l = F.lex.v(pwords[1]) if len(pwords) > 1 else None\n",
    "            if w1l in cmpl_as_obj_preps and F.prs.v(w1) in no_prs and not (w1l == 'L' and w2l in body_parts):\n",
    "                if w1l == 'K': these_objects['K'].add(p)\n",
    "                elif w1l == 'L': these_objects['L'].add(p)\n",
    "        \n",
    "    # find clause objects\n",
    "    for ac in L.d(L.u(c, 'sentence')[0], 'clause'):\n",
    "        mothers = list(E.mother.f(ac))\n",
    "        if not (mothers and mothers[0] == c): continue\n",
    "        cr = F.rela.v(ac)\n",
    "        ct = F.typ.v(ac)\n",
    "        if cr in {'Objc'} or ct in {'InfC'}:\n",
    "            clause_objects[c].add(ac)\n",
    "            if cr in {'Objc'}:\n",
    "                label = cr\n",
    "                direct_objects_cat['c_'+label].add(ac)\n",
    "                these_objects['direct'].add(ac)\n",
    "                these_objects['clause'].add(ac)\n",
    "            elif ct in {'InfC'}:\n",
    "                if F.lex.v(L.d(ac, 'word')[0]) == 'L':\n",
    "                    these_objects['infinitive'].add(ac)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # order the objects in the natural ordering\n",
    "    direct_objects_order = sortNodes(these_objects.get('direct', set()))\n",
    "    nobjects = len(direct_objects_order)\n",
    "\n",
    "    # compute the principal object\n",
    "    principal_object = None\n",
    "\n",
    "    for x in [1]:\n",
    "        # just one object \n",
    "        if nobjects == 1:\n",
    "            # we have chosen not to mark a principal object if there is only one object\n",
    "            # the alternative is to mark it if it is a phrase. Uncomment the next 2 lines if you want this\n",
    "            # theobject = list(dobjects_set)[0]\n",
    "            # if F.otype.v(theobject) == 'phrase': principal_object = theobject\n",
    "            break\n",
    "        # rule 1: suffixes and promoted objects\n",
    "        principal_candidates =\\\n",
    "            direct_objects_cat.get('p_PreO', set()) |\\\n",
    "            direct_objects_cat.get('p_PtcO', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            principal_object = sortNodes(principal_candidates)[0]\n",
    "            break\n",
    "        principal_candidates = direct_objects_cat.get('p_Objc', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            if len(principal_candidates) == 1:\n",
    "                principal_object = list(principal_candidates)[0]\n",
    "                break\n",
    "            objects_marked = set()\n",
    "            objects_unmarked = set()\n",
    "            for p in principal_candidates:\n",
    "                if is_marked(p):\n",
    "                    objects_marked.add(p)\n",
    "                else:\n",
    "                    objects_unmarked.add(p)\n",
    "            if len(objects_marked) != 0:\n",
    "                principal_object = sortNodes(objects_marked)[0]\n",
    "                break\n",
    "            if len(objects_unmarked) != 0:\n",
    "                principal_object = sortNodes(objects_unmarked)[0]\n",
    "                break            \n",
    "    if principal_object != None:\n",
    "        these_objects['principal'].add(principal_object)\n",
    "    if len(these_objects['infinitive']) and not len(these_objects['direct']):\n",
    "        # we do not mark an infinitive object if there is no proper direct object around\n",
    "        these_objects['infinitive'] = set()\n",
    "    if len(these_objects['principal']):\n",
    "        these_objects['direct'] -= these_objects['principal']\n",
    "        for x in these_objects['direct'] - these_objects['clause']:\n",
    "            # the NP objects are the non-principal phrase like direct objects\n",
    "            these_objects['NP'].add(x)\n",
    "        these_objects['direct'] -= these_objects['NP']\n",
    "    if len(these_objects['principal']) == 0 and len(these_objects['direct']) and (\n",
    "        len(these_objects['NP']) or\\\n",
    "        len(these_objects['L']) or\\\n",
    "        len(these_objects['K']) or\\\n",
    "        len(these_objects['infinitive'])\n",
    "    ): # promote the direct objects to principal direct objects\n",
    "        these_objects['principal'] = these_objects['direct']\n",
    "        these_objects['direct'] = set()\n",
    "\n",
    "    for kind in object_kinds:\n",
    "        n = len(these_objects.get(kind, set()))\n",
    "        objects_count[kind][n] += 1\n",
    "        if n:\n",
    "            objects[kind] |= these_objects[kind]\n",
    "\n",
    "utils.caption(0, '\\tDone')\n",
    "\n",
    "for kind in object_kinds:\n",
    "    total = 0\n",
    "    for (count, n) in sorted(objects_count[kind].items(), key=lambda y: -y[0]):\n",
    "        if count: total += n\n",
    "        utils.caption(0, '\\t{:>5} clauses with {:>2} {:>10} object{}'.format(n, count, kind, 's' if count != 1 else ''))\n",
    "    utils.caption(0, '\\t{:>5} clauses with {:>2} {:>10} object'.format(total, 'a', kind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Indirect objects\n",
    "\n",
    "The ETCBC database has not feature that marks indirect objects.\n",
    "We will use computation to determine whether a complement is an indirect object or a locative.\n",
    "This computation is just an approximation.\n",
    "\n",
    "[More on indirect objects](https://github.com/ETCBC/valence/wiki/Discussion#indirect-objects)\n",
    "\n",
    "### The decision\n",
    "\n",
    "We take a decision as follows.\n",
    "Based on indicators $ind$ and $loc$ that are proxies for the degree in which the complement is an indirect object or a locative, we arrive at a decision $L$ (complement is *locative*) or $I$ (complement is *indirect object*) or $C$ (complement is neither *locative* nor *indirect object*) as follows:\n",
    "\n",
    "(1) $ loc > 0 \\wedge ind = 0 \\Rightarrow L $\n",
    "\n",
    "(2) $ loc = 0 \\wedge ind > 0 \\Rightarrow I $\n",
    "\n",
    "(3) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc - 1 > ind \\Rightarrow L$\n",
    "\n",
    "(4) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc + 1 < ind \\Rightarrow I$\n",
    "\n",
    "(5) $ loc > 0 \\wedge ind > 0 \\wedge |ind - loc| <= 1 \\Rightarrow C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complfuncs = set('''\n",
    "Cmpl PreC\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_iobj_preps = set('''\n",
    "L >L\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locative_lexemes = set('''\n",
    ">RY/ >YL/ >XR/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJN/ BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "'''.strip().split())\n",
    "\n",
    "personal_lexemes = set('''\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/ MYRJ/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         19s Determinig kind of complements                                                 .\n",
      "..............................................................................................\n",
      "|         20s \tDone\n",
      "|         20s \tPhrases of kind C :  19300\n",
      "|         20s \tPhrases of kind L :  11681\n",
      "|         20s \tPhrases of kind I :   6016\n",
      "|         20s \tTotal complements :  36997\n",
      "|         20s \tTotal phrases     : 214665\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Determinig kind of complements')\n",
    "\n",
    "complements_c = collections.defaultdict(lambda: collections.defaultdict(lambda: []))\n",
    "complements = {}\n",
    "complementk = {}\n",
    "kcomplements = collections.Counter()\n",
    "\n",
    "nphrases = 0\n",
    "ncomplements = 0\n",
    "\n",
    "for c in clause_verb:\n",
    "    for p in L.d(c, 'phrase'):\n",
    "        nphrases += 1\n",
    "        pf = pf_corr.get(p, F.function.v(p))\n",
    "        if pf not in complfuncs: continue\n",
    "        ncomplements += 1\n",
    "        words = L.d(p, 'word')\n",
    "        lexemes = [F.lex.v(w) for w in words]\n",
    "        lexeme_set = set(lexemes)\n",
    "\n",
    "        # measuring locativity\n",
    "        lex_locativity = len(locative_lexemes & lexeme_set)\n",
    "        prep_b = len([x for x in lexeme_set if x == 'B'])\n",
    "        topo = len([x for x in words if F.nametype.v(x) == 'topo'])\n",
    "        h_loc = len([x for x in words if F.uvf.v(x) == 'H'])\n",
    "        body_part = 0\n",
    "        if len(words) > 1 and F.lex.v(words[0]) == 'L' and F.lex.v(words[1]) in body_parts:\n",
    "            body_part = 2\n",
    "        loca = lex_locativity + topo + prep_b + h_loc + body_part\n",
    "\n",
    "        # measuring indirect object\n",
    "        prep_l = len([x for x in words if F.lex.v(x) in cmpl_as_iobj_preps and F.prs.v(x) not in no_prs])\n",
    "        prep_lpr = 0\n",
    "        lwn = len(words)\n",
    "        for (n, wn) in enumerate(words):\n",
    "            if F.lex.v(wn) in cmpl_as_iobj_preps:\n",
    "                if n+1 < lwn:\n",
    "                    nextw = words[n+1]\n",
    "                    if F.lex.v(nextw) in personal_lexemes or F.ls.v(nextw) == 'gntl' or (\n",
    "                        F.sp.v(nextw) == 'nmpr' and F.nametype.v(nextw) == 'pers'):\n",
    "                        prep_lpr += 1                        \n",
    "        indi = prep_l + prep_lpr\n",
    "\n",
    "        # the verdict\n",
    "        ckind = 'C'\n",
    "        if loca == 0 and indi > 0: ckind = 'I'\n",
    "        elif loca > 0 and indi == 0: ckind = 'L'\n",
    "        elif loca > indi + 1: ckind = 'L'\n",
    "        elif loca < indi - 1: ckind = 'I'\n",
    "        complementk[p] = (loca, indi, ckind)\n",
    "        kcomplements[ckind] += 1\n",
    "        complements_c[c][ckind].append(p)\n",
    "        complements[p] = (pf, ckind)\n",
    "\n",
    "utils.caption(0, '\\tDone')\n",
    "for (label, n) in sorted(kcomplements.items(), key=lambda y: -y[1]):\n",
    "    utils.caption(0, '\\tPhrases of kind {:<2}: {:>6}'.format(label, n))\n",
    "utils.caption(0, '\\tTotal complements : {:>6}'.format(ncomplements))\n",
    "utils.caption(0, '\\tTotal phrases     : {:>6}'.format(nphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_L(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len(words) > 0 and F.lex.v(words[0] == 'L')\n",
    "\n",
    "def is_lex_personal(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len(words) > 1 and (F.lex.v(words[1]) in personal_lexemes or F.nametype.v(words[1]) == 'pers')\n",
    "\n",
    "def is_lex_local(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len({F.lex.v(w) for w in words} & locative_lexemes) > 0\n",
    "\n",
    "def has_H_locale(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len({w for w in words if F.uvf.v(w) == 'H'}) > 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Generic logic\n",
    "\n",
    "This is the function that applies the generic rules about (in)direct objects and locatives.\n",
    "It takes a phrase node and a set of new label values, and modifies those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grule_as_str = {\n",
    "    'pdos':   '''direct_object => principal_direct_object''',\n",
    "    'pdos-x': '''non-object => principal_direct_object''',\n",
    "    'ndos':   '''direct_object => NP_direct_object''',\n",
    "    'ndos-x': '''non-object => NP_direct_object''',\n",
    "    'dos':    '''non-object => direct_object''',\n",
    "    'ldos':   '''non-object => L_object''',\n",
    "    'kdos':   '''non-object => K_object''',\n",
    "    'inds-c': '''complement => indirect_object''',\n",
    "    'locs-c': '''complement => location''',\n",
    "    'inds-p': '''predicate complement => indirect_object''',\n",
    "    'locs-p': '''predicate complement => location''',\n",
    "    'cdos':   '''direct-object =(superfluously)=> direct object (clause)''',\n",
    "    'cdos-x': '''non-object => direct object (clause)''',\n",
    "    'idos':   '''infinitive_object =(superfluously)=> infinitive_object (clause)''',\n",
    "    'idos-x': '''infinitive clause => infinitive_object''',\n",
    "}\n",
    "\n",
    "def rule_as_str_g(x, i): return '{}-{}'.format(i, grule_as_str[i])\n",
    "\n",
    "rule_as_str = dict(\n",
    "    generic=rule_as_str_g,\n",
    ")\n",
    "\n",
    "def generic_logic_p(pn, values):\n",
    "    gl = None\n",
    "    if pn in objects['principal']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'pdos'\n",
    "        else:\n",
    "            gl = 'pdos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'principal_direct_object'\n",
    "    elif pn in objects['NP']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'ndos'\n",
    "        else:\n",
    "            gl = 'ndos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'NP_direct_object'\n",
    "    elif pn in objects['direct']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv != 'direct_object':\n",
    "            gl = 'dos'\n",
    "            values['original'] = oldv\n",
    "            values['grammatical'] = 'direct_object'\n",
    "    elif pn in objects['L']:\n",
    "        oldv = values['grammatical']\n",
    "        gl = 'ldos'\n",
    "        values['original'] = oldv\n",
    "        values['grammatical'] = 'L_object'\n",
    "    elif pn in objects['K']:\n",
    "        oldv = values['grammatical']\n",
    "        gl = 'kdos'\n",
    "        values['original'] = oldv\n",
    "        values['grammatical'] = 'K_object'\n",
    "    elif pn in complements:\n",
    "        (pf, ck) = complements[pn]\n",
    "        if ck in {'I', 'L'}:\n",
    "            if pf == 'Cmpl':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 'inds-c'\n",
    "                else:\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 'locs-c'\n",
    "            elif pf == 'PreC':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 'inds-p'\n",
    "                else:\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 'locs-p'\n",
    "    return gl\n",
    "\n",
    "def generic_logic_c(cn, values):\n",
    "    gl = None\n",
    "    if cn in objects['clause']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'cdos'\n",
    "        else:\n",
    "            gl = 'cdos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'direct_object'\n",
    "    elif cn in objects['infinitive']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'infinitive_object':\n",
    "            gl = 'idos'\n",
    "        else:\n",
    "            gl = 'idos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'infinitive_object'\n",
    "    return gl\n",
    "\n",
    "generic_logic = dict(\n",
    "    phrase=generic_logic_p,\n",
    "    clause=generic_logic_c,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Verb specific rules\n",
    "\n",
    "The verb-specific enrichment rules are stored in a dictionary, keyed  by the verb lexeme.\n",
    "The rule itself is a list of items.\n",
    "\n",
    "The last item is a tuple of conditions that need to be fulfilled to apply the rule.\n",
    "\n",
    "A condition can take the shape of\n",
    "\n",
    "* a function, taking a phrase or clause node as argument and returning a boolean value\n",
    "* an ETCBC feature for phrases or clauses : value, \n",
    "  which is true iff that feature has that value for the phrase or clause in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbl_obj_rules = (\n",
    "    (\n",
    "        ('semantic', 'benefactive'), \n",
    "        ('function:Adju', has_L, is_lex_personal),\n",
    "    ),\n",
    "    (\n",
    "        ('lexical', 'location'),\n",
    "        ('function:Cmpl', has_H_locale),\n",
    "    ),\n",
    "    (\n",
    "        ('lexical', 'location'),\n",
    "        ('semantic', 'location'),\n",
    "        ('function:Cmpl', is_lex_local),\n",
    "    ),\n",
    ")\n",
    "enrich_logic = dict(\n",
    "    phrase={\n",
    "        'CJT': dbl_obj_rules,\n",
    "        'FJM': dbl_obj_rules,\n",
    "    },\n",
    "    clause={\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         21s Checking enrichment logic                                                      .\n",
      "..............................................................................................\n",
      "CJT-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "CJT-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "CJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "FJM-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "FJM-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "FJM-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "|         21s \tAll 6 rules OK\n"
     ]
    }
   ],
   "source": [
    "rule_index = collections.defaultdict(lambda: [])\n",
    "\n",
    "def rule_as_str_s(vl, i):\n",
    "    (conditions, sfassignments) = rule_index[vl][i]\n",
    "    label = '{}-{}\\n'.format(vl, i+1)\n",
    "    rule = '\\tIF   {}'.format('\\n\\tAND  '.join(\n",
    "        '{:<10} = {:<8}'.format(\n",
    "                *c.split(':')\n",
    "            ) if type(c) is str else '{:<15}'.format(\n",
    "                c.__name__\n",
    "            ) for c in conditions,\n",
    "    ))\n",
    "    ass = []\n",
    "    for (i, sfa) in enumerate(sfassignments):\n",
    "        ass.append('\\t\\t{:<10} => {:<15}\\n'.format(*sfa))\n",
    "    return '{}{}\\n\\tTHEN\\n{}'.format(label, rule, ''.join(ass))\n",
    "\n",
    "rule_as_str['specific'] = rule_as_str_s\n",
    "\n",
    "def check_logic():\n",
    "    utils.caption(4, 'Checking enrichment logic')\n",
    "    errors = 0\n",
    "    nrules = 0\n",
    "    for kind in sorted(enrich_logic):\n",
    "        for vl in sorted(enrich_logic[kind]):\n",
    "            for items in enrich_logic[kind][vl]:\n",
    "                rule_index[vl].append((items[-1], items[0:-1]))\n",
    "            for (i, (conditions, sfassignments)) in enumerate(rule_index[vl]):\n",
    "                if not SCRIPT: utils.caption(0, rule_as_str_s(vl, i), continuation=True)\n",
    "                nrules += 1\n",
    "                for (sf, sfval) in sfassignments:\n",
    "                    if sf not in enrich_fields:\n",
    "                        utils.caption(0, 'ERROR: {}: \"{}\" not a valid enrich field'.format(kind, sf), continuation=True)\n",
    "                        errors += 1\n",
    "                    elif sfval not in enrich_fields[sf]:\n",
    "                        utils.caption(0, 'ERROR: {}: `{}`: \"{}\" not a valid enrich field value'.format(kind, sf, sfval), continuation=True)\n",
    "                        errors += 1\n",
    "                for c in conditions:\n",
    "                    if type(c) == str:\n",
    "                        x = c.split(':')\n",
    "                        if len(x) != 2:\n",
    "                            utils.caption(0, 'ERROR: {}: Wrong feature condition {}'.format(kind, c), continuation=True)\n",
    "                            errors += 1\n",
    "                        else:\n",
    "                            (feat, val) = x\n",
    "                            if feat not in legal_values:\n",
    "                                utils.caption(0, 'ERROR: {}: Feature `{}` not in use'.format(kind, feat), continuation=True)\n",
    "                                errors += 1\n",
    "                            elif val not in legal_values[feat]:\n",
    "                                utils.caption(0, 'ERROR: {}: Feature `{}`: not a valid value \"{}\"'.format(kind, feat, val), continuation=True)\n",
    "                                errors += 1\n",
    "    if errors:\n",
    "        utils.caption(0, '\\tERROR: There were {} errors in {} rules'.format(errors, nrules))\n",
    "    else:\n",
    "        utils.caption(0, '\\tAll {} rules OK'.format(nrules))\n",
    "\n",
    "check_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rule_cases = collections.defaultdict(lambda: collections.defaultdict(lambda: {}))\n",
    "\n",
    "def apply_logic(kind, vl, n, init_values):\n",
    "    values = deepcopy(init_values)\n",
    "    gr = generic_logic[kind](n, values)\n",
    "    if gr:\n",
    "        rule_cases['generic'][kind].setdefault(('', gr), []).append(n)\n",
    "    verb_rules = enrich_logic[kind].get(vl, [])\n",
    "    for (i, items) in enumerate(verb_rules):\n",
    "        conditions = items[-1]\n",
    "        sfassignments = items[0:-1]\n",
    "\n",
    "        ok = True\n",
    "        for condition in conditions:\n",
    "            if type(condition) is str:\n",
    "                (feature, value) = condition.split(':')\n",
    "                if feature == 'function' and kind == 'phrase':\n",
    "                    fval = pf_corr.get(n, F.function.v(n))\n",
    "                else:\n",
    "                    fval = F.item[feature].v(n)\n",
    "                this_ok =  fval == value\n",
    "            else:\n",
    "                this_ok = condition(vl, n)\n",
    "            if not this_ok:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                values[sf] = sfval\n",
    "            rule_cases['specific'][kind].setdefault((vl, i), []).append(n)\n",
    "    return tuple(values[sf] for sf in enrich_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Generate enrichments\n",
    "\n",
    "First we generate enriched values for all relevant phrases.\n",
    "The generated enrichment values are computed on the basis of generic logic.\n",
    "Additionally, verb-bound logic is applied, if it has been specified.\n",
    "\n",
    "We store the enriched features in a dictionary, first keyed by the type of constituent that\n",
    "receives the enrichments (`phrase` or `clause`), and then by the node number of the constituent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         21s Generating enrichments                                                         .\n",
      "..............................................................................................\n",
      "|         27s \tGenerated enrichment values for 1380 verbs:\n",
      "|         27s \tEnriched values for 221480 nodes\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Generating enrichments')\n",
    "\n",
    "seen = collections.defaultdict(collections.Counter)\n",
    "enrichFields = dict()\n",
    "\n",
    "def gen_enrich(verb):\n",
    "    clauses_seen = set()\n",
    "\n",
    "    for wn in occs[verb]:\n",
    "        cn = L.u(wn, 'clause')[0]\n",
    "        if cn in clauses_seen:\n",
    "            continue\n",
    "        clauses_seen.add(cn)\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        for pn in L.d(cn, 'phrase'):\n",
    "            seen['phrase'][pn] += 1\n",
    "            pf = pf_corr.get(pn, F.function.v(pn))\n",
    "            enrichFields[pn] = apply_logic('phrase', vl, pn, transform['phrase'][pf])\n",
    "        for scn in clause_objects[cn]:\n",
    "            seen['clause'][scn] += 1\n",
    "            scty = F.typ.v(scn)\n",
    "            scr = F.rela.v(scn)\n",
    "            enrichFields[scn] = apply_logic('clause', vl, scn, transform['clause'][scr if scr == 'Objc' else scty])       \n",
    "\n",
    "for verb in verb_clause_index:\n",
    "    gen_enrich(verb)\n",
    "utils.caption(0, '\\tGenerated enrichment values for {} verbs:'.format(len(verb_clause_index)))\n",
    "utils.caption(0, '\\tEnriched values for {:>5} nodes'.format(len(enrichFields)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         27s \tOverview of rule applications:\n",
      "|         27s generic-phrase rules:\n",
      "|         27s  593 x\n",
      "\tlocs-p-predicate complement => location\n",
      "\t783401, 651346, 651714, 652353, 652867, 653153, 653166, 653201, 653526, 653657\n",
      "\n",
      "|         27s 1107 x\n",
      "\tndos-direct_object => NP_direct_object\n",
      "\t784175, 788609, 886801, 668479, 677394, 680315, 696123, 699926, 705151, 705713\n",
      "\n",
      "|         27s 3880 x\n",
      "\tpdos-direct_object => principal_direct_object\n",
      "\t784176, 786299, 788608, 744485, 746150, 770742, 875780, 886800, 891627, 899828\n",
      "\n",
      "|         27s 11111 x\n",
      "\tlocs-c-complement => location\n",
      "\t813119, 651350, 652403, 652422, 653630, 653825, 653829, 654592, 655468, 659553\n",
      "\n",
      "|         27s 4185 x\n",
      "\tldos-non-object => L_object\n",
      "\t651456, 651459, 651644, 653752, 655485, 655779, 658449, 659412, 660324, 663446\n",
      "\n",
      "|         27s 5183 x\n",
      "\tinds-c-complement => indirect_object\n",
      "\t651642, 653059, 653537, 653761, 653772, 653983, 653989, 655443, 656787, 657219\n",
      "\n",
      "|         27s  371 x\n",
      "\tinds-p-predicate complement => indirect_object\n",
      "\t651748, 654465, 655403, 655523, 655788, 655857, 656788, 657220, 658635, 659143\n",
      "\n",
      "|         27s  122 x\n",
      "\tkdos-non-object => K_object\n",
      "\t655950, 673441, 678445, 794026, 810194, 812671, 816234, 818068, 822357, 830362\n",
      "\n",
      "|         27s  26552 generic-phrase rule applications\n",
      "|         27s generic-clause rules:\n",
      "|         27s 1298 x\n",
      "\tidos-x-infinitive clause => infinitive_object\n",
      "\t472744, 505829, 514037, 514226, 447650, 447655, 497190, 506179, 428311, 434075\n",
      "\n",
      "|         27s 1334 x\n",
      "\tcdos-direct-object =(superfluously)=> direct object (clause)\n",
      "\t467066, 477473, 428557, 435935, 455810, 457913, 468288, 472504, 485768, 495459\n",
      "\n",
      "|         27s   2632 generic-clause rule applications\n",
      "|         27s  29184 generic rule applications\n",
      "|         27s specific-phrase rules:\n",
      "|         27s  108 x\n",
      "\tFJM-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "\t651757, 654687, 657403, 657797, 657890, 660169, 660277, 661237, 661586, 662184\n",
      "\n",
      "|         27s    1 x\n",
      "\tFJM-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "\t812839\n",
      "\n",
      "|         27s    8 x\n",
      "\tCJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "\t652139, 652555, 665499, 677115, 700297, 822426, 835708, 843543\n",
      "\n",
      "|         27s    117 specific-phrase rule applications\n",
      "|         27s    117 specific rule applications\n",
      "|         27s \t204747 phrase seen 1  time(s)\n",
      "|         27s \t  9324 phrase seen 2  time(s)\n",
      "|         27s \t   553 phrase seen 3  time(s)\n",
      "|         27s \t    32 phrase seen 4  time(s)\n",
      "|         27s \t     9 phrase seen 5  time(s)\n",
      "|         27s \t214665 phrase seen in total\n",
      "|         27s \t  6629 clause seen 1  time(s)\n",
      "|         27s \t   176 clause seen 2  time(s)\n",
      "|         27s \t     9 clause seen 3  time(s)\n",
      "|         27s \t     1 clause seen 4  time(s)\n",
      "|         27s \t  6815 clause seen in total\n"
     ]
    }
   ],
   "source": [
    "utils.caption(0, '\\tOverview of rule applications:')\n",
    "\n",
    "for scope in rule_cases:\n",
    "    totalscope = 0\n",
    "    for kind in rule_cases[scope]:\n",
    "        utils.caption(0, '{}-{} rules:'.format(scope, kind))\n",
    "        totalkind = 0\n",
    "        for rule_spec in rule_cases[scope][kind]:\n",
    "            cases = rule_cases[scope][kind][rule_spec]\n",
    "            n = len(cases)\n",
    "            totalscope += n\n",
    "            totalkind += n\n",
    "            if not SCRIPT:\n",
    "                if scope == 'generic':\n",
    "                    utils.caption(0, '{:>4} x\\n\\t{}\\n\\t{}\\n'.format(\n",
    "                        n, rule_as_str[scope](*rule_spec), \n",
    "                        ', '.join(str(c) for c in cases[0:10]),\n",
    "                    ))\n",
    "                else:                \n",
    "                    utils.caption(0, '{:>4} x\\n\\t{}\\n\\t{}\\n'.format(\n",
    "                        n, rule_as_str[scope](*rule_spec),\n",
    "                        ', '.join(str(c) for c in cases[0:10]),\n",
    "                    ))\n",
    "        utils.caption(0, '{:>6} {}-{} rule applications'.format(totalkind, scope, kind))\n",
    "    utils.caption(0, '{:>6} {} rule applications'.format(totalscope, scope))\n",
    "\n",
    "for kind in seen:\n",
    "    stats = collections.Counter()\n",
    "    for (node, times) in seen[kind].items(): stats[times] += 1\n",
    "    if not SCRIPT:\n",
    "        for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "            utils.caption(0, '\\t{:>6} {} seen {:<2} time(s)'.format(n, kind, times))\n",
    "    utils.caption(0, '\\t{:>6} {} seen in total'.format(len(seen[kind]), kind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For selected verbs, we write the enrichments to spreadsheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "onode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "verb_lexeme\n",
      "verb_stem\n",
      "verb_occurrence\n",
      "text\n",
      "constituent\n",
      "type\n",
      "rela\n",
      "type\n",
      "function\n",
      "valence\n",
      "predication\n",
      "grammatical\n",
      "original\n",
      "lexical\n",
      "semantic\n"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    onode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "    text\n",
    "    constituent\n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    type\n",
    "    function\n",
    "'''.strip().split()\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    type\n",
    "    rela\n",
    "'''.strip().split()\n",
    "\n",
    "field_names = COMMON_FIELDS + CLAUSE_FIELDS + PHRASE_FIELDS + list(enrich_fields) \n",
    "pfillrows = len(CLAUSE_FIELDS)\n",
    "cfillrows = len(PHRASE_FIELDS)\n",
    "fillrows =  pfillrows + cfillrows + len(enrich_fields)\n",
    "if not SCRIPT: print('\\n'.join(field_names))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         27s Generate blank enrichment sheets                                               .\n",
      "..............................................................................................\n",
      "|         27s \tas /Users/dirk/github/etcbc/valence/source/c/enrich_blank/{verb}.csv\n",
      "|         28s \t\tfor verb NF> ( 2884 rows)\n",
      "|         28s \t\tfor verb QR> ( 3724 rows)\n",
      "|         28s \t\tfor verb NPL ( 1933 rows)\n",
      "|         28s \t\tfor verb BW> (11097 rows)\n",
      "|         28s \t\tfor verb CJT (  381 rows)\n",
      "|         28s \t\tfor verb NWS (  618 rows)\n",
      "|         29s \t\tfor verb NTN ( 9830 rows)\n",
      "|         29s \t\tfor verb CWB ( 4326 rows)\n",
      "|         29s \t\tfor verb PQD ( 1283 rows)\n",
      "|         30s \t\tfor verb JY> ( 4626 rows)\n",
      "|         30s \t\tfor verb BR> (  221 rows)\n",
      "|         30s \t\tfor verb SWR ( 1280 rows)\n",
      "|         30s \t\tfor verb HLK ( 5820 rows)\n",
      "|         30s \t\tfor verb <BR ( 2349 rows)\n",
      "|         30s \t\tfor verb <LH ( 3893 rows)\n",
      "|         30s \t\tfor verb JRD ( 1593 rows)\n",
      "|         31s \t\tfor verb <FH (11317 rows)\n",
      "|         32s \t\tfor verb FJM ( 2919 rows)\n",
      "|         32s \tDone\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Generate blank enrichment sheets')\n",
    "sheetKind = 'enrich_blank'\n",
    "utils.caption(0, '\\tas {}'.format(vfile('{verb}', sheetKind)[1]))\n",
    "\n",
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cn = L.u(wn, 'clause')[0]\n",
    "        if cn in clauses_seen: continue\n",
    "        clauses_seen.add(cn)\n",
    "        vn = L.u(wn, 'verse')[0]\n",
    "        bn = L.u(wn, 'book')[0]\n",
    "        (book_name, chapter, verse) = T.sectionFromNode(cn, lang='la')\n",
    "        book = T.sectionFromNode(cn)[0]\n",
    "        ln = linkShebanq+(linkPassage.format(book_name, chapter, verse))+linkAppearance\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.text([wn], fmt='text-trans-plain')\n",
    "        ct = T.text(L.d(cn, 'word'), fmt='text-trans-plain')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, vl, vstem, vt, ct, '')\n",
    "        rows.append(common_fields + (('',)*fillrows))\n",
    "        for pn in L.d(cn, 'phrase'):\n",
    "            seen['phrase'][pn] += 1\n",
    "            pt = T.text(L.d(pn, 'word'), fmt='text-trans-plain')\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vstem, '', pt, 'phrase')\n",
    "            pty = F.typ.v(pn)\n",
    "            pf = pf_corr.get(pn, F.function.v(pn))\n",
    "            phrase_fields =\\\n",
    "                ('',)*pfillrows +\\\n",
    "                (pty, pf) +\\\n",
    "                enrichFields[pn]\n",
    "            rows.append(common_fields + phrase_fields)\n",
    "        for scn in clause_objects[cn]:\n",
    "            seen['clause'][scn] += 1\n",
    "            sct = T.text(L.d(scn, 'word'), fmt='text-trans-plain')\n",
    "            common_fields = (cn, wn, scn, book, chapter, verse, vl, vstem, '', sct, 'clause')\n",
    "            scty = F.typ.v(scn)\n",
    "            scr = F.rela.v(scn)\n",
    "            clause_fields =\\\n",
    "                (scty, scr) +\\\n",
    "                ('',)*cfillrows +\\\n",
    "                enrichFields[scn]\n",
    "            rows.append(common_fields + clause_fields)\n",
    "\n",
    "    location = vfile(verb, sheetKind)\n",
    "    if location == None: return\n",
    "    (baseName, fileName) = location\n",
    "\n",
    "    row_file = open(fileName, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    utils.caption(0, '\\t\\tfor verb {} ({:>5} rows)'.format(verb, len(rows)))\n",
    "    \n",
    "for verb in verbs: gen_sheet_enrich(verb)\n",
    "\n",
    "utils.caption(0, '\\tDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showcase(n):\n",
    "    otype = F.otype.v(n)\n",
    "    att1 = pf_corr.get(n, F.function.v(n)) if otype == 'phrase' else F.rela.v(n)\n",
    "    att2 = F.typ.v(n)\n",
    "    utils.caption(0, '''{} ({}-{}) {}\\n{} {}:{}    {}\\n'''.format(\n",
    "        otype, att1, att2,\n",
    "        T.text(L.d(n, 'word'), fmt='text-trans-plain'),\n",
    "        *T.sectionFromNode(n),\n",
    "        T.text(L.d(L.u(n, 'verse')[0], 'word'), fmt='text-trans-plain'),\n",
    "    ), continuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase (Conj-CP) W\n",
      "Genesis 14:15    WJXLQ <LJHM05 LJLH HW> W<BDJW WJKM WJRDPM <D&XWBH >CR MFM>L LDMFQ00 \n",
      "\n",
      "clause (Attr-xQt0) >CR L>&JD<W HJWM VWB WR< \n",
      "Deuteronomy 1:39    WVPKM >CR >MRTM LBZ JHJH WBNJKM >CR L>&JD<W HJWM VWB WR< HMH JB>W CMH WLHM >TNNH WHM JJRCWH00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not SCRIPT:\n",
    "    showcase(654844)\n",
    "    showcase(445014)\n",
    "    #showcase(426954)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb BW>: 2570 occurrences. He locales in Cmpl phrases: 164\n",
      "\t257027, 184326, 26119, 256522, 26128, 146457, 187931, 197149, 95264, 272417, 24617, 184361, 398378, 289837, 201264, 136755, 78902, 401471, 100417, 32834, 5699, 100420, 198219, 200269, 24654, 100945, 24659, 112214, 141912, 186971, 196701, 28769, 34405, 298605, 248942, 132208, 162928, 12403, 5748, 146054, 90248, 396937, 153226, 97426, 134802, 151186, 188053, 426134, 257176, 21657, 136348, 162980, 24741, 200360, 214698, 25778, 257203, 158388, 4790, 100534, 4794, 160444, 214718, 90817, 272580, 139973, 249031, 38600, 113868, 8921, 138458, 19167, 20704, 26851, 282852, 43240, 8425, 8938, 145137, 397042, 170739, 254714, 154364, 79614, 200959, 27392, 176386, 165636, 426244, 206086, 208647, 269580, 26381, 106253, 157457, 257304, 149795, 170792, 211243, 26415, 27439, 126768, 9526, 246073, 109370, 172350, 249151, 26432, 16705, 4930, 64833, 398144, 168781, 154974, 132965, 47465, 393580, 47471, 157551, 37237, 100213, 23416, 269181, 23934, 24449, 411011, 78213, 93578, 26000, 133528, 12699, 19356, 24477, 191392, 93605, 170919, 18345, 157618, 267700, 8634, 244671, 63424, 256963, 167369, 175563, 110543, 138704, 37329, 108497, 5587, 111058, 11733, 143830, 20439, 175571, 192984, 99802, 264148, 170973, 218594, 25063, 269296, 26100, 110583\n",
      "Verb BW>: 2570 occurrences. He locales in Loca phrases: 5\n",
      "\t285000, 29642, 289870, 284976, 289882\n",
      "Verb BW>: 2570 occurrences. He locales in Adju phrases: 2\n",
      "\t75707, 322829\n"
     ]
    }
   ],
   "source": [
    "def check_h(vl, show_results=False):\n",
    "    hl = {}\n",
    "    total = 0\n",
    "    for w in F.otype.s('word'):\n",
    "        if F.sp.v(w) != 'verb' or F.lex.v(w).rstrip('[=/') != vl: continue\n",
    "        total += 1\n",
    "        c = L.u(w, 'clause')[0]\n",
    "        ps = L.d(c, 'phrase')\n",
    "        phs = {p for p in ps if len({w for w in L.d(p, 'word') if F.uvf.v(w) == 'H'}) > 0}\n",
    "        for f in ('Cmpl', 'Adju', 'Loca'):\n",
    "            phc = {p for p in ps if pf_corr.get(p, None) or (pf_corr.get(p, F.function.v(p))) == f}\n",
    "            if len(phc & phs): hl.setdefault(f, set()).add(w)\n",
    "    for f in hl:\n",
    "        utils.caption(0, 'Verb {}: {} occurrences. He locales in {} phrases: {}'.format(vl, total, f, len(hl[f])), continuation=True)\n",
    "        if show_results: utils.caption(0, '\\t{}'.format(', '.join(str(x) for x in hl[f])), continuation=True)\n",
    "\n",
    "if not SCRIPT:\n",
    "    check_h('BW>', show_results=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be handy to generate an informational spreadsheet that shows all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Process the filled in enrichments\n",
    "\n",
    "We read the enrichments, and perform some consistency checks.\n",
    "If the filled-in sheet does not exist, we take the blank sheet, with the default assignment of the new features.\n",
    "If a phrase got conflicting features, because it occurs in sheets for multiple verbs, the values in the filled-in sheet take precedence over the values in the blank sheet. If both occur in a filled in sheet, a warning will be issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_enrich():\n",
    "    of_enriched = {\n",
    "        False: {}, # for enrichments found in blank sheets\n",
    "        True: {}, # for enrichments found in filled sheets\n",
    "    }\n",
    "    repeated = {\n",
    "        False: collections.defaultdict(list), # for blank sheets\n",
    "        True: collections.defaultdict(list), # for filled sheets\n",
    "    }\n",
    "    wrong_value = {\n",
    "        False: collections.defaultdict(list),\n",
    "        True: collections.defaultdict(list),\n",
    "    }\n",
    "\n",
    "    non_match = collections.defaultdict(list)\n",
    "    wrong_node = collections.defaultdict(list)\n",
    "\n",
    "    results = []\n",
    "    dev_results = [] # results that deviate from the filled sheet\n",
    "    \n",
    "    ERR_LIMIT = 10\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        vresults = {\n",
    "            False: {}, # for blank sheets\n",
    "            True: {}, # for filled sheets\n",
    "        }\n",
    "        for check in (\n",
    "            (False, 'blank'), \n",
    "            (True, 'filled'),\n",
    "        ):\n",
    "            is_filled = check[0]\n",
    "            \n",
    "            location = vfile(verb, 'enrich_{}'.format(check[1]))\n",
    "            if location == None: continue\n",
    "            (baseName, fileName) = location\n",
    "\n",
    "            if not os.path.exists(fileName):\n",
    "                if not is_filled:\n",
    "                    utils.caption(0, '\\tNO {} enrichment sheet for {}'.format(check[1], baseName))\n",
    "                continue\n",
    "            utils.caption(0, '\\t{} enrichment sheet for {}'.format(check[1], baseName))\n",
    "\n",
    "            with open(fileName) as fh:\n",
    "                header = fh.__next__()\n",
    "                for line in fh:\n",
    "                    fields = line.rstrip().split(';')\n",
    "                    on = int(fields[2])\n",
    "                    if on < 0: continue\n",
    "                    kind = fields[10]\n",
    "                    objects_seen[kind][on] += 1\n",
    "                    vvals = tuple(fields[-nef:])\n",
    "                    for (f, v) in zip(enrich_fields, vvals):\n",
    "                        if v != '' and v != 'X' and v != 'NA' and v not in enrich_fields[f]:\n",
    "                            wrong_value[is_filled][on].append((verb, f, v))\n",
    "                    vresults[is_filled][on] = vvals\n",
    "                    if on in of_enriched[is_filled]:\n",
    "                        if on not in repeated[is_filled]:\n",
    "                            repeated[is_filled][on] = [of_enriched[is_filled][on]]\n",
    "                        repeated[is_filled][on].append((verb, vvals))\n",
    "                    else:\n",
    "                        of_enriched[is_filled][on] = (verb, vvals)\n",
    "                    if F.otype.v(on) != kind: \n",
    "                        non_match[on].append((verb, kind))\n",
    "            for on in sorted(vresults[True]):          # check whether the phrase ids are not mangled\n",
    "                if on not in vresults[False]:\n",
    "                    wrong_node[on].append(verb)\n",
    "            for on in sorted(vresults[False]):      # now collect all results, give precedence to filled values\n",
    "                if F.otype.v(on) == 'phrase':\n",
    "                    f_corr = on in pf_corr  # manual correction in phrase function\n",
    "                    f_good = pf_corr.get(on, F.function.v(on)) \n",
    "                else:\n",
    "                    f_corr = ''\n",
    "                    f_good = ''\n",
    "                s_manual = on in vresults[True] and vresults[False][on] != vresults[True][on] # real change\n",
    "\n",
    "                # here we determine which value is going to be put in a feature\n",
    "                # basic rule: if there is an filled-in sheet, take the value from there, else from the blank one\n",
    "                # exception: \n",
    "                # if a value is empty in the filled-in sheet, but not in the blank one, take the non-empty one\n",
    "                #\n",
    "                # Why? Well, sometimes we improve the enrich logic. There may be filled-in sheets based on older\n",
    "                # blank sheets. \n",
    "                # We want to push new values in blank sheets through unfilled in values in the filled sheets.\n",
    "                # If it is intentional to remove a value from the blank sheet, \n",
    "                # you can put an X in the corresponding filled field.\n",
    "                blank_results = vresults[False][on]\n",
    "                these_results = []\n",
    "\n",
    "                for (i, br) in enumerate(blank_results):\n",
    "                    the_value = br\n",
    "                    if s_manual and vresults[True][on][i] != '':\n",
    "                        the_value = vresults[True][on][i]\n",
    "                        if the_value == 'X':\n",
    "                            the_value = ''\n",
    "                    these_results.append(the_value)\n",
    "                these_results = tuple(these_results)\n",
    "                            \n",
    "                # these_results = vresults[True][on] if s_manual else vresults[False][on]\n",
    "                \n",
    "                if f_corr or s_manual:\n",
    "                    dev_results.append((on,)+these_results+(f_good, f_corr, s_manual))\n",
    "                results.append((on,)+these_results+(f_good, f_corr, s_manual))\n",
    "\n",
    "    for check in (\n",
    "        (False, 'blank'), \n",
    "        (True, 'filled'),\n",
    "    ):\n",
    "        if len(wrong_value[check[0]]): #illegal values in sheets\n",
    "            wrongs = wrong_value[check[0]]\n",
    "            for x in sorted(wrongs)[0:ERR_LIMIT]:\n",
    "                px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "                ref_node = L.u(x, 'clause')[0] if F.otype.v(x) != 'clause' else x\n",
    "                cx = T.text(L.d(ref_node, 'word'), fmt='ev')\n",
    "                passage = T.sectionFromNode(x)\n",
    "                utils.caption(0, 'ERROR: {} Illegal value(s) in {}: {} = {} in {}:'.format(\n",
    "                    passage, check[1], x, px, cx\n",
    "                ), continuation=True)\n",
    "                for (verb, f, v) in wrongs[x]:\n",
    "                    utils.caption(0, 'ERROR: \\t\"{}\" is an illegal value for \"{}\" in verb {}'.format(\n",
    "                        v, f, verb,\n",
    "                    ), continuation=True)\n",
    "            ne = len(wrongs)\n",
    "            if ne > ERR_LIMIT: utils.caption(0, ' ... AND {} CASES MORE'.format(ne - ERR_LIMIT), continuation=True)\n",
    "        else:\n",
    "            utils.caption(0, '\\tOK: The used {} enrichment sheets have legal values'.format(check[1]))\n",
    "\n",
    "        nerrors = 0\n",
    "        if len(repeated[check[0]]): # duplicates in sheets, check consistency\n",
    "            repeats = repeated[check[0]]\n",
    "            for x in sorted(repeats):\n",
    "                overview = collections.defaultdict(list)\n",
    "                for y in repeats[x]: overview[y[1]].append(y[0])\n",
    "                px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "                ref_node = L.u(x, 'clause')[0] if F.otype.v(x) != 'clause' else x\n",
    "                cx = T.text(L.d(ref_node, 'word'), fmt='ev')\n",
    "                passage = T.sectionFromNode(x)\n",
    "                if len(overview) > 1:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        utils.caption(0, 'ERROR: {} Conflict in {}: {} = {} in {}:'.format(\n",
    "                            passage, check[1], x, px, cx\n",
    "                        ), continuation=True)\n",
    "                        for vals in overview:\n",
    "                            utils.caption(0, '\\t{:<40} in verb(s) {}'.format(\n",
    "                                ', '.join(vals),\n",
    "                                ', '.join(overview[vals]),\n",
    "                        ), continuation=True)\n",
    "                elif False: # for debugging purposes\n",
    "                #else:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        utils.caption(0, '\\t{} Agreement in {} {} = {} in {}: {}'.format(\n",
    "                            passage, check[1], x, px, cx, ','.join(list(overview.values())[0]),\n",
    "                        ), continuation=True)\n",
    "            ne = nerrors\n",
    "            if ne > ERR_LIMIT: utils.caption(0, ' ... AND {} CASES MORE'.format(ne - ERR_LIMIT), continuation=True)\n",
    "        if nerrors == 0:\n",
    "            utils.caption(0, '\\tOK: The used {} enrichment sheets are consistent'.format(check[1]))\n",
    "\n",
    "    if len(non_match):\n",
    "        utils.caption(0, 'ERROR: Enrichments have been applied to nodes with non-matching types:')\n",
    "        for x in sorted(non_match)[0:ERR_LIMIT]:\n",
    "            (verb, shouldbe) = non_match[x]\n",
    "            px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "            utils.caption(0, 'ERROR: {}: {} Node {} is not a {} but a {}'.format(\n",
    "                verb, T.sectionFromNode(x), x, shouldbe, F.otype.v(x),\n",
    "            ), continuation=True)\n",
    "        ne = len(non_phrase)\n",
    "        if ne > ERR_LIMIT: utils.caption(0, ' ... AND {} CASES MORE'.format(ne - ERR_LIMIT), continuation=True)\n",
    "    else:\n",
    "        utils.caption(0, '\\tOK: all enriched nodes where phrase nodes')\n",
    "\n",
    "    if len(wrong_node):\n",
    "        utils.caption(0, 'ERROR: Node in filled sheet did not occur in blank sheet:')\n",
    "        for x in sorted(wrong_node)[0:ERR_LIMIT]:\n",
    "            px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "            utils.caption(0, '{}: {} node {}'.format(\n",
    "                wrong_node[x], T.sectionFromNode(x), x,\n",
    "            ), continuation=True)\n",
    "        ne = len(wrong_node)\n",
    "        if ne > ERR_LIMIT: utils.caption(0, ' ... AND {} CASES MORE'.format(ne - ERR_LIMIT), continuation=True)\n",
    "    else:\n",
    "        utils.caption(0, '\\tOK: all enriched nodes occurred in the blank sheet')\n",
    "\n",
    "    if len(dev_results):\n",
    "        utils.caption(0, '\\tOK: there are {} manual correction/enrichment annotations'.format(len(dev_results)))\n",
    "        for r in dev_results[0:ERR_LIMIT]:\n",
    "            (x, *vals, f_good, f_corr, s_manual) = r\n",
    "            px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "            cx = T.text(L.d(L.u(x, 'clause')[0], 'word'), fmt='ev')\n",
    "            utils.caption(0, '{:<30} {:>7} => {:<3} {:<3} {}\\n\\t{}\\n\\t\\t{}'.format(\n",
    "                'COR' if f_corr else '',\n",
    "                'MAN' if s_manual else'',\n",
    "                '{} {}:{}'.format(*T.sectionFromNode(x)), x, ','.join(vals), px, cx\n",
    "            ), continuation=True)\n",
    "        ne = len(dev_results)\n",
    "        if ne > ERR_LIMIT: utils.caption(0, '... AND {} ANNOTATIONS MORE'.format(ne - ERR_LIMIT), continuation=True)\n",
    "    else:\n",
    "        utils.caption(0, '\\tthere are no manual correction/enrichment annotations')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         34s Processing enrichment sheets ...                                               .\n",
      "..............................................................................................\n",
      "|         34s \tas /Users/dirk/github/etcbc/valence/source/c/enrich_filled/{verb}.csv\n",
      "|         34s \tblank enrichment sheet for oBR\n",
      "|         34s \tblank enrichment sheet for oFH\n",
      "|         34s \tblank enrichment sheet for oLH\n",
      "|         34s \tblank enrichment sheet for BRa\n",
      "|         34s \tblank enrichment sheet for BWa\n",
      "|         34s \tblank enrichment sheet for CJT\n",
      "|         34s \tblank enrichment sheet for CWB\n",
      "|         34s \tblank enrichment sheet for FJM\n",
      "|         34s \tblank enrichment sheet for HLK\n",
      "|         34s \tblank enrichment sheet for JRD\n",
      "|         34s \tblank enrichment sheet for JYa\n",
      "|         34s \tblank enrichment sheet for NFa\n",
      "|         34s \tblank enrichment sheet for NPL\n",
      "|         35s \tblank enrichment sheet for NTN\n",
      "|         35s \tblank enrichment sheet for NWS\n",
      "|         35s \tblank enrichment sheet for PQD\n",
      "|         35s \tblank enrichment sheet for QRa\n",
      "|         35s \tblank enrichment sheet for SWR\n",
      "|         35s \tOK: The used blank enrichment sheets have legal values\n",
      "|         35s \tOK: The used blank enrichment sheets are consistent\n",
      "|         35s \tOK: The used filled enrichment sheets have legal values\n",
      "|         35s \tOK: The used filled enrichment sheets are consistent\n",
      "|         35s \tOK: all enriched nodes where phrase nodes\n",
      "|         35s \tOK: all enriched nodes occurred in the blank sheet\n",
      "|         35s \tthere are no manual correction/enrichment annotations\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Processing enrichment sheets ...')\n",
    "sheetKind = 'enrich_filled'\n",
    "\n",
    "utils.caption(0, '\\tas {}'.format(vfile('{verb}', sheetKind)[1]))\n",
    "objects_seen = collections.defaultdict(collections.Counter)\n",
    "sheetResults = read_enrich()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not SCRIPT:\n",
    "    list(enrichFields.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not SCRIPT:\n",
    "    sheetResults[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the sheet results with the generic results in one single dictionary, keyed by node number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         35s Combine the manual results with the generic results                            .\n",
      "..............................................................................................\n",
      "|         35s \tAnnotations from sheets for 53780 nodes\n",
      "|         35s \tMerging 221480 annotations from generic enrichment\n",
      "|         35s \tResulting in annotations for 221480 nodes\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Combine the manual results with the generic results')\n",
    "allResults = dict()\n",
    "for (n, *features) in sheetResults:\n",
    "    allResults[n] = features\n",
    "utils.caption(0, '\\tAnnotations from sheets for {} nodes'.format(len(allResults)))\n",
    "utils.caption(0, '\\tMerging {} annotations from generic enrichment'.format(len(enrichFields)))\n",
    "for (n, features) in enrichFields.items():\n",
    "    if n in allResults: continue\n",
    "    allResults[n] = features + ('', '', False)\n",
    "utils.caption(0, '\\tResulting in annotations for {} nodes'.format(len(allResults)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Generate data\n",
    "\n",
    "We write the correction and enrichment data as a data module in text-fabric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newFeatures = list(enrich_fields.keys())+['function', 'f_correction', 's_manual']\n",
    "\n",
    "description = dict(\n",
    "    title='Correction and enrichment features',\n",
    "    description='Corrections, alternatives and additions to the ETCBC4b encoding of the Hebrew Bible',\n",
    "    purpose='Support the decision process of assigning valence to verbs',\n",
    "    method='Generated blank correction and enrichment spreadsheets with selected clauses',\n",
    "    steps='sheets filled out by researcher; read back in by program; generated new features based on contents',\n",
    "    author='The content and nature of the features are by Janet Dyk, the workflow is by Dirk Roorda',\n",
    ")\n",
    "\n",
    "metaData = {\n",
    "    '': description,\n",
    "    'valence': {\n",
    "        'description': 'verbal valence main classification',\n",
    "    },\n",
    "    'predication': {\n",
    "        'description': 'verbal function main classification',\n",
    "    },\n",
    "    'grammatical': {\n",
    "        'description': 'constituent role main classification',\n",
    "    },\n",
    "    'original': {\n",
    "        'description': 'default value before enrichment logic has been applied',\n",
    "    },\n",
    "    'lexical': {\n",
    "        'description': 'additional lexical characteristics',\n",
    "    },\n",
    "    'semantic': {\n",
    "        'description': 'additional semantic characteristics',\n",
    "    },\n",
    "    'f_correction': {\n",
    "        'description': 'whether the phrase function has been manually corrected',\n",
    "    },\n",
    "    's_manual': {\n",
    "        'description': 'whether the generated enrichment features have been manually changed',\n",
    "    },\n",
    "    'function': {\n",
    "        'description': 'corrected phrase function, only present for phrases that were in a correction sheet',\n",
    "    },\n",
    "}\n",
    "\n",
    "for f in newFeatures: metaData[f]['valueType'] = 'str'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodeFeatures = dict()\n",
    "\n",
    "for (node, featureVals) in allResults.items():\n",
    "    for (fName, fVal) in zip(newFeatures, featureVals):\n",
    "        fValRep = fVal\n",
    "        if type(fVal) is bool:\n",
    "            fValRep = 'y' if fVal else ''\n",
    "        nodeFeatures.setdefault(fName, {})[node] = fValRep\n",
    "\n",
    "RENAMES = [('function', 'cfunction')]\n",
    "for (oldF, newF) in RENAMES:\n",
    "    for data in (nodeFeatures, metaData):\n",
    "        data[newF] = data[oldF]\n",
    "        del data[oldF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         37s Writing TF enrichment features                                                 .\n",
      "..............................................................................................\n",
      "   |     0.45s T cfunction            to /Users/dirk/github/etcbc/valence/_temp/c/tf\n",
      "   |     0.42s T f_correction         to /Users/dirk/github/etcbc/valence/_temp/c/tf\n",
      "   |     0.52s T grammatical          to /Users/dirk/github/etcbc/valence/_temp/c/tf\n",
      "   |     0.60s T lexical              to /Users/dirk/github/etcbc/valence/_temp/c/tf\n",
      "   |     0.54s T original             to /Users/dirk/github/etcbc/valence/_temp/c/tf\n",
      "   |     0.67s T predication          to /Users/dirk/github/etcbc/valence/_temp/c/tf\n",
      "   |     0.76s T s_manual             to /Users/dirk/github/etcbc/valence/_temp/c/tf\n",
      "   |     0.63s T semantic             to /Users/dirk/github/etcbc/valence/_temp/c/tf\n",
      "   |     0.69s T valence              to /Users/dirk/github/etcbc/valence/_temp/c/tf\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Writing TF enrichment features')\n",
    "TF = Fabric(locations=thisTempTf, silent=True)\n",
    "TF.save(nodeFeatures=nodeFeatures, edgeFeatures={}, metaData=metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffs\n",
    "\n",
    "Check differences with previous versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         42s Check differences with previous version                                        .\n",
      "..............................................................................................\n",
      "|         42s \tno features to add\n",
      "|         42s \tno features to delete\n",
      "|         42s \t9 features in common\n",
      "|         42s cfunction                 ... no changes\n",
      "|         42s f_correction              ... no changes\n",
      "|         43s grammatical               ... no changes\n",
      "|         43s lexical                   ... no changes\n",
      "|         43s original                  ... no changes\n",
      "|         43s predication               ... no changes\n",
      "|         44s s_manual                  ... no changes\n",
      "|         44s semantic                  ... no changes\n",
      "|         44s valence                   ... no changes\n",
      "|         44s Done\n"
     ]
    }
   ],
   "source": [
    "utils.checkDiffs(thisTempTf, thisTf, only=set(nodeFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliver \n",
    "\n",
    "Copy the new TF features from the temporary location where they have been created to their final destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         44s Deliver features to /Users/dirk/github/etcbc/valence/tf/c                      .\n",
      "..............................................................................................\n",
      "|         44s \tvalence\n",
      "|         44s \tpredication\n",
      "|         44s \tgrammatical\n",
      "|         44s \toriginal\n",
      "|         44s \tlexical\n",
      "|         44s \tsemantic\n",
      "|         44s \tf_correction\n",
      "|         44s \ts_manual\n",
      "|         44s \tcfunction\n"
     ]
    }
   ],
   "source": [
    "utils.deliverFeatures(thisTempTf, thisTf, nodeFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         44s Load and compile the new TF features                                           .\n",
      "..............................................................................................\n",
      "This is Text-Fabric 3.0.2\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "117 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.35s B lex_utf8             from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.16s B lex                  from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B gloss                from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.16s B sp                   from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.15s B vs                   from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.28s B rela                 from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.29s B typ                  from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.10s B function             from /Users/dirk/github/etcbc/bhsa/tf/c\n",
      "   |     0.85s T valence              from /Users/dirk/github/etcbc/valence/tf/c\n",
      "   |     0.81s T predication          from /Users/dirk/github/etcbc/valence/tf/c\n",
      "   |     0.96s T grammatical          from /Users/dirk/github/etcbc/valence/tf/c\n",
      "   |     0.38s T original             from /Users/dirk/github/etcbc/valence/tf/c\n",
      "   |     0.52s T lexical              from /Users/dirk/github/etcbc/valence/tf/c\n",
      "   |     0.53s T semantic             from /Users/dirk/github/etcbc/valence/tf/c\n",
      "   |     0.38s T f_correction         from /Users/dirk/github/etcbc/valence/tf/c\n",
      "   |     0.38s T s_manual             from /Users/dirk/github/etcbc/valence/tf/c\n",
      "   |     0.47s T cfunction            from /Users/dirk/github/etcbc/valence/tf/c\n",
      "   |     0.00s Feature overview: 112 for nodes; 4 for edges; 1 configs; 7 computed\n",
      "    14s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, 'Load and compile the new TF features')\n",
    "\n",
    "TF = Fabric(locations=[coreTf, thisTf], modules=[''])\n",
    "api = TF.load('''\n",
    "    lex gloss lex_utf8\n",
    "    sp vs lex rela typ\n",
    "    function\n",
    "''' + ' '.join(nodeFeatures))\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "Take the first 10 phrases and retrieve the corrected and uncorrected function feature.\n",
    "Note that the corrected function feature is only filled in, if it occurs in a clause in which a selected verb occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time - Time - True\n",
      "Pred - Pred - True\n",
      "Subj - Subj - True\n",
      "Objc - Objc - True\n",
      "Conj -  - True\n",
      "Subj -  - True\n",
      "Pred -  - True\n",
      "PreC -  - True\n",
      "Conj - None - False\n",
      "Subj - None - False\n"
     ]
    }
   ],
   "source": [
    "for i in list(F.otype.s('phrase'))[0:10]: \n",
    "    print('{} - {} - {}'.format(\n",
    "        F.function.v(i), \n",
    "        F.cfunction.v(i),\n",
    "        L.u(i, 'clause')[0] in clause_verb,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    stop(good=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We put all corrections and enrichments in a single csv file for checking.\n",
    "\n",
    "We also generate a smaller csv, with only the data for selected verbs in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    29s collecting constituents ...\n",
      "    30s   2439 selected of  10000 constituents in   698 selected of  3065 clauses ...\n",
      "    30s   4980 selected of  20000 constituents in  1444 selected of  6135 clauses ...\n",
      "    31s   8078 selected of  30000 constituents in  2299 selected of  9096 clauses ...\n",
      "    32s  10451 selected of  40000 constituents in  2965 selected of 12043 clauses ...\n",
      "    33s  13045 selected of  50000 constituents in  3714 selected of 15059 clauses ...\n",
      "    34s  15631 selected of  60000 constituents in  4470 selected of 18127 clauses ...\n",
      "    35s  18574 selected of  70000 constituents in  5306 selected of 21091 clauses ...\n",
      "    35s  21265 selected of  80000 constituents in  6133 selected of 24167 clauses ...\n",
      "    36s  24093 selected of  90000 constituents in  6962 selected of 27182 clauses ...\n",
      "    37s  26843 selected of 100000 constituents in  7780 selected of 30269 clauses ...\n",
      "    38s  29681 selected of 110000 constituents in  8674 selected of 33442 clauses ...\n",
      "    39s  31693 selected of 120000 constituents in  9317 selected of 36908 clauses ...\n",
      "    40s  33939 selected of 130000 constituents in 10005 selected of 40260 clauses ...\n",
      "    41s  36358 selected of 140000 constituents in 10736 selected of 43403 clauses ...\n",
      "    41s  39112 selected of 150000 constituents in 11553 selected of 46527 clauses ...\n",
      "    42s  41480 selected of 160000 constituents in 12281 selected of 49785 clauses ...\n",
      "    43s  43089 selected of 170000 constituents in 12801 selected of 53363 clauses ...\n",
      "    44s  44771 selected of 180000 constituents in 13374 selected of 56977 clauses ...\n",
      "    45s  46274 selected of 190000 constituents in 13878 selected of 60445 clauses ...\n",
      "    46s  48402 selected of 200000 constituents in 14534 selected of 63713 clauses ...\n",
      "    46s  50643 selected of 210000 constituents in 15165 selected of 66711 clauses ...\n",
      "    47s  53369 selected of 220000 constituents in 15936 selected of 69702 clauses ...\n",
      "    47s  53802 selected of 221472 constituents in 16053 selected of 70131 clauses done\n"
     ]
    }
   ],
   "source": [
    "f = open(allResults, 'w')\n",
    "g = open(selectedResults, 'w')\n",
    "\n",
    "NALLFIELDS = 17\n",
    "tpl = ('{};' * (NALLFIELDS - 1))+'{}\\n'\n",
    "\n",
    "utils.caption(0, 'collecting constituents ...')\n",
    "f.write(tpl.format(\n",
    "    '-',\n",
    "    '-',\n",
    "    'passage',\n",
    "    'verb(s) text',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    'clause text',\n",
    "    'clause node',\n",
    "))\n",
    "f.write(tpl.format(\n",
    "    'corrected',\n",
    "    'enriched',\n",
    "    'passage',\n",
    "    '-',\n",
    "    'object type',\n",
    "    'clause rela',\n",
    "    'clause type',\n",
    "    'phrase function (old)',\n",
    "    'phrase function (new)',\n",
    "    'phrase type',\n",
    "    'valence',\n",
    "    'predication',\n",
    "    'grammatical',\n",
    "    'original',\n",
    "    'lexical',\n",
    "    'semantic',\n",
    "    'object text',\n",
    "    'object node',\n",
    "))\n",
    "i = 0\n",
    "h = 0\n",
    "j = 0\n",
    "c = 0\n",
    "d = 0\n",
    "CHUNK_SIZE = 10000\n",
    "sel_verbs = set(verbs)\n",
    "for cn in sorted(clause_verb):\n",
    "    c += 1\n",
    "    vrbs = sorted(clause_verb[cn])\n",
    "    lex_vrbs = {F.lex.v(verb).rstrip('[=') for verb in vrbs}\n",
    "    selected = len(lex_vrbs & sel_verbs) != 0\n",
    "    if selected:\n",
    "        d += 1\n",
    "        sel_vrbs = [v for v in vrbs if F.lex.v(v).rstrip('[=') in verbs]\n",
    "        \n",
    "        g.write(tpl.format(\n",
    "            '',\n",
    "            '',\n",
    "            '{} {}:{}'.format(*T.sectionFromNode(cn)),\n",
    "            ' '.join(F.lex.v(verb) for verb in sel_vrbs),\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            T.text(L.d(cn, 'word'), fmt='text-trans-plain'),\n",
    "            cn,\n",
    "        ))\n",
    "\n",
    "    f.write(tpl.format(\n",
    "        '',\n",
    "        '',\n",
    "        '{} {}:{}'.format(*T.sectionFromNode(cn)),\n",
    "        ' '.join(F.lex.v(verb) for verb in vrbs),\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        T.text(L.d(cn, 'word'), fmt='text-trans-plain'),\n",
    "        cn,\n",
    "    ))\n",
    "    for pn in L.d(cn, 'phrase'):\n",
    "        i += 1\n",
    "        if selected: h += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            utils.caption(0, '{:>6} selected of {:>6} constituents in {:>5} selected of {:>5} clauses ...'.format(h, i, d, c))\n",
    "            \n",
    "        material = tpl.format(\n",
    "            'COR' if F.f_correction.v(pn) == 'y' else '',\n",
    "            'MAN' if F.s_manual.v(pn) == 'y' else '',\n",
    "            '{} {}:{}'.format(*T.sectionFromNode(pn)),\n",
    "            '',\n",
    "            'phrase',\n",
    "            '',\n",
    "            '',\n",
    "            F.function.v(pn),\n",
    "            F.cfunction.v(pn),\n",
    "            F.typ.v(pn),\n",
    "            F.valence.v(pn),\n",
    "            F.predication.v(pn),\n",
    "            F.grammatical.v(pn),\n",
    "            F.original.v(pn),\n",
    "            F.lexical.v(pn),\n",
    "            F.semantic.v(pn),\n",
    "            T.text(L.d(pn, 'word'), fmt='text-trans-plain'),\n",
    "            pn,\n",
    "        )\n",
    "        f.write(material)\n",
    "        if selected:\n",
    "            g.write(material)\n",
    "    for scn in clause_objects[cn]:\n",
    "        i += 1\n",
    "        if selected: h += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            utils.caption(0, '{:>6} constituents in {:>5} clauses ...'.format(i, c))\n",
    "        material = tpl.format(\n",
    "            '',\n",
    "            '',\n",
    "            '{} {}:{}'.format(*T.sectionFromNode(scn)),\n",
    "            '',\n",
    "            'clause',\n",
    "            F.rela.v(scn),\n",
    "            F.typ.v(scn),\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            F.valence.v(scn),\n",
    "            F.predication.v(scn),\n",
    "            F.grammatical.v(scn),\n",
    "            F.original.v(scn),\n",
    "            F.lexical.v(scn),\n",
    "            F.semantic.v(scn),\n",
    "            T.text(L.d(scn, 'word'), fmt='text-trans-plain'),\n",
    "            scn,\n",
    "        )\n",
    "        f.write(material)\n",
    "        if selected:\n",
    "            g.write(material)\n",
    "\n",
    "f.close()\n",
    "g.close()\n",
    "utils.caption(0, '{:>6} selected of {:>6} constituents in {:>5} selected of {:>5} clauses done'.format(h, i, d, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cmpl\n",
      "True\n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x  = 671522\n",
    "print(pf_corr.get(x, F.function.v(x)))\n",
    "print(is_lex_local('FJM',x))\n",
    "print(x in rule_cases['specific']['phrase'][('FJM', 2)])\n",
    "print(F.lexical.v(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
