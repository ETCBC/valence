{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-listing",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/dans-small.png\"/>\n",
    "<img align=\"right\" src=\"images/tf-small.png\"/>\n",
    "<img align=\"right\" src=\"images/etcbc.png\"/>\n",
    "\n",
    "\n",
    "# Corrections and enrichment\n",
    "\n",
    "In order to do\n",
    "[verbal valence analysis](flowchart.ipynb)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "We also need to enrich constituents surrounding the\n",
    "verb occurrences with higher level features, that can be used\n",
    "as input for the flow chart decisions.\n",
    "\n",
    "Read more in the [wiki](https://github.com/ETCBC/valence/wiki/Workflows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "See [operation](https://github.com/ETCBC/pipeline/blob/master/README.md#operation)\n",
    "for how to run this script in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This notebook processes excel sheets with manual corrections and enrichments.\n",
    "These have been entered against the `4b` version.\n",
    "However, the `4b` version in this repository has been regenerated from scratch,\n",
    "and in that process the node numbers have changed.\n",
    "As the sheets rely on node numbers to let the entered data flow back to the right nodes,\n",
    "these sheets no longer work on this version.\n",
    "It should be possible to identify the material in those sheets on the basis of\n",
    "book, chapter and verse info.\n",
    "But we leave that as an exercise to posterity.\n",
    "\n",
    "\n",
    "For all other versions, we keep the mechanism in place, but for now we work with zero manual input\n",
    "for those versions.\n",
    "\n",
    "As far as *corrections* are concerned: we expect to see them turn up in the continuous version `c`\n",
    "of the core [BHSA](https://github.com/ETCBC/bhsa) data.\n",
    "\n",
    "As far as *enrichments* are concerned: there are very few manual enrichments.\n",
    "Most of the cases are handled by the algorithm in the notebook.\n",
    "\n",
    "We recommend to harvest exceptions in the notebook itself, it has already a mechanism to apply\n",
    "verb specific logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-reproduction",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "import utils\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SCRIPT\" not in locals():\n",
    "    SCRIPT = False\n",
    "    FORCE = True\n",
    "    CORE_NAME = \"bhsa\"\n",
    "    NAME = \"valence\"\n",
    "    VERSION = \"2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def stop(good=False):\n",
    "    if SCRIPT:\n",
    "        sys.exit(0 if good else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Janet Dyk and Dirk Roorda](https://github.com/ETCBC/valence/wiki/Authors)\n",
    "\n",
    "Last modified 2017-09-13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[References](https://github.com/ETCBC/valence/wiki/References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We have carried out the valence project against the Hebrew Text Database BHSA, version `4b`.\n",
    "See the description of the [sources](https://github.com/ETCBC/valence/wiki/Sources).\n",
    "\n",
    "However, we can run our stuff also against the newer versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Start the engines. We use the Python package\n",
    "[text-fabric](https://github.com/Dans-labs/text-fabric)\n",
    "to process the data of the Hebrew Text Database smoothly and efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the context: source file and target directories\n",
    "\n",
    "The conversion is executed in an environment of directories, so that sources, temp files and\n",
    "results are in convenient places and do not have to be shifted around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-police",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "repoBase = os.path.expanduser(\"~/github/etcbc\")\n",
    "coreRepo = \"{}/{}\".format(repoBase, CORE_NAME)\n",
    "thisRepo = \"{}/{}\".format(repoBase, NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "coreTf = \"{}/tf/{}\".format(coreRepo, VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "thisSource = \"{}/source/{}\".format(thisRepo, VERSION)\n",
    "thisTemp = \"{}/_temp/{}\".format(thisRepo, VERSION)\n",
    "thisTempTf = \"{}/tf\".format(thisTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "thisTf = \"{}/tf/{}\".format(thisRepo, VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Check whether this conversion is needed in the first place.\n",
    "Only when run as a script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-spotlight",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[4]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    (good, work) = utils.mustRun(\n",
    "        None, \"{}/.tf/{}.tfx\".format(thisTf, \"valence\"), force=FORCE\n",
    "    )\n",
    "    print(good, work)\n",
    "    if not good:\n",
    "        stop(good=False)\n",
    "    if not work:\n",
    "        stop(good=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the feature data\n",
    "\n",
    "We load the features we need from the BHSA core database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-aaron",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".       0.00s Load the existing TF dataset                                                   .\n",
      "..............................................................................................\n",
      "This is Text-Fabric 8.5.13\n",
      "Api reference : https://annotation.github.io/text-fabric/tf/cheatsheet.html\n",
      "\n",
      "88 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Load the existing TF dataset\")\n",
    "TF = Fabric(locations=coreTf, modules=[\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-seeker",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.00s Dataset without structure sections in otext:no structure functions in the T-API\n",
      "  4.86s All features loaded/computed - for details use loadLog()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Computed',\n",
       "  'computed-data',\n",
       "  ('C Computed', 'Call AllComputeds', 'Cs ComputedString')),\n",
       " ('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString')),\n",
       " ('Fabric', 'loading', ('TF',)),\n",
       " ('Locality', 'locality', ('L Locality',)),\n",
       " ('Nodes', 'navigating-nodes', ('N Nodes',)),\n",
       " ('Features',\n",
       "  'node-features',\n",
       "  ('F Feature', 'Fall AllFeatures', 'Fs FeatureString')),\n",
       " ('Search', 'search', ('S Search',)),\n",
       " ('Text', 'text', ('T Text',))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = TF.load(\n",
    "    \"\"\"\n",
    "    lex gloss lex_utf8\n",
    "    sp vs lex uvf prs nametype ls\n",
    "    function rela typ\n",
    "    mother\n",
    "\"\"\"\n",
    ")\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-projector",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkShebanq = \"https://shebanq.ancient-data.org/hebrew/text\"\n",
    "linkPassage = \"?book={}&chapter={}&verse={}\"\n",
    "linkAppearance = \"&version={}&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x\".format(\n",
    "    VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDir = \"{}/results\".format(thisTemp)\n",
    "allResults = \"{}/all.csv\".format(resultDir)\n",
    "selectedResults = \"{}/selected.csv\".format(resultDir)\n",
    "kinds = (\"corr_blank\", \"corr_filled\", \"enrich_blank\", \"enrich_filled\")\n",
    "kdir = {}\n",
    "for k in kinds:\n",
    "    kd = \"{}/{}\".format(thisSource, k)\n",
    "    kdir[k] = kd\n",
    "    if not os.path.exists(kd):\n",
    "        os.makedirs(kd)\n",
    "if not os.path.exists(resultDir):\n",
    "    os.makedirs(resultDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def vfile(verb, kind):\n",
    "    if kind not in kinds:\n",
    "        utils.caption(0, \"ERROR: Unknown kind `{}`\".format(kind))\n",
    "        return None\n",
    "    baseName = verb.replace(\">\", \"a\").replace(\"<\", \"o\")\n",
    "    return (baseName, \"{}/{}.csv\".format(kdir[kind], baseName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain\n",
    "Here is a subset of verbs that interest us.\n",
    "In fact, we are interested in all verbs, but we have subjected the occurrences of these verbs to closer inspection,\n",
    "together with the contexts they occur in.\n",
    "\n",
    "Manual additions in the correction and enrichment workflow can only happen for selected verbs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-cameroon",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[8]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_initial = set(\n",
    "    \"\"\"\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_verbs = set(\n",
    "    \"\"\"\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_object_verbs = set(\n",
    "    \"\"\"\n",
    "    NTN\n",
    "    <FH\n",
    "    FJM\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_qal_verbs = set(\n",
    "    \"\"\"\n",
    "    NF>\n",
    "    PQD\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "verbs = verbs_initial | motion_verbs | double_object_verbs | complex_qal_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Correction workflow\n",
    "\n",
    "## 1.1 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-cache",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[9]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "predicate_functions = {\n",
    "    \"Pred\",\n",
    "    \"PreS\",\n",
    "    \"PreO\",\n",
    "    \"PreC\",\n",
    "    \"PtcO\",\n",
    "    \"PrcS\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-psychology",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[10]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "legal_values = dict(\n",
    "    function={F.function.v(p) for p in F.otype.s(\"phrase\")},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-grammar",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[11]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "error_values = dict(\n",
    "    function=dict(\n",
    "        BoundErr=\"this constituent is part of another constituent and does not merit its own function/type/rela value\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the `error_values` to the legal values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-cathedral",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[12]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         37s {'function': {'Frnt', 'Conj', 'Modi', 'BoundErr', 'PrcS', 'Objc', 'Subj', 'NCop', 'Pred', 'PreS', 'Cmpl', 'Intj', 'Nega', 'Adju', 'PreC', 'Supp', 'Ques', 'Loca', 'NCoS', 'PreO', 'IntS', 'Rela', 'ModS', 'PtcO', 'PrAd', 'Exst', 'EPPr', 'Voct', 'ExsS', 'Time'}}\n"
     ]
    }
   ],
   "source": [
    "for feature in set(legal_values.keys()) | set(error_values.keys()):\n",
    "    ev = error_values.get(feature, {})\n",
    "    if ev:\n",
    "        lv = legal_values.setdefault(feature, set())\n",
    "        lv |= set(ev.keys())\n",
    "if not SCRIPT:\n",
    "    utils.caption(0, \"{}\".format(legal_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-replication",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[13]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         46s Finding occurrences ...                                                        .\n",
      "..............................................................................................\n",
      "|         48s \tDone\n",
      "|         48s \tAll:      1380 verbs with  73710 verb occurrences in 70150 clauses\n",
      "|         48s \tSelected:   18 verbs with  16209 verb occurrences in 16052 clauses\n",
      "|         48s \t<BR   556 occurrences of which   33 outside a predicate phrase\n",
      "|         48s \t<FH  2629 occurrences of which   59 outside a predicate phrase\n",
      "|         48s \t<LH   890 occurrences of which   10 outside a predicate phrase\n",
      "|         48s \tBR>    54 occurrences of which    3 outside a predicate phrase\n",
      "|         48s \tBW>  2570 occurrences of which   27 outside a predicate phrase\n",
      "|         48s \tCJT    85 occurrences of which    1 outside a predicate phrase\n",
      "|         48s \tCWB  1056 occurrences of which   22 outside a predicate phrase\n",
      "|         48s \tFJM   609 occurrences of which    3 outside a predicate phrase\n",
      "|         48s \tHLK  1554 occurrences of which   32 outside a predicate phrase\n",
      "|         48s \tJRD   377 occurrences of which   16 outside a predicate phrase\n",
      "|         48s \tJY>  1069 occurrences of which   32 outside a predicate phrase\n",
      "|         48s \tNF>   656 occurrences of which   53 outside a predicate phrase\n",
      "|         48s \tNPL   445 occurrences of which   11 outside a predicate phrase\n",
      "|         48s \tNTN  2017 occurrences of which   10 outside a predicate phrase\n",
      "|         48s \tNWS   159 occurrences of which    4 outside a predicate phrase\n",
      "|         48s \tPQD   303 occurrences of which   72 outside a predicate phrase\n",
      "|         48s \tQR>   883 occurrences of which   12 outside a predicate phrase\n",
      "|         48s \tSWR   297 occurrences of which    1 outside a predicate phrase\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Finding occurrences ...\")\n",
    "occs = collections.defaultdict(\n",
    "    list\n",
    ")  # dictionary of verb occurrence nodes per verb lexeme\n",
    "npoccs = collections.defaultdict(list)  # same, but those not occurring in a \"predicate\"\n",
    "clause_verb = collections.defaultdict(\n",
    "    list\n",
    ")  # dictionary of verb occurrence nodes per clause node\n",
    "sel_clause_verb = collections.defaultdict(\n",
    "    list\n",
    ")  # dictionary of selected verb occurrence nodes per clause node\n",
    "clause_verb_index = collections.defaultdict(\n",
    "    set\n",
    ")  # mapping from clauses to its main verb(s)\n",
    "sel_clause_verb_index = collections.defaultdict(\n",
    "    set\n",
    ")  # mapping from clauses to its main verb(s), for selected verbs\n",
    "verb_clause_index = collections.defaultdict(\n",
    "    list\n",
    ")  # mapping from verbs to the clauses of which it is main verb\n",
    "sel_verb_clause_index = collections.defaultdict(\n",
    "    list\n",
    ")  # mapping from selected verbs to the clauses of which it is main verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = 0\n",
    "sel_nw = 0\n",
    "for w in F.otype.s(\"word\"):\n",
    "    if F.sp.v(w) != \"verb\":\n",
    "        continue\n",
    "    lex = F.lex.v(w).rstrip(\"[=\")\n",
    "    nw += 1\n",
    "    pf = F.function.v(L.u(w, \"phrase\")[0])\n",
    "    if pf not in predicate_functions:\n",
    "        npoccs[lex].append(w)\n",
    "    occs[lex].append(w)\n",
    "    cn = L.u(w, \"clause\")[0]\n",
    "    clause_verb[cn].append(w)\n",
    "    clause_verb_index[cn].add(lex)\n",
    "    verb_clause_index[lex].append(cn)\n",
    "    if lex in verbs:\n",
    "        sel_nw += 1\n",
    "        sel_clause_verb[cn].append(w)\n",
    "        sel_clause_verb_index[cn].add(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_verb_clause_index = dict(\n",
    "    (lex, cns) for (lex, cns) in verb_clause_index.items() if lex in verbs\n",
    ")\n",
    "sel_clause_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(0, \"\\tDone\")\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"\\tAll:      {:>4} verbs with {:>6} verb occurrences in {} clauses\".format(\n",
    "        len(verb_clause_index), nw, len(clause_verb)\n",
    "    ),\n",
    ")\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"\\tSelected: {:>4} verbs with {:>6} verb occurrences in {} clauses\".format(\n",
    "        len(sel_verb_clause_index), sel_nw, len(sel_clause_verb)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-significance",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for verb in sorted(verbs):\n",
    "    utils.caption(\n",
    "        0,\n",
    "        \"\\t{} {:>5} occurrences of which {:>4} outside a predicate phrase\".format(\n",
    "            verb,\n",
    "            len(occs[verb]),\n",
    "            len(npoccs[verb]),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC transliterated consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-estimate",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[14]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(4, \"Generating blank correction sheets ...\")\n",
    "sheetKind = \"corr_blank\"\n",
    "utils.caption(0, \"\\tas {}\".format(vfile(\"{verb}\", sheetKind)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_seen = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = \";\"\n",
    "    field_names = \"\"\"\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    \"\"\".strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u(wn, \"clause\")[0]\n",
    "        if cln in clauses_seen:\n",
    "            continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u(wn, \"verse\")[0]\n",
    "        (bookName, ch, vs) = T.sectionFromNode(vn, lang=\"la\")\n",
    "        passage_label = \"{} {}:{}\".format(*T.sectionFromNode(vn))\n",
    "        ln = linkShebanq + (linkPassage.format(bookName, ch, vs)) + linkAppearance\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.text([wn], fmt=\"text-trans-plain\")\n",
    "        vstem = F.vs.v(wn)\n",
    "        np = \"* \" if wn in npoccs[verb] else \"\"\n",
    "        row = [cln, wn, passage_label, lnx, np + vt, vstem]\n",
    "        phrases = L.d(cln, \"phrase\")\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases:\n",
    "            max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            phrases_seen[pn] += 1\n",
    "            pt = T.text(L.d(pn, \"word\"), fmt=\"text-trans-plain\")\n",
    "            pf = F.function.v(pn)\n",
    "            pnp = np if pf in predicate_functions else \"\"\n",
    "            row.extend((pn, pnp + pt, pf, \"\"))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend(\n",
    "            \"\"\"\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        \"\"\".format(\n",
    "                i=i + 1\n",
    "            )\n",
    "            .strip()\n",
    "            .split()\n",
    "        )\n",
    "    location = vfile(verb, sheetKind)\n",
    "    if location is None:\n",
    "        return\n",
    "    (baseName, fileName) = location\n",
    "    row_file = open(fileName, \"w\")\n",
    "    row_file.write(\"{}\\n\".format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write(\"{}\\n\".format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    utils.caption(0, \"\\t\\tfor verb {}\".format(baseName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "for verb in verbs:\n",
    "    gen_sheet(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      1m 06s Generating blank correction sheets ...                                         .\n",
      "..............................................................................................\n",
      "|      1m 06s \tas /Users/dirk/github/etcbc/valence/source/2021/corr_blank/{verb}.csv\n",
      "|      1m 06s \t\tfor verb oFH\n",
      "|      1m 06s \t\tfor verb PQD\n",
      "|      1m 06s \t\tfor verb BWa\n",
      "|      1m 07s \t\tfor verb NFa\n",
      "|      1m 07s \t\tfor verb NPL\n",
      "|      1m 07s \t\tfor verb QRa\n",
      "|      1m 07s \t\tfor verb HLK\n",
      "|      1m 07s \t\tfor verb SWR\n",
      "|      1m 07s \t\tfor verb FJM\n",
      "|      1m 07s \t\tfor verb NWS\n",
      "|      1m 07s \t\tfor verb CWB\n",
      "|      1m 07s \t\tfor verb oLH\n",
      "|      1m 08s \t\tfor verb NTN\n",
      "|      1m 08s \t\tfor verb BRa\n",
      "|      1m 08s \t\tfor verb CJT\n",
      "|      1m 08s \t\tfor verb JYa\n",
      "|      1m 08s \t\tfor verb oBR\n",
      "|      1m 08s \t\tfor verb JRD\n",
      "|      1m 08s \t52060  phrases seen 1  time(s)\n",
      "|      1m 08s \t185    phrases seen 2  time(s)\n",
      "|      1m 08s \t9      phrases seen 3  time(s)\n",
      "|      1m 08s \tTotal phrases seen: 52254\n"
     ]
    }
   ],
   "source": [
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items():\n",
    "    stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    utils.caption(0, \"\\t{:<6} phrases seen {:<2} time(s)\".format(n, times))\n",
    "utils.caption(0, \"\\tTotal phrases seen: {}\".format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-evolution",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[15]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(4, \"Processing filled correction sheets ...\")\n",
    "sheetKind = \"corr_filled\"\n",
    "utils.caption(0, \"\\tas {}\".format(vfile(\"{verb}\", sheetKind)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "pf_corr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corr():\n",
    "    function_values = legal_values[\"function\"]\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        repeated = collections.defaultdict(list)\n",
    "        non_phrase = set()\n",
    "        illegal_fvalue = set()\n",
    "        nodeNumberErrors = []\n",
    "\n",
    "        location = vfile(verb, sheetKind)\n",
    "        if location is None:\n",
    "            continue\n",
    "        (baseName, fileName) = location\n",
    "        if not os.path.exists(fileName):\n",
    "            utils.caption(0, \"\\t\\tNO file for {}\".format(baseName))\n",
    "            continue\n",
    "        else:\n",
    "            utils.caption(0, \"\\t\\tverb {}\".format(baseName))\n",
    "        with open(fileName) as f:\n",
    "            for (i, line) in enumerate(f):\n",
    "                fields = line.rstrip().split(\";\")\n",
    "                cn = int(fields[0])\n",
    "                wn = int(fields[1])\n",
    "                if F.otype.v(cn) != \"clause\":\n",
    "                    nodeNumberErrors.append([i, \"{} is not a clause node\".format(cn)])\n",
    "                if F.otype.v(wn) != \"word\":\n",
    "                    nodeNumberErrors.append([i, \"{} is not a word node\".format(wn)])\n",
    "                words = set(L.d(cn, \"word\"))\n",
    "                phrases = set(L.d(cn, \"phrase\"))\n",
    "                if wn not in words:\n",
    "                    nodeNumberErrors.append(\n",
    "                        [i, \"{} is not a word of clause {}\".format(wn, cn)]\n",
    "                    )\n",
    "                for i in range(1, len(fields) // 4):\n",
    "                    (pn, pc) = (fields[2 + 4 * i], fields[2 + 4 * i + 3])\n",
    "                    if pn != \"\":\n",
    "                        pn = int(pn)\n",
    "                        if F.otype.v(pn) != \"phrase\":\n",
    "                            nodeNumberErrors.append(\n",
    "                                [i, \"{} is not a phrase node\".format(pn)]\n",
    "                            )\n",
    "                        if pn not in phrases:\n",
    "                            nodeNumberErrors.append(\n",
    "                                [i, \"{} is not a phrase of clause {}\".format(pn, cn)]\n",
    "                            )\n",
    "                        pc = pc.strip()\n",
    "                        phrases_seen[pn] += 1\n",
    "                        if pc != \"\":\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        utils.caption(\n",
    "            0,\n",
    "            \"\\t{}: Found {:>5} corrections in {}\".format(verb, len(pf_corr), fileName),\n",
    "        )\n",
    "        if len(nodeNumberErrors):\n",
    "            for (i, msg) in nodeNumberErrors:\n",
    "                utils.caption(0, \"ERROR: Line {:>3}: {}\".format(i + 1, msg))\n",
    "        else:\n",
    "            utils.caption(0, \"\\tOK: node numbers in sheet are consistent\")\n",
    "        if len(repeated):\n",
    "            utils.caption(0, \"ERROR: Some phrases have been corrected multiple times!\")\n",
    "            for x in sorted(repeated):\n",
    "                utils.caption(0, \"\\t{:>6}: {}\".format(x, \", \".join(repeated[x])))\n",
    "        else:\n",
    "            utils.caption(\n",
    "                0, \"\\tOK: Corrected phrases did not receive multiple corrections\"\n",
    "            )\n",
    "        if len(non_phrase):\n",
    "            utils.caption(\n",
    "                0,\n",
    "                \"ERROR: Corrections have been applied to non-phrase nodes: {}\".format(\n",
    "                    \",\".join(non_phrase)\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            utils.caption(0, \"\\tOK: all corrected nodes where phrase nodes\")\n",
    "        if len(illegal_fvalue):\n",
    "            utils.caption(\n",
    "                0, \"ERROR: Some corrections supply illegal values for phrase function!\"\n",
    "            )\n",
    "            utils.caption(0, \"\\t`{}`\".format(\"`, `\".join(illegal_fvalue)))\n",
    "        else:\n",
    "            utils.caption(0, \"\\tOK: all corrected values are legal\")\n",
    "    utils.caption(\n",
    "        0, \"\\tFound {} corrections in the phrase function\".format(len(pf_corr))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      1m 21s Processing filled correction sheets ...                                        .\n",
      "..............................................................................................\n",
      "|      1m 21s \tas /Users/dirk/github/etcbc/valence/source/2021/corr_filled/{verb}.csv\n",
      "|      1m 21s \t\tNO file for oBR\n",
      "|      1m 21s \t\tNO file for oFH\n",
      "|      1m 21s \t\tNO file for oLH\n",
      "|      1m 21s \t\tNO file for BRa\n",
      "|      1m 21s \t\tNO file for BWa\n",
      "|      1m 21s \t\tNO file for CJT\n",
      "|      1m 21s \t\tNO file for CWB\n",
      "|      1m 21s \t\tNO file for FJM\n",
      "|      1m 21s \t\tNO file for HLK\n",
      "|      1m 21s \t\tNO file for JRD\n",
      "|      1m 21s \t\tNO file for JYa\n",
      "|      1m 21s \t\tNO file for NFa\n",
      "|      1m 21s \t\tNO file for NPL\n",
      "|      1m 21s \t\tNO file for NTN\n",
      "|      1m 21s \t\tNO file for NWS\n",
      "|      1m 21s \t\tNO file for PQD\n",
      "|      1m 21s \t\tNO file for QRa\n",
      "|      1m 21s \t\tNO file for SWR\n",
      "|      1m 21s \tFound 0 corrections in the phrase function\n",
      "|      1m 21s \tTotal phrases seen: 0\n"
     ]
    }
   ],
   "source": [
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items():\n",
    "    stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    utils.caption(0, \"\\t{:<6} phrases seen {:<2} time(s)\".format(n, times))\n",
    "utils.caption(0, \"\\tTotal phrases seen: {}\".format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Enrichment workflow\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-suite",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[16]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      1m 50s 6 Enrich field specifications OK                                               .\n",
      "..............................................................................................\n",
      "|      1m 50s \tgrammatical has possible values\n",
      "|      1m 50s \t\tdirect_object\n",
      "|      1m 50s \t\tindirect_object\n",
      "|      1m 50s \t\tsubject\n",
      "|      1m 50s \t\tNP_direct_object\n",
      "|      1m 50s \t\tinfinitive_object\n",
      "|      1m 50s \t\tNA\n",
      "|      1m 50s \t\tL_object\n",
      "|      1m 50s \t\t*\n",
      "|      1m 50s \t\tprincipal_direct_object\n",
      "|      1m 50s \t\tK_object\n",
      "|      1m 50s \tlexical has possible values\n",
      "|      1m 50s \t\tlocation\n",
      "|      1m 50s \t\ttime\n",
      "|      1m 50s \toriginal has possible values\n",
      "|      1m 50s \t\tdirect_object\n",
      "|      1m 50s \t\tindirect_object\n",
      "|      1m 50s \t\tsubject\n",
      "|      1m 50s \t\tNP_direct_object\n",
      "|      1m 50s \t\tinfinitive_object\n",
      "|      1m 50s \t\tNA\n",
      "|      1m 50s \t\tL_object\n",
      "|      1m 50s \t\t*\n",
      "|      1m 50s \t\tprincipal_direct_object\n",
      "|      1m 50s \t\tK_object\n",
      "|      1m 50s \tpredication has possible values\n",
      "|      1m 50s \t\tcopula\n",
      "|      1m 50s \t\tNA\n",
      "|      1m 50s \t\tregular\n",
      "|      1m 50s \tsemantic has possible values\n",
      "|      1m 50s \t\tbenefactive\n",
      "|      1m 50s \t\tlocation\n",
      "|      1m 50s \t\tinstrument\n",
      "|      1m 50s \t\tmanner\n",
      "|      1m 50s \t\ttime\n",
      "|      1m 50s \tvalence has possible values\n",
      "|      1m 50s \t\tcomplement\n",
      "|      1m 50s \t\tcore\n",
      "|      1m 50s \t\tadjunct\n"
     ]
    }
   ],
   "source": [
    "enrich_field_spec = \"\"\"\n",
    "valence\n",
    "    adjunct\n",
    "    complement\n",
    "    core\n",
    "\n",
    "predication\n",
    "    NA\n",
    "    regular\n",
    "    copula\n",
    "\n",
    "grammatical\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "original\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "lexical\n",
    "    location\n",
    "    time\n",
    "\n",
    "semantic\n",
    "    benefactive\n",
    "    time\n",
    "    location\n",
    "    instrument\n",
    "    manner\n",
    "\"\"\"\n",
    "enrich_fields = collections.OrderedDict()\n",
    "cur_e = None\n",
    "for line in enrich_field_spec.strip().split(\"\\n\"):\n",
    "    if line.startswith(\" \"):\n",
    "        enrich_fields.setdefault(cur_e, set()).add(line.strip())\n",
    "    else:\n",
    "        cur_e = line.strip()\n",
    "nef = len(enrich_fields)\n",
    "if None in enrich_fields:\n",
    "    utils.caption(0, \"ERROR: Invalid enrich field specification\")\n",
    "else:\n",
    "    utils.caption(4, \"{} Enrich field specifications OK\".format(nef))\n",
    "for (ef, fields) in sorted(enrich_fields.items()):\n",
    "    utils.caption(0, \"\\t{} has possible values\".format(ef))\n",
    "    for field in fields:\n",
    "        utils.caption(0, \"\\t\\t{}\".format(field))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-question",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[17]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "enrich_baseline_rules = dict(\n",
    "    phrase=\"\"\"Adju\tAdjunct\tadjunct\tNA\tNA\t\t\t\n",
    "Cmpl\tComplement\tcomplement\tNA\t*\t\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\t\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\tNA\t\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Exst\tExistence\tcore\tcopula\tNA\t\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\t\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\t\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tNA\t\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\t\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\tNA\t\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\t\tNA\tNA\n",
    "Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\tNA\t\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PreC\tPredicate complement\tcore\tregular\tNA\t\t\t\n",
    "Pred\tPredicate\tcore\tregular\tNA\t\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\t\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\t\tNA\tNA\n",
    "Subj\tSubject\tcore\tNA\tsubject\t\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\tNA\t\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\tNA\t\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\t\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\t\tNA\tNA\"\"\",  # noqa W291\n",
    "    clause=\"\"\"Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "InfC\tInfinitive Construct clause\tNA\tNA\t\t\t\t\"\"\",  # noqa W291\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-progressive",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[18]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(4, \"\\tChecking enrich baseline rules\")\n",
    "transform = collections.OrderedDict(((\"phrase\", {}), (\"clause\", {})))\n",
    "errors = 0\n",
    "good = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      2m 04s \tChecking enrich baseline rules                                                .\n",
      "..............................................................................................\n",
      "|      2m 04s \tEnrich baseline rules are OK (204 good)\n"
     ]
    }
   ],
   "source": [
    "for kind in (\"phrase\", \"clause\"):\n",
    "    for line in enrich_baseline_rules[kind].split(\"\\n\"):\n",
    "        x = line.split(\"\\t\")\n",
    "        nefields = len(x) - 2\n",
    "        if len(x) - 2 != nef:\n",
    "            utils.caption(\n",
    "                0,\n",
    "                \"ERROR: Wrong number of fields ({} must be {}) in {}:\\n{}\".format(\n",
    "                    nefields, nef, kind, line\n",
    "                ),\n",
    "            )\n",
    "            errors += 1\n",
    "        transform[kind][x[0]] = dict(zip(enrich_fields, x[2:]))\n",
    "    for e in error_values[\"function\"]:\n",
    "        transform[kind][e] = dict(zip(enrich_fields, [\"\"] * nef))\n",
    "\n",
    "    for f in transform[kind]:\n",
    "        for e in enrich_fields:\n",
    "            val = transform[kind][f][e]\n",
    "            if val != \"\" and val != \"NA\" and val not in enrich_fields[e]:\n",
    "                utils.caption(\n",
    "                    0,\n",
    "                    'ERROR: Defaults for `{}` ({}): wrong `{}` value: \"{}\"'.format(\n",
    "                        f, kind, e, val\n",
    "                    ),\n",
    "                )\n",
    "                errors += 1\n",
    "            else:\n",
    "                good += 1\n",
    "if errors:\n",
    "    utils.caption(0, \"ERROR: There were {} errors ({} good)\".format(errors, good))\n",
    "else:\n",
    "    utils.caption(0, \"\\tEnrich baseline rules are OK ({} good)\".format(good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us pretty-print the baseline rules of enrichment for easier reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-final",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[19]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func    : valence        predication    grammatical    original       lexical        semantic       \n",
      "[phrase]\n",
      "Adju    : adjunct        NA             NA                                                          \n",
      "BoundErr:                                                                                           \n",
      "Cmpl    : complement     NA             *                                                           \n",
      "Conj    : NA             NA             NA                            NA             NA             \n",
      "EPPr    : NA             copula         NA                                                          \n",
      "ExsS    : core           copula         subject                                                     \n",
      "Exst    : core           copula         NA                                                          \n",
      "Frnt    : NA             NA             NA                            NA             NA             \n",
      "IntS    : core           NA             subject                                                     \n",
      "Intj    : NA             NA             NA                            NA             NA             \n",
      "Loca    : adjunct        NA             NA                            location       location       \n",
      "ModS    : core           NA             subject                                                     \n",
      "Modi    : NA             NA             NA                            NA             NA             \n",
      "NCoS    : core           copula         subject                                                     \n",
      "NCop    : core           copula         NA                                                          \n",
      "Nega    : NA             NA             NA                            NA             NA             \n",
      "Objc    : complement     NA             direct_object                                               \n",
      "PrAd    : adjunct        NA             NA                                                          \n",
      "PrcS    : core           regular        subject                                                     \n",
      "PreC    : core           regular        NA                                                          \n",
      "PreO    : core           regular        direct_object                                               \n",
      "PreS    : core           regular        subject                                                     \n",
      "Pred    : core           regular        NA                                                          \n",
      "PtcO    : core           regular        direct_object                                               \n",
      "Ques    : NA             NA             NA                            NA             NA             \n",
      "Rela    : NA             NA             NA                            NA             NA             \n",
      "Subj    : core           NA             subject                                                     \n",
      "Supp    : adjunct        NA             NA                                           benefactive    \n",
      "Time    : adjunct        NA             NA                            time           time           \n",
      "Unkn    : NA             NA             NA                            NA             NA             \n",
      "Voct    : NA             NA             NA                            NA             NA             \n",
      "[clause]\n",
      "BoundErr:                                                                                           \n",
      "InfC    : NA             NA                                                                         \n",
      "Objc    : complement     NA             direct_object                                               \n"
     ]
    }
   ],
   "source": [
    "if not SCRIPT:\n",
    "    ltpl = \"{:<8}: \" + (\"{:<15}\" * nef)\n",
    "    utils.caption(0, ltpl.format(\"func\", *enrich_fields), continuation=True)\n",
    "    for kind in transform:\n",
    "        utils.caption(0, \"[{}]\".format(kind), continuation=True)\n",
    "        for f in sorted(transform[kind]):\n",
    "            sfs = transform[kind][f]\n",
    "            utils.caption(\n",
    "                0, ltpl.format(f, *[sfs[sf] for sf in enrich_fields]), continuation=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Enrichment logic\n",
    "\n",
    "We apply enrichment logic to *all* verbs, not only to selected verbs.\n",
    "But only selected verbs can receive manual enrichment enhancements.\n",
    "\n",
    "For some verbs, selected or not, additional logic specific to that verb can be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Direct objects\n",
    "\n",
    "We have to do some work to identify (multiple) direct objects and indirect objects.\n",
    "\n",
    "[More on direct objects](https://github.com/ETCBC/valence/wiki/Discussion#direct-objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-history",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[20]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectfuncs = set(\n",
    "    \"\"\"\n",
    "Objc PreO PtcO\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_as_obj_preps = set(\n",
    "    \"\"\"\n",
    "K L\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "no_prs = set(\n",
    "    \"\"\"\n",
    "absent n/a\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-stomach",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[21]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "body_parts = set(\n",
    "    \"\"\"\n",
    ">NP/ >P/ >PSJM/ >YB</ >ZN/\n",
    "<JN/ <NQ/ <RP/ <YM/ <YM==/\n",
    "BHN/ BHWN/ BVN/\n",
    "CD=/ CD===/ CKM/ CN/\n",
    "DD/\n",
    "GRGRT/ GRM/ GRWN/ GW/ GW=/ GWJH/ GWPH/ GXWN/\n",
    "FPH/\n",
    "JD/ JRK/ JRKH/\n",
    "KRF/ KSL=/ KTP/\n",
    "L</ LCN/ LCWN/ LXJ/\n",
    "M<H/ MPRQT/ MTL<WT/ MTNJM/ MYX/\n",
    "NBLH=/\n",
    "P<M/ PGR/ PH/ PM/ PNH/ PT=/\n",
    "QRSL/\n",
    "R>C/ RGL/\n",
    "XDH/ XLY/ XMC=/ XRY/\n",
    "YW>R/\n",
    "ZRW</\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-macedonia",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[24]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(4, \"Finding direct objects and determining the principal one\")\n",
    "clause_objects = collections.defaultdict(set)\n",
    "objects = collections.defaultdict(set)\n",
    "objects_count = collections.defaultdict(collections.Counter)\n",
    "object_kinds = (\n",
    "    \"principal\",\n",
    "    \"direct\",\n",
    "    \"NP\",\n",
    "    \"L\",\n",
    "    \"K\",\n",
    "    \"clause\",\n",
    "    \"infinitive\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_marked(phr):\n",
    "    # simple criterion for determining whether a direct object is marked:\n",
    "    # has it the object marker somewhere?\n",
    "    words = L.d(p, \"word\")\n",
    "    has_et = False\n",
    "    for w in words:\n",
    "        if F.lex.v(w) == \">T\":\n",
    "            has_et = True\n",
    "            break\n",
    "    return has_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in clause_verb:\n",
    "    these_objects = collections.defaultdict(set)\n",
    "    direct_objects_cat = collections.defaultdict(set)\n",
    "\n",
    "    for p in L.d(c, \"phrase\"):\n",
    "        pf = pf_corr.get(\n",
    "            p, F.function.v(p)\n",
    "        )  # NB we take the corrected value for phrase function if there is one\n",
    "        if pf in objectfuncs:\n",
    "            direct_objects_cat[\"p_\" + pf].add(p)\n",
    "            these_objects[\"direct\"].add(p)\n",
    "        elif pf == \"Cmpl\":\n",
    "            pwords = L.d(p, \"word\")\n",
    "            w1 = pwords[0]\n",
    "            w1l = F.lex.v(w1)\n",
    "            w2l = F.lex.v(pwords[1]) if len(pwords) > 1 else None\n",
    "            if (\n",
    "                w1l in cmpl_as_obj_preps\n",
    "                and F.prs.v(w1) in no_prs\n",
    "                and not (w1l == \"L\" and w2l in body_parts)\n",
    "            ):\n",
    "                if w1l == \"K\":\n",
    "                    these_objects[\"K\"].add(p)\n",
    "                elif w1l == \"L\":\n",
    "                    these_objects[\"L\"].add(p)\n",
    "\n",
    "    # find clause objects\n",
    "    for ac in L.d(L.u(c, \"sentence\")[0], \"clause\"):\n",
    "        mothers = list(E.mother.f(ac))\n",
    "        if not (mothers and mothers[0] == c):\n",
    "            continue\n",
    "        cr = F.rela.v(ac)\n",
    "        ct = F.typ.v(ac)\n",
    "        if cr in {\"Objc\"} or ct in {\"InfC\"}:\n",
    "            clause_objects[c].add(ac)\n",
    "            if cr in {\"Objc\"}:\n",
    "                label = cr\n",
    "                direct_objects_cat[\"c_\" + label].add(ac)\n",
    "                these_objects[\"direct\"].add(ac)\n",
    "                these_objects[\"clause\"].add(ac)\n",
    "            elif ct in {\"InfC\"}:\n",
    "                if F.lex.v(L.d(ac, \"word\")[0]) == \"L\":\n",
    "                    these_objects[\"infinitive\"].add(ac)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # order the objects in the natural ordering\n",
    "    direct_objects_order = N.sortNodes(these_objects.get(\"direct\", set()))\n",
    "    nobjects = len(direct_objects_order)\n",
    "\n",
    "    # compute the principal object\n",
    "    principal_object = None\n",
    "\n",
    "    for x in [1]:\n",
    "        # just one object\n",
    "        if nobjects == 1:\n",
    "            # we have chosen not to mark a principal object if there is only one object\n",
    "            # the alternative is to mark it if it is a phrase. Uncomment the next 2 lines if you want this\n",
    "            # theobject = list(dobjects_set)[0]\n",
    "            # if F.otype.v(theobject) == 'phrase': principal_object = theobject\n",
    "            break\n",
    "        # rule 1: suffixes and promoted objects\n",
    "        principal_candidates = direct_objects_cat.get(\n",
    "            \"p_PreO\", set()\n",
    "        ) | direct_objects_cat.get(\"p_PtcO\", set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            principal_object = N.sortNodes(principal_candidates)[0]\n",
    "            break\n",
    "        principal_candidates = direct_objects_cat.get(\"p_Objc\", set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            if len(principal_candidates) == 1:\n",
    "                principal_object = list(principal_candidates)[0]\n",
    "                break\n",
    "            objects_marked = set()\n",
    "            objects_unmarked = set()\n",
    "            for p in principal_candidates:\n",
    "                if is_marked(p):\n",
    "                    objects_marked.add(p)\n",
    "                else:\n",
    "                    objects_unmarked.add(p)\n",
    "            if len(objects_marked) != 0:\n",
    "                principal_object = N.sortNodes(objects_marked)[0]\n",
    "                break\n",
    "            if len(objects_unmarked) != 0:\n",
    "                principal_object = N.sortNodes(objects_unmarked)[0]\n",
    "                break\n",
    "    if principal_object is not None:\n",
    "        these_objects[\"principal\"].add(principal_object)\n",
    "    if len(these_objects[\"infinitive\"]) and not len(these_objects[\"direct\"]):\n",
    "        # we do not mark an infinitive object if there is no proper direct object around\n",
    "        these_objects[\"infinitive\"] = set()\n",
    "    if len(these_objects[\"principal\"]):\n",
    "        these_objects[\"direct\"] -= these_objects[\"principal\"]\n",
    "        for x in these_objects[\"direct\"] - these_objects[\"clause\"]:\n",
    "            # the NP objects are the non-principal phrase like direct objects\n",
    "            these_objects[\"NP\"].add(x)\n",
    "        these_objects[\"direct\"] -= these_objects[\"NP\"]\n",
    "    if (\n",
    "        len(these_objects[\"principal\"]) == 0\n",
    "        and len(these_objects[\"direct\"])\n",
    "        and (\n",
    "            len(these_objects[\"NP\"])\n",
    "            or len(these_objects[\"L\"])\n",
    "            or len(these_objects[\"K\"])\n",
    "            or len(these_objects[\"infinitive\"])\n",
    "        )\n",
    "    ):  # promote the direct objects to principal direct objects\n",
    "        these_objects[\"principal\"] = these_objects[\"direct\"]\n",
    "        these_objects[\"direct\"] = set()\n",
    "\n",
    "    for kind in object_kinds:\n",
    "        n = len(these_objects.get(kind, set()))\n",
    "        objects_count[kind][n] += 1\n",
    "        if n:\n",
    "            objects[kind] |= these_objects[kind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      6m 22s Generate blank enrichment sheets                                               .\n",
      "..............................................................................................\n",
      "|      6m 22s \tas /Users/dirk/github/etcbc/valence/source/2021/enrich_blank/{verb}.csv\n",
      "|      6m 23s \t\tfor verb <FH (11315 rows)\n",
      "|      6m 23s \t\tfor verb PQD ( 1283 rows)\n",
      "|      6m 23s \t\tfor verb BW> (11087 rows)\n",
      "|      6m 24s \t\tfor verb NF> ( 2886 rows)\n",
      "|      6m 24s \t\tfor verb NPL ( 1933 rows)\n",
      "|      6m 24s \t\tfor verb QR> ( 3726 rows)\n",
      "|      6m 24s \t\tfor verb HLK ( 5814 rows)\n",
      "|      6m 24s \t\tfor verb SWR ( 1272 rows)\n",
      "|      6m 24s \t\tfor verb FJM ( 2918 rows)\n",
      "|      6m 24s \t\tfor verb NWS (  618 rows)\n",
      "|      6m 24s \t\tfor verb CWB ( 4325 rows)\n",
      "|      6m 25s \t\tfor verb <LH ( 3893 rows)\n",
      "|      6m 25s \t\tfor verb NTN ( 9822 rows)\n",
      "|      6m 25s \t\tfor verb BR> (  219 rows)\n",
      "|      6m 25s \t\tfor verb CJT (  381 rows)\n",
      "|      6m 25s \t\tfor verb JY> ( 4623 rows)\n",
      "|      6m 25s \t\tfor verb <BR ( 2349 rows)\n",
      "|      6m 25s \t\tfor verb JRD ( 1593 rows)\n",
      "|      6m 25s \tDone\n"
     ]
    }
   ],
   "source": [
    "utils.caption(0, \"\\tDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      3m 57s Finding direct objects and determining the principal one                       .\n",
      "..............................................................................................\n",
      "|      4m 00s \tDone\n",
      "|      4m 00s \t 3649 clauses with  1  principal object\n",
      "|      4m 00s \t66501 clauses with  0  principal objects\n",
      "|      4m 00s \t 3649 clauses with  a  principal object\n",
      "|      4m 00s \t23694 clauses with  1     direct object\n",
      "|      4m 00s \t46456 clauses with  0     direct objects\n",
      "|      4m 00s \t23694 clauses with  a     direct object\n",
      "|      4m 00s \t 1001 clauses with  1         NP object\n",
      "|      4m 00s \t69149 clauses with  0         NP objects\n",
      "|      4m 00s \t 1001 clauses with  a         NP object\n",
      "|      4m 00s \t   33 clauses with  2          L objects\n",
      "|      4m 00s \t 3828 clauses with  1          L object\n",
      "|      4m 00s \t66289 clauses with  0          L objects\n",
      "|      4m 00s \t 3861 clauses with  a          L object\n",
      "|      4m 00s \t  115 clauses with  1          K object\n",
      "|      4m 00s \t70035 clauses with  0          K objects\n",
      "|      4m 00s \t  115 clauses with  a          K object\n",
      "|      4m 00s \t 1310 clauses with  1     clause object\n",
      "|      4m 00s \t68840 clauses with  0     clause objects\n",
      "|      4m 00s \t 1310 clauses with  a     clause object\n",
      "|      4m 00s \t    1 clauses with  3 infinitive objects\n",
      "|      4m 00s \t   18 clauses with  2 infinitive objects\n",
      "|      4m 00s \t 1196 clauses with  1 infinitive object\n",
      "|      4m 00s \t68935 clauses with  0 infinitive objects\n",
      "|      4m 00s \t 1215 clauses with  a infinitive object\n"
     ]
    }
   ],
   "source": [
    "for kind in object_kinds:\n",
    "    total = 0\n",
    "    for (count, n) in sorted(objects_count[kind].items(), key=lambda y: -y[0]):\n",
    "        if count:\n",
    "            total += n\n",
    "        utils.caption(\n",
    "            0,\n",
    "            \"\\t{:>5} clauses with {:>2} {:>10} object{}\".format(\n",
    "                n, count, kind, \"s\" if count != 1 else \"\"\n",
    "            ),\n",
    "        )\n",
    "    utils.caption(\n",
    "        0, \"\\t{:>5} clauses with {:>2} {:>10} object\".format(total, \"a\", kind)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Indirect objects\n",
    "\n",
    "The BHSA database has not feature that marks indirect objects.\n",
    "We will use computation to determine whether a complement is an indirect object or a locative.\n",
    "This computation is just an approximation.\n",
    "\n",
    "[More on indirect objects](https://github.com/ETCBC/valence/wiki/Discussion#indirect-objects)\n",
    "\n",
    "### The decision\n",
    "\n",
    "We take a decision as follows.\n",
    "Based on indicators $ind$ and $loc$ that are proxies for the degree in which the complement is an indirect object or a locative, we arrive at a decision $L$ (complement is *locative*) or $I$ (complement is *indirect object*) or $C$ (complement is neither *locative* nor *indirect object*) as follows:\n",
    "\n",
    "(1) $ loc > 0 \\wedge ind = 0 \\Rightarrow L $\n",
    "\n",
    "(2) $ loc = 0 \\wedge ind > 0 \\Rightarrow I $\n",
    "\n",
    "(3) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc - 1 > ind \\Rightarrow L$\n",
    "\n",
    "(4) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc + 1 < ind \\Rightarrow I$\n",
    "\n",
    "(5) $ loc > 0 \\wedge ind > 0 \\wedge |ind - loc| <= 1 \\Rightarrow C$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-supervisor",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[25]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "complfuncs = set(\n",
    "    \"\"\"\n",
    "Cmpl PreC\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "cmpl_as_iobj_preps = set(\n",
    "    \"\"\"\n",
    "L >L\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-bulletin",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[26]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "locative_lexemes = set(\n",
    "    \"\"\"\n",
    ">RY/ >YL/ >XR/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJN/ BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "\"\"\".strip().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-message",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "personal_lexemes = set(\n",
    "    \"\"\"\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/ MYRJ/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "\"\"\".strip().split()  # noqa W291\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-performance",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[27]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(4, \"Determinig kind of complements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "complements_c = collections.defaultdict(lambda: collections.defaultdict(lambda: []))\n",
    "complements = {}\n",
    "complementk = {}\n",
    "kcomplements = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "nphrases = 0\n",
    "ncomplements = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in clause_verb:\n",
    "    for p in L.d(c, \"phrase\"):\n",
    "        nphrases += 1\n",
    "        pf = pf_corr.get(p, F.function.v(p))\n",
    "        if pf not in complfuncs:\n",
    "            continue\n",
    "        ncomplements += 1\n",
    "        words = L.d(p, \"word\")\n",
    "        lexemes = [F.lex.v(w) for w in words]\n",
    "        lexeme_set = set(lexemes)\n",
    "\n",
    "        # measuring locativity\n",
    "        lex_locativity = len(locative_lexemes & lexeme_set)\n",
    "        prep_b = len([x for x in lexeme_set if x == \"B\"])\n",
    "        topo = len([x for x in words if F.nametype.v(x) == \"topo\"])\n",
    "        h_loc = len([x for x in words if F.uvf.v(x) == \"H\"])\n",
    "        body_part = 0\n",
    "        if (\n",
    "            len(words) > 1\n",
    "            and F.lex.v(words[0]) == \"L\"\n",
    "            and F.lex.v(words[1]) in body_parts\n",
    "        ):\n",
    "            body_part = 2\n",
    "        loca = lex_locativity + topo + prep_b + h_loc + body_part\n",
    "\n",
    "        # measuring indirect object\n",
    "        prep_l = len(\n",
    "            [\n",
    "                x\n",
    "                for x in words\n",
    "                if F.lex.v(x) in cmpl_as_iobj_preps and F.prs.v(x) not in no_prs\n",
    "            ]\n",
    "        )\n",
    "        prep_lpr = 0\n",
    "        lwn = len(words)\n",
    "        for (n, wn) in enumerate(words):\n",
    "            if F.lex.v(wn) in cmpl_as_iobj_preps:\n",
    "                if n + 1 < lwn:\n",
    "                    nextw = words[n + 1]\n",
    "                    if (\n",
    "                        F.lex.v(nextw) in personal_lexemes\n",
    "                        or F.ls.v(nextw) == \"gntl\"\n",
    "                        or (F.sp.v(nextw) == \"nmpr\" and F.nametype.v(nextw) == \"pers\")\n",
    "                    ):\n",
    "                        prep_lpr += 1\n",
    "        indi = prep_l + prep_lpr\n",
    "\n",
    "        # the verdict\n",
    "        ckind = \"C\"\n",
    "        if loca == 0 and indi > 0:\n",
    "            ckind = \"I\"\n",
    "        elif loca > 0 and indi == 0:\n",
    "            ckind = \"L\"\n",
    "        elif loca > indi + 1:\n",
    "            ckind = \"L\"\n",
    "        elif loca < indi - 1:\n",
    "            ckind = \"I\"\n",
    "        complementk[p] = (loca, indi, ckind)\n",
    "        kcomplements[ckind] += 1\n",
    "        complements_c[c][ckind].append(p)\n",
    "        complements[p] = (pf, ckind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      4m 45s Determinig kind of complements                                                 .\n",
      "..............................................................................................\n",
      "|      4m 46s \tDone\n",
      "|      4m 46s \tPhrases of kind C :  16806\n",
      "|      4m 46s \tPhrases of kind L :  12365\n",
      "|      4m 46s \tPhrases of kind I :   7838\n",
      "|      4m 46s \tTotal complements :  37009\n",
      "|      4m 46s \tTotal phrases     : 214525\n"
     ]
    }
   ],
   "source": [
    "utils.caption(0, \"\\tDone\")\n",
    "for (label, n) in sorted(kcomplements.items(), key=lambda y: -y[1]):\n",
    "    utils.caption(0, \"\\tPhrases of kind {:<2}: {:>6}\".format(label, n))\n",
    "utils.caption(0, \"\\tTotal complements : {:>6}\".format(ncomplements))\n",
    "utils.caption(0, \"\\tTotal phrases     : {:>6}\".format(nphrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-tomato",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[28]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_L(vl, pn):\n",
    "    words = L.d(pn, \"word\")\n",
    "    return len(words) > 0 and F.lex.v(words[0] == \"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lex_personal(vl, pn):\n",
    "    words = L.d(pn, \"word\")\n",
    "    return len(words) > 1 and (\n",
    "        F.lex.v(words[1]) in personal_lexemes or F.nametype.v(words[1]) == \"pers\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lex_local(vl, pn):\n",
    "    words = L.d(pn, \"word\")\n",
    "    return len({F.lex.v(w) for w in words} & locative_lexemes) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def has_H_locale(vl, pn):\n",
    "    words = L.d(pn, \"word\")\n",
    "    return len({w for w in words if F.uvf.v(w) == \"H\"}) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Generic logic\n",
    "\n",
    "This is the function that applies the generic rules about (in)direct objects and locatives.\n",
    "It takes a phrase node and a set of new label values, and modifies those values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-chance",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[29]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "grule_as_str = {\n",
    "    \"pdos\": \"\"\"direct_object => principal_direct_object\"\"\",\n",
    "    \"pdos-x\": \"\"\"non-object => principal_direct_object\"\"\",\n",
    "    \"ndos\": \"\"\"direct_object => NP_direct_object\"\"\",\n",
    "    \"ndos-x\": \"\"\"non-object => NP_direct_object\"\"\",\n",
    "    \"dos\": \"\"\"non-object => direct_object\"\"\",\n",
    "    \"ldos\": \"\"\"non-object => L_object\"\"\",\n",
    "    \"kdos\": \"\"\"non-object => K_object\"\"\",\n",
    "    \"inds-c\": \"\"\"complement => indirect_object\"\"\",\n",
    "    \"locs-c\": \"\"\"complement => location\"\"\",\n",
    "    \"inds-p\": \"\"\"predicate complement => indirect_object\"\"\",\n",
    "    \"locs-p\": \"\"\"predicate complement => location\"\"\",\n",
    "    \"cdos\": \"\"\"direct-object =(superfluously)=> direct object (clause)\"\"\",\n",
    "    \"cdos-x\": \"\"\"non-object => direct object (clause)\"\"\",\n",
    "    \"idos\": \"\"\"infinitive_object =(superfluously)=> infinitive_object (clause)\"\"\",\n",
    "    \"idos-x\": \"\"\"infinitive clause => infinitive_object\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_as_str_g(x, i):\n",
    "    return \"{}-{}\".format(i, grule_as_str[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_as_str = dict(\n",
    "    generic=rule_as_str_g,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_logic_p(pn, values):\n",
    "    gl = None\n",
    "    if pn in objects[\"principal\"]:\n",
    "        oldv = values[\"grammatical\"]\n",
    "        if oldv == \"direct_object\":\n",
    "            gl = \"pdos\"\n",
    "        else:\n",
    "            gl = \"pdos-x\"\n",
    "            values[\"original\"] = oldv\n",
    "        values[\"grammatical\"] = \"principal_direct_object\"\n",
    "    elif pn in objects[\"NP\"]:\n",
    "        oldv = values[\"grammatical\"]\n",
    "        if oldv == \"direct_object\":\n",
    "            gl = \"ndos\"\n",
    "        else:\n",
    "            gl = \"ndos-x\"\n",
    "            values[\"original\"] = oldv\n",
    "        values[\"grammatical\"] = \"NP_direct_object\"\n",
    "    elif pn in objects[\"direct\"]:\n",
    "        oldv = values[\"grammatical\"]\n",
    "        if oldv != \"direct_object\":\n",
    "            gl = \"dos\"\n",
    "            values[\"original\"] = oldv\n",
    "            values[\"grammatical\"] = \"direct_object\"\n",
    "    elif pn in objects[\"L\"]:\n",
    "        oldv = values[\"grammatical\"]\n",
    "        gl = \"ldos\"\n",
    "        values[\"original\"] = oldv\n",
    "        values[\"grammatical\"] = \"L_object\"\n",
    "    elif pn in objects[\"K\"]:\n",
    "        oldv = values[\"grammatical\"]\n",
    "        gl = \"kdos\"\n",
    "        values[\"original\"] = oldv\n",
    "        values[\"grammatical\"] = \"K_object\"\n",
    "    elif pn in complements:\n",
    "        (pf, ck) = complements[pn]\n",
    "        if ck in {\"I\", \"L\"}:\n",
    "            if pf == \"Cmpl\":\n",
    "                if ck == \"I\":\n",
    "                    values[\"grammatical\"] = \"indirect_object\"\n",
    "                    gl = \"inds-c\"\n",
    "                else:\n",
    "                    values[\"lexical\"] = \"location\"\n",
    "                    values[\"semantic\"] = \"location\"\n",
    "                    gl = \"locs-c\"\n",
    "            elif pf == \"PreC\":\n",
    "                if ck == \"I\":\n",
    "                    values[\"grammatical\"] = \"indirect_object\"\n",
    "                    gl = \"inds-p\"\n",
    "                else:\n",
    "                    values[\"lexical\"] = \"location\"\n",
    "                    values[\"semantic\"] = \"location\"\n",
    "                    gl = \"locs-p\"\n",
    "    return gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_logic_c(cn, values):\n",
    "    gl = None\n",
    "    if cn in objects[\"clause\"]:\n",
    "        oldv = values[\"grammatical\"]\n",
    "        if oldv == \"direct_object\":\n",
    "            gl = \"cdos\"\n",
    "        else:\n",
    "            gl = \"cdos-x\"\n",
    "            values[\"original\"] = oldv\n",
    "        values[\"grammatical\"] = \"direct_object\"\n",
    "    elif cn in objects[\"infinitive\"]:\n",
    "        oldv = values[\"grammatical\"]\n",
    "        if oldv == \"infinitive_object\":\n",
    "            gl = \"idos\"\n",
    "        else:\n",
    "            gl = \"idos-x\"\n",
    "            values[\"original\"] = oldv\n",
    "        values[\"grammatical\"] = \"infinitive_object\"\n",
    "    return gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "generic_logic = dict(\n",
    "    phrase=generic_logic_p,\n",
    "    clause=generic_logic_c,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Verb specific rules\n",
    "\n",
    "The verb-specific enrichment rules are stored in a dictionary, keyed  by the verb lexeme.\n",
    "The rule itself is a list of items.\n",
    "\n",
    "The last item is a tuple of conditions that need to be fulfilled to apply the rule.\n",
    "\n",
    "A condition can take the shape of\n",
    "\n",
    "* a function, taking a phrase or clause node as argument and returning a boolean value\n",
    "* an BHSA feature for phrases or clauses : value,\n",
    "  which is true if and only if that feature has that value for the phrase or clause in question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-forty",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[30]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dbl_obj_rules = (\n",
    "    (\n",
    "        (\"semantic\", \"benefactive\"),\n",
    "        (\"function:Adju\", has_L, is_lex_personal),\n",
    "    ),\n",
    "    (\n",
    "        (\"lexical\", \"location\"),\n",
    "        (\"function:Cmpl\", has_H_locale),\n",
    "    ),\n",
    "    (\n",
    "        (\"lexical\", \"location\"),\n",
    "        (\"semantic\", \"location\"),\n",
    "        (\"function:Cmpl\", is_lex_local),\n",
    "    ),\n",
    ")\n",
    "enrich_logic = dict(\n",
    "    phrase={\n",
    "        \"CJT\": dbl_obj_rules,\n",
    "        \"FJM\": dbl_obj_rules,\n",
    "    },\n",
    "    clause={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-screening",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[31]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_index = collections.defaultdict(lambda: [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_as_str_s(vl, i):\n",
    "    (conditions, sfassignments) = rule_index[vl][i]\n",
    "    label = \"{}-{}\\n\".format(vl, i + 1)\n",
    "    rule = \"\\tIF   {}\".format(\n",
    "        \"\\n\\tAND  \".join(\n",
    "            \"{:<10} = {:<8}\".format(*c.split(\":\"))\n",
    "            if type(c) is str\n",
    "            else \"{:<15}\".format(c.__name__)\n",
    "            for c in conditions\n",
    "        )\n",
    "    )\n",
    "    ass = []\n",
    "    for (i, sfa) in enumerate(sfassignments):\n",
    "        ass.append(\"\\t\\t{:<10} => {:<15}\\n\".format(*sfa))\n",
    "    return \"{}{}\\n\\tTHEN\\n{}\".format(label, rule, \"\".join(ass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_as_str[\"specific\"] = rule_as_str_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_logic():\n",
    "    utils.caption(4, \"Checking enrichment logic\")\n",
    "    errors = 0\n",
    "    nrules = 0\n",
    "    for kind in sorted(enrich_logic):\n",
    "        for vl in sorted(enrich_logic[kind]):\n",
    "            for items in enrich_logic[kind][vl]:\n",
    "                rule_index[vl].append((items[-1], items[0:-1]))\n",
    "            for (i, (conditions, sfassignments)) in enumerate(rule_index[vl]):\n",
    "                if not SCRIPT:\n",
    "                    utils.caption(0, rule_as_str_s(vl, i), continuation=True)\n",
    "                nrules += 1\n",
    "                for (sf, sfval) in sfassignments:\n",
    "                    if sf not in enrich_fields:\n",
    "                        utils.caption(\n",
    "                            0,\n",
    "                            'ERROR: {}: \"{}\" not a valid enrich field'.format(kind, sf),\n",
    "                            continuation=True,\n",
    "                        )\n",
    "                        errors += 1\n",
    "                    elif sfval not in enrich_fields[sf]:\n",
    "                        utils.caption(\n",
    "                            0,\n",
    "                            'ERROR: {}: `{}`: \"{}\" not a valid enrich field value'.format(\n",
    "                                kind, sf, sfval\n",
    "                            ),\n",
    "                            continuation=True,\n",
    "                        )\n",
    "                        errors += 1\n",
    "                for c in conditions:\n",
    "                    if type(c) == str:\n",
    "                        x = c.split(\":\")\n",
    "                        if len(x) != 2:\n",
    "                            utils.caption(\n",
    "                                0,\n",
    "                                \"ERROR: {}: Wrong feature condition {}\".format(kind, c),\n",
    "                                continuation=True,\n",
    "                            )\n",
    "                            errors += 1\n",
    "                        else:\n",
    "                            (feat, val) = x\n",
    "                            if feat not in legal_values:\n",
    "                                utils.caption(\n",
    "                                    0,\n",
    "                                    \"ERROR: {}: Feature `{}` not in use\".format(\n",
    "                                        kind, feat\n",
    "                                    ),\n",
    "                                    continuation=True,\n",
    "                                )\n",
    "                                errors += 1\n",
    "                            elif val not in legal_values[feat]:\n",
    "                                utils.caption(\n",
    "                                    0,\n",
    "                                    'ERROR: {}: Feature `{}`: not a valid value \"{}\"'.format(\n",
    "                                        kind, feat, val\n",
    "                                    ),\n",
    "                                    continuation=True,\n",
    "                                )\n",
    "                                errors += 1\n",
    "    if errors:\n",
    "        utils.caption(\n",
    "            0, \"\\tERROR: There were {} errors in {} rules\".format(errors, nrules)\n",
    "        )\n",
    "    else:\n",
    "        utils.caption(0, \"\\tAll {} rules OK\".format(nrules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      5m 30s Checking enrichment logic                                                      .\n",
      "..............................................................................................\n",
      "CJT-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "CJT-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "CJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "FJM-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "FJM-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "FJM-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "|      5m 30s \tAll 6 rules OK\n"
     ]
    }
   ],
   "source": [
    "check_logic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-bailey",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[32]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_cases = collections.defaultdict(lambda: collections.defaultdict(lambda: {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def apply_logic(kind, vl, n, init_values):\n",
    "    values = deepcopy(init_values)\n",
    "    gr = generic_logic[kind](n, values)\n",
    "    if gr:\n",
    "        rule_cases[\"generic\"][kind].setdefault((\"\", gr), []).append(n)\n",
    "    verb_rules = enrich_logic[kind].get(vl, [])\n",
    "    for (i, items) in enumerate(verb_rules):\n",
    "        conditions = items[-1]\n",
    "        sfassignments = items[0:-1]\n",
    "\n",
    "        ok = True\n",
    "        for condition in conditions:\n",
    "            if type(condition) is str:\n",
    "                (feature, value) = condition.split(\":\")\n",
    "                if feature == \"function\" and kind == \"phrase\":\n",
    "                    fval = pf_corr.get(n, F.function.v(n))\n",
    "                else:\n",
    "                    fval = F.item[feature].v(n)\n",
    "                this_ok = fval == value\n",
    "            else:\n",
    "                this_ok = condition(vl, n)\n",
    "            if not this_ok:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                values[sf] = sfval\n",
    "            rule_cases[\"specific\"][kind].setdefault((vl, i), []).append(n)\n",
    "    return tuple(values[sf] for sf in enrich_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Generate enrichments\n",
    "\n",
    "First we generate enriched values for all relevant phrases.\n",
    "The generated enrichment values are computed on the basis of generic logic.\n",
    "Additionally, verb-bound logic is applied, if it has been specified.\n",
    "\n",
    "We store the enriched features in a dictionary, first keyed by the type of constituent that\n",
    "receives the enrichments (`phrase` or `clause`), and then by the node number of the constituent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-guitar",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[33]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(4, \"Generating enrichments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = collections.defaultdict(collections.Counter)\n",
    "enrichFields = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_enrich(verb):\n",
    "    clauses_seen = set()\n",
    "\n",
    "    for wn in occs[verb]:\n",
    "        cn = L.u(wn, \"clause\")[0]\n",
    "        if cn in clauses_seen:\n",
    "            continue\n",
    "        clauses_seen.add(cn)\n",
    "        vl = F.lex.v(wn).rstrip(\"[=\")\n",
    "        for pn in L.d(cn, \"phrase\"):\n",
    "            seen[\"phrase\"][pn] += 1\n",
    "            pf = pf_corr.get(pn, F.function.v(pn))\n",
    "            enrichFields[pn] = apply_logic(\"phrase\", vl, pn, transform[\"phrase\"][pf])\n",
    "        for scn in clause_objects[cn]:\n",
    "            seen[\"clause\"][scn] += 1\n",
    "            scty = F.typ.v(scn)\n",
    "            scr = F.rela.v(scn)\n",
    "            enrichFields[scn] = apply_logic(\n",
    "                \"clause\", vl, scn, transform[\"clause\"][scr if scr == \"Objc\" else scty]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      5m 52s Generating enrichments                                                         .\n",
      "..............................................................................................\n",
      "|      5m 56s \tGenerated enrichment values for 1380 verbs:\n",
      "|      5m 56s \tEnriched values for 221353 nodes\n"
     ]
    }
   ],
   "source": [
    "for verb in verb_clause_index:\n",
    "    gen_enrich(verb)\n",
    "utils.caption(\n",
    "    0, \"\\tGenerated enrichment values for {} verbs:\".format(len(verb_clause_index))\n",
    ")\n",
    "utils.caption(0, \"\\tEnriched values for {:>5} nodes\".format(len(enrichFields)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-cornell",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[34]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(0, \"\\tOverview of rule applications:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scope in rule_cases:\n",
    "    totalscope = 0\n",
    "    for kind in rule_cases[scope]:\n",
    "        utils.caption(0, \"{}-{} rules:\".format(scope, kind))\n",
    "        totalkind = 0\n",
    "        for rule_spec in rule_cases[scope][kind]:\n",
    "            cases = rule_cases[scope][kind][rule_spec]\n",
    "            n = len(cases)\n",
    "            totalscope += n\n",
    "            totalkind += n\n",
    "            if not SCRIPT:\n",
    "                if scope == \"generic\":\n",
    "                    utils.caption(\n",
    "                        0,\n",
    "                        \"{:>4} x\\n\\t{}\\n\\t{}\\n\".format(\n",
    "                            n,\n",
    "                            rule_as_str[scope](*rule_spec),\n",
    "                            \", \".join(str(c) for c in cases[0:10]),\n",
    "                        ),\n",
    "                    )\n",
    "                else:\n",
    "                    utils.caption(\n",
    "                        0,\n",
    "                        \"{:>4} x\\n\\t{}\\n\\t{}\\n\".format(\n",
    "                            n,\n",
    "                            rule_as_str[scope](*rule_spec),\n",
    "                            \", \".join(str(c) for c in cases[0:10]),\n",
    "                        ),\n",
    "                    )\n",
    "        utils.caption(0, \"{:>6} {}-{} rule applications\".format(totalkind, scope, kind))\n",
    "    utils.caption(0, \"{:>6} {} rule applications\".format(totalscope, scope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      6m 02s \tOverview of rule applications:\n",
      "|      6m 02s generic-phrase rules:\n",
      "|      6m 02s 1098 x\n",
      "\tndos-direct_object => NP_direct_object\n",
      "\t651871, 652852, 784482, 788915, 887094, 668755, 677667, 680588, 696396, 700200\n",
      "\n",
      "|      6m 02s 3871 x\n",
      "\tpdos-direct_object => principal_direct_object\n",
      "\t651873, 652853, 784483, 786606, 788914, 744784, 746449, 771047, 876074, 887093\n",
      "\n",
      "|      6m 02s  625 x\n",
      "\tlocs-p-predicate complement => location\n",
      "\t783707, 651621, 651984, 652623, 653139, 653425, 653438, 653473, 653798, 653929\n",
      "\n",
      "|      6m 02s 11750 x\n",
      "\tlocs-c-complement => location\n",
      "\t813424, 651625, 652673, 652692, 653902, 654097, 654101, 654150, 654153, 654156\n",
      "\n",
      "|      6m 02s 4186 x\n",
      "\tldos-non-object => L_object\n",
      "\t651731, 651734, 651914, 654024, 655756, 656050, 658720, 659683, 660594, 663717\n",
      "\n",
      "|      6m 02s 6170 x\n",
      "\tinds-c-complement => indirect_object\n",
      "\t651912, 653331, 653809, 654033, 654044, 654255, 654261, 655715, 657058, 657490\n",
      "\n",
      "|      6m 02s  434 x\n",
      "\tinds-p-predicate complement => indirect_object\n",
      "\t652018, 654737, 654825, 655213, 655675, 655794, 656059, 656128, 657059, 657491\n",
      "\n",
      "|      6m 02s  122 x\n",
      "\tkdos-non-object => K_object\n",
      "\t656221, 673715, 678718, 794332, 810498, 812976, 816543, 818376, 822658, 830663\n",
      "\n",
      "|      6m 02s  28256 generic-phrase rule applications\n",
      "|      6m 02s generic-clause rules:\n",
      "|      6m 02s 1301 x\n",
      "\tidos-x-infinitive clause => infinitive_object\n",
      "\t472837, 505957, 514171, 514360, 447666, 447671, 497328, 506307, 428311, 434077\n",
      "\n",
      "|      6m 02s 1334 x\n",
      "\tcdos-direct-object =(superfluously)=> direct object (clause)\n",
      "\t467145, 477589, 428557, 435937, 455879, 457985, 468370, 472597, 485904, 495596\n",
      "\n",
      "|      6m 02s   2635 generic-clause rule applications\n",
      "|      6m 02s  30891 generic rule applications\n",
      "|      6m 02s specific-phrase rules:\n",
      "|      6m 02s  108 x\n",
      "\tFJM-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "\t652027, 654959, 657674, 658068, 658161, 660439, 660547, 661507, 661858, 662456\n",
      "\n",
      "|      6m 02s    1 x\n",
      "\tFJM-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "\t813144\n",
      "\n",
      "|      6m 02s    8 x\n",
      "\tCJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "\t652409, 652827, 665769, 677388, 700571, 822727, 836009, 843846\n",
      "\n",
      "|      6m 02s    117 specific-phrase rule applications\n",
      "|      6m 02s    117 specific rule applications\n",
      "|      6m 02s \t204574 phrase seen 1  time(s)\n",
      "|      6m 02s \t  9361 phrase seen 2  time(s)\n",
      "|      6m 02s \t   549 phrase seen 3  time(s)\n",
      "|      6m 02s \t    32 phrase seen 4  time(s)\n",
      "|      6m 02s \t     9 phrase seen 5  time(s)\n",
      "|      6m 02s \t214525 phrase seen in total\n",
      "|      6m 02s \t  6641 clause seen 1  time(s)\n",
      "|      6m 02s \t   177 clause seen 2  time(s)\n",
      "|      6m 02s \t     9 clause seen 3  time(s)\n",
      "|      6m 02s \t     1 clause seen 4  time(s)\n",
      "|      6m 02s \t  6828 clause seen in total\n"
     ]
    }
   ],
   "source": [
    "for kind in seen:\n",
    "    stats = collections.Counter()\n",
    "    for (node, times) in seen[kind].items():\n",
    "        stats[times] += 1\n",
    "    if not SCRIPT:\n",
    "        for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "            utils.caption(0, \"\\t{:>6} {} seen {:<2} time(s)\".format(n, kind, times))\n",
    "    utils.caption(0, \"\\t{:>6} {} seen in total\".format(len(seen[kind]), kind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For selected verbs, we write the enrichments to spreadsheets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-crown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[35]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_FIELDS = \"\"\"\n",
    "    cnode#\n",
    "    vnode#\n",
    "    onode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "    text\n",
    "    constituent\n",
    "\"\"\".strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASE_FIELDS = \"\"\"\n",
    "    type\n",
    "    function\n",
    "\"\"\".strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAUSE_FIELDS = \"\"\"\n",
    "    type\n",
    "    rela\n",
    "\"\"\".strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "onode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "verb_lexeme\n",
      "verb_stem\n",
      "verb_occurrence\n",
      "text\n",
      "constituent\n",
      "type\n",
      "rela\n",
      "type\n",
      "function\n",
      "valence\n",
      "predication\n",
      "grammatical\n",
      "original\n",
      "lexical\n",
      "semantic\n"
     ]
    }
   ],
   "source": [
    "field_names = COMMON_FIELDS + CLAUSE_FIELDS + PHRASE_FIELDS + list(enrich_fields)\n",
    "pfillrows = len(CLAUSE_FIELDS)\n",
    "cfillrows = len(PHRASE_FIELDS)\n",
    "fillrows = pfillrows + cfillrows + len(enrich_fields)\n",
    "if not SCRIPT:\n",
    "    print(\"\\n\".join(field_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-following",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[36]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(4, \"Generate blank enrichment sheets\")\n",
    "sheetKind = \"enrich_blank\"\n",
    "utils.caption(0, \"\\tas {}\".format(vfile(\"{verb}\", sheetKind)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = \";\"\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cn = L.u(wn, \"clause\")[0]\n",
    "        if cn in clauses_seen:\n",
    "            continue\n",
    "        clauses_seen.add(cn)\n",
    "        (book_name, chapter, verse) = T.sectionFromNode(cn, lang=\"la\")\n",
    "        book = T.sectionFromNode(cn)[0]\n",
    "        vl = F.lex.v(wn).rstrip(\"[=\")\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.text([wn], fmt=\"text-trans-plain\")\n",
    "        ct = T.text(L.d(cn, \"word\"), fmt=\"text-trans-plain\")\n",
    "\n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, vl, vstem, vt, ct, \"\")\n",
    "        rows.append(common_fields + ((\"\",) * fillrows))\n",
    "        for pn in L.d(cn, \"phrase\"):\n",
    "            seen[\"phrase\"][pn] += 1\n",
    "            pt = T.text(L.d(pn, \"word\"), fmt=\"text-trans-plain\")\n",
    "            common_fields = (\n",
    "                cn,\n",
    "                wn,\n",
    "                pn,\n",
    "                book,\n",
    "                chapter,\n",
    "                verse,\n",
    "                vl,\n",
    "                vstem,\n",
    "                \"\",\n",
    "                pt,\n",
    "                \"phrase\",\n",
    "            )\n",
    "            pty = F.typ.v(pn)\n",
    "            pf = pf_corr.get(pn, F.function.v(pn))\n",
    "            phrase_fields = (\"\",) * pfillrows + (pty, pf) + enrichFields[pn]\n",
    "            rows.append(common_fields + phrase_fields)\n",
    "        for scn in clause_objects[cn]:\n",
    "            seen[\"clause\"][scn] += 1\n",
    "            sct = T.text(L.d(scn, \"word\"), fmt=\"text-trans-plain\")\n",
    "            common_fields = (\n",
    "                cn,\n",
    "                wn,\n",
    "                scn,\n",
    "                book,\n",
    "                chapter,\n",
    "                verse,\n",
    "                vl,\n",
    "                vstem,\n",
    "                \"\",\n",
    "                sct,\n",
    "                \"clause\",\n",
    "            )\n",
    "            scty = F.typ.v(scn)\n",
    "            scr = F.rela.v(scn)\n",
    "            clause_fields = (scty, scr) + (\"\",) * cfillrows + enrichFields[scn]\n",
    "            rows.append(common_fields + clause_fields)\n",
    "\n",
    "    location = vfile(verb, sheetKind)\n",
    "    if location is None:\n",
    "        return\n",
    "    (baseName, fileName) = location\n",
    "\n",
    "    row_file = open(fileName, \"w\")\n",
    "    row_file.write(\"{}\\n\".format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write(\"{}\\n\".format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    utils.caption(0, \"\\t\\tfor verb {} ({:>5} rows)\".format(verb, len(rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for verb in verbs:\n",
    "    gen_sheet_enrich(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-footage",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "utils.caption(0, \"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-arbitration",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[37]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def showcase(n):\n",
    "    otype = F.otype.v(n)\n",
    "    att1 = pf_corr.get(n, F.function.v(n)) if otype == \"phrase\" else F.rela.v(n)\n",
    "    att2 = F.typ.v(n)\n",
    "    utils.caption(\n",
    "        0,\n",
    "        \"\"\"{} ({}-{}) {}\\n{} {}:{}    {}\\n\"\"\".format(\n",
    "            otype,\n",
    "            att1,\n",
    "            att2,\n",
    "            T.text(L.d(n, \"word\"), fmt=\"text-trans-plain\"),\n",
    "            *T.sectionFromNode(n),\n",
    "            T.text(L.d(L.u(n, \"verse\")[0], \"word\"), fmt=\"text-trans-plain\"),\n",
    "        ),\n",
    "        continuation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-living",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[38]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase (Pred-VP) JKLW \n",
      "Genesis 13:6    WL>&NF> >TM H>RY LCBT JXDW KJ&HJH RKWCM RB WL> JKLW LCBT JXDW00 \n",
      "\n",
      "clause (Coor-WxY0) WLW&>TN >T&H>RY WLBNJW \n",
      "Deuteronomium 1:36    ZWLTJ KLB BN&JPNH HW> JR>NH WLW&>TN >T&H>RY >CR DRK&BH WLBNJW J<N >CR ML> >XRJ JHWH00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not SCRIPT:\n",
    "    showcase(654844)\n",
    "    showcase(445014)\n",
    "    # showcase(426954)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-forest",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[39]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_h(vl, show_results=False):\n",
    "    hl = {}\n",
    "    total = 0\n",
    "    for w in F.otype.s(\"word\"):\n",
    "        if F.sp.v(w) != \"verb\" or F.lex.v(w).rstrip(\"[=/\") != vl:\n",
    "            continue\n",
    "        total += 1\n",
    "        c = L.u(w, \"clause\")[0]\n",
    "        ps = L.d(c, \"phrase\")\n",
    "        phs = {\n",
    "            p for p in ps if len({w for w in L.d(p, \"word\") if F.uvf.v(w) == \"H\"}) > 0\n",
    "        }\n",
    "        for f in (\"Cmpl\", \"Adju\", \"Loca\"):\n",
    "            phc = {\n",
    "                p\n",
    "                for p in ps\n",
    "                if pf_corr.get(p, None) or (pf_corr.get(p, F.function.v(p))) == f\n",
    "            }\n",
    "            if len(phc & phs):\n",
    "                hl.setdefault(f, set()).add(w)\n",
    "    for f in hl:\n",
    "        utils.caption(\n",
    "            0,\n",
    "            \"Verb {}: {} occurrences. He locales in {} phrases: {}\".format(\n",
    "                vl, total, f, len(hl[f])\n",
    "            ),\n",
    "            continuation=True,\n",
    "        )\n",
    "        if show_results:\n",
    "            utils.caption(\n",
    "                0, \"\\t{}\".format(\", \".join(str(x) for x in hl[f])), continuation=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb BW>: 2570 occurrences. He locales in Cmpl phrases: 164\n",
      "\t257028, 184327, 26120, 256523, 26129, 146458, 187932, 197150, 95265, 272418, 24618, 184362, 398381, 289838, 201265, 136756, 78903, 100418, 5699, 32835, 100421, 401474, 198220, 200270, 24655, 100946, 24660, 112215, 141913, 186972, 196702, 28770, 34406, 298606, 248943, 132209, 162929, 12403, 5748, 146055, 90249, 153227, 396940, 97427, 134803, 151187, 188054, 257177, 21658, 426137, 136349, 162981, 24742, 200361, 214699, 25779, 257204, 158389, 4790, 100535, 4794, 160445, 214719, 90818, 272581, 139974, 249032, 38601, 113869, 8921, 138459, 19168, 20705, 26852, 282853, 8425, 8938, 43241, 145138, 170740, 397045, 254715, 154365, 79615, 200960, 27393, 176387, 165637, 206087, 208648, 426247, 269581, 26382, 106254, 157458, 257305, 149796, 170793, 211244, 26416, 27440, 126769, 9526, 246074, 109371, 172351, 249152, 26433, 4930, 16706, 64834, 398147, 168782, 154975, 132966, 47466, 393583, 47472, 157552, 37238, 100214, 23417, 269182, 23935, 24450, 78214, 411014, 93579, 26001, 133529, 12699, 19357, 24478, 191393, 93606, 170920, 18346, 157619, 267701, 8634, 244672, 63425, 256964, 167370, 175564, 110544, 138705, 37330, 5587, 108498, 11733, 111059, 143831, 20440, 175572, 192985, 99803, 264149, 170974, 218595, 25064, 269297, 26101, 110584\n",
      "Verb BW>: 2570 occurrences. He locales in Loca phrases: 5\n",
      "\t285001, 29643, 289871, 284977, 289883\n",
      "Verb BW>: 2570 occurrences. He locales in Adju phrases: 2\n",
      "\t75708, 322830\n"
     ]
    }
   ],
   "source": [
    "if not SCRIPT:\n",
    "    check_h(\"BW>\", show_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be handy to generate an informational spreadsheet that shows all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Process the filled in enrichments\n",
    "\n",
    "We read the enrichments, and perform some consistency checks.\n",
    "If the filled-in sheet does not exist, we take the blank sheet, with the default assignment of the new features.\n",
    "If a phrase got conflicting features, because it occurs in sheets for multiple verbs, the values in the filled-in sheet take precedence over the values in the blank sheet. If both occur in a filled in sheet, a warning will be issued."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-english",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[43]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def read_enrich():\n",
    "    of_enriched = {\n",
    "        False: {},  # for enrichments found in blank sheets\n",
    "        True: {},  # for enrichments found in filled sheets\n",
    "    }\n",
    "    repeated = {\n",
    "        False: collections.defaultdict(list),  # for blank sheets\n",
    "        True: collections.defaultdict(list),  # for filled sheets\n",
    "    }\n",
    "    wrong_value = {\n",
    "        False: collections.defaultdict(list),\n",
    "        True: collections.defaultdict(list),\n",
    "    }\n",
    "\n",
    "    non_match = collections.defaultdict(list)\n",
    "    wrong_node = collections.defaultdict(list)\n",
    "\n",
    "    results = []\n",
    "    dev_results = []  # results that deviate from the filled sheet\n",
    "\n",
    "    ERR_LIMIT = 10\n",
    "\n",
    "    ev = \"text-trans-plain\"\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        vresults = {\n",
    "            False: {},  # for blank sheets\n",
    "            True: {},  # for filled sheets\n",
    "        }\n",
    "        for check in (\n",
    "            (False, \"blank\"),\n",
    "            (True, \"filled\"),\n",
    "        ):\n",
    "            is_filled = check[0]\n",
    "\n",
    "            location = vfile(verb, \"enrich_{}\".format(check[1]))\n",
    "            if location is None:\n",
    "                continue\n",
    "            (baseName, fileName) = location\n",
    "\n",
    "            if not os.path.exists(fileName):\n",
    "                if not is_filled:\n",
    "                    utils.caption(\n",
    "                        0, \"\\tNO {} enrichment sheet for {}\".format(check[1], baseName)\n",
    "                    )\n",
    "                continue\n",
    "            utils.caption(0, \"\\t{} enrichment sheet for {}\".format(check[1], baseName))\n",
    "\n",
    "            with open(fileName) as fh:\n",
    "                fh.__next__()\n",
    "                for line in fh:\n",
    "                    fields = line.rstrip().split(\";\")\n",
    "                    on = int(fields[2])\n",
    "                    if on < 0:\n",
    "                        continue\n",
    "                    kind = fields[10]\n",
    "                    objects_seen[kind][on] += 1\n",
    "                    vvals = tuple(fields[-nef:])\n",
    "                    for (f, v) in zip(enrich_fields, vvals):\n",
    "                        if (\n",
    "                            v != \"\"\n",
    "                            and v != \"X\"\n",
    "                            and v != \"NA\"\n",
    "                            and v not in enrich_fields[f]\n",
    "                        ):\n",
    "                            wrong_value[is_filled][on].append((verb, f, v))\n",
    "                    vresults[is_filled][on] = vvals\n",
    "                    if on in of_enriched[is_filled]:\n",
    "                        if on not in repeated[is_filled]:\n",
    "                            repeated[is_filled][on] = [of_enriched[is_filled][on]]\n",
    "                        repeated[is_filled][on].append((verb, vvals))\n",
    "                    else:\n",
    "                        of_enriched[is_filled][on] = (verb, vvals)\n",
    "                    if F.otype.v(on) != kind:\n",
    "                        non_match[on].append((verb, kind))\n",
    "            for on in sorted(\n",
    "                vresults[True]\n",
    "            ):  # check whether the phrase ids are not mangled\n",
    "                if on not in vresults[False]:\n",
    "                    wrong_node[on].append(verb)\n",
    "            for on in sorted(\n",
    "                vresults[False]\n",
    "            ):  # now collect all results, give precedence to filled values\n",
    "                if F.otype.v(on) == \"phrase\":\n",
    "                    f_corr = on in pf_corr  # manual correction in phrase function\n",
    "                    f_good = pf_corr.get(on, F.function.v(on))\n",
    "                else:\n",
    "                    f_corr = \"\"\n",
    "                    f_good = \"\"\n",
    "                s_manual = (\n",
    "                    on in vresults[True] and vresults[False][on] != vresults[True][on]\n",
    "                )  # real change\n",
    "\n",
    "                # here we determine which value is going to be put in a feature\n",
    "                # basic rule: if there is an filled-in sheet, take the value from there, else from the blank one\n",
    "                # exception:\n",
    "                # if a value is empty in the filled-in sheet, but not in the blank one, take the non-empty one\n",
    "                #\n",
    "                # Why? Well, sometimes we improve the enrich logic. There may be filled-in sheets based on older\n",
    "                # blank sheets.\n",
    "                # We want to push new values in blank sheets through unfilled in values in the filled sheets.\n",
    "                # If it is intentional to remove a value from the blank sheet,\n",
    "                # you can put an X in the corresponding filled field.\n",
    "                blank_results = vresults[False][on]\n",
    "                these_results = []\n",
    "\n",
    "                for (i, br) in enumerate(blank_results):\n",
    "                    the_value = br\n",
    "                    if s_manual and vresults[True][on][i] != \"\":\n",
    "                        the_value = vresults[True][on][i]\n",
    "                        if the_value == \"X\":\n",
    "                            the_value = \"\"\n",
    "                    these_results.append(the_value)\n",
    "                these_results = tuple(these_results)\n",
    "\n",
    "                # these_results = vresults[True][on] if s_manual else vresults[False][on]\n",
    "\n",
    "                if f_corr or s_manual:\n",
    "                    dev_results.append(\n",
    "                        (on,) + these_results + (f_good, f_corr, s_manual)\n",
    "                    )\n",
    "                results.append((on,) + these_results + (f_good, f_corr, s_manual))\n",
    "\n",
    "    for check in (\n",
    "        (False, \"blank\"),\n",
    "        (True, \"filled\"),\n",
    "    ):\n",
    "        if len(wrong_value[check[0]]):  # illegal values in sheets\n",
    "            wrongs = wrong_value[check[0]]\n",
    "            for x in sorted(wrongs)[0:ERR_LIMIT]:\n",
    "                px = T.text(L.d(x, \"word\"), fmt=ev)\n",
    "                ref_node = L.u(x, \"clause\")[0] if F.otype.v(x) != \"clause\" else x\n",
    "                cx = T.text(L.d(ref_node, \"word\"), fmt=ev)\n",
    "                passage = T.sectionFromNode(x)\n",
    "                utils.caption(\n",
    "                    0,\n",
    "                    \"ERROR: {} Illegal value(s) in {}: {} = {} in {}:\".format(\n",
    "                        passage, check[1], x, px, cx\n",
    "                    ),\n",
    "                    continuation=True,\n",
    "                )\n",
    "                for (verb, f, v) in wrongs[x]:\n",
    "                    utils.caption(\n",
    "                        0,\n",
    "                        'ERROR: \\t\"{}\" is an illegal value for \"{}\" in verb {}'.format(\n",
    "                            v,\n",
    "                            f,\n",
    "                            verb,\n",
    "                        ),\n",
    "                        continuation=True,\n",
    "                    )\n",
    "            ne = len(wrongs)\n",
    "            if ne > ERR_LIMIT:\n",
    "                utils.caption(\n",
    "                    0,\n",
    "                    \" ... AND {} CASES MORE\".format(ne - ERR_LIMIT),\n",
    "                    continuation=True,\n",
    "                )\n",
    "        else:\n",
    "            utils.caption(\n",
    "                0,\n",
    "                \"\\tOK: The used {} enrichment sheets have legal values\".format(\n",
    "                    check[1]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        nerrors = 0\n",
    "        if len(repeated[check[0]]):  # duplicates in sheets, check consistency\n",
    "            repeats = repeated[check[0]]\n",
    "            for x in sorted(repeats):\n",
    "                overview = collections.defaultdict(list)\n",
    "                for y in repeats[x]:\n",
    "                    overview[y[1]].append(y[0])\n",
    "                px = T.text(L.d(x, \"word\"), fmt=ev)\n",
    "                ref_node = L.u(x, \"clause\")[0] if F.otype.v(x) != \"clause\" else x\n",
    "                cx = T.text(L.d(ref_node, \"word\"), fmt=ev)\n",
    "                passage = T.sectionFromNode(x)\n",
    "                if len(overview) > 1:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        utils.caption(\n",
    "                            0,\n",
    "                            \"ERROR: {} Conflict in {}: {} = {} in {}:\".format(\n",
    "                                passage, check[1], x, px, cx\n",
    "                            ),\n",
    "                            continuation=True,\n",
    "                        )\n",
    "                        for vals in overview:\n",
    "                            utils.caption(\n",
    "                                0,\n",
    "                                \"\\t{:<40} in verb(s) {}\".format(\n",
    "                                    \", \".join(vals),\n",
    "                                    \", \".join(overview[vals]),\n",
    "                                ),\n",
    "                                continuation=True,\n",
    "                            )\n",
    "                elif False:  # for debugging purposes\n",
    "                    # else:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        utils.caption(\n",
    "                            0,\n",
    "                            \"\\t{} Agreement in {} {} = {} in {}: {}\".format(\n",
    "                                passage,\n",
    "                                check[1],\n",
    "                                x,\n",
    "                                px,\n",
    "                                cx,\n",
    "                                \",\".join(list(overview.values())[0]),\n",
    "                            ),\n",
    "                            continuation=True,\n",
    "                        )\n",
    "            ne = nerrors\n",
    "            if ne > ERR_LIMIT:\n",
    "                utils.caption(\n",
    "                    0,\n",
    "                    \" ... AND {} CASES MORE\".format(ne - ERR_LIMIT),\n",
    "                    continuation=True,\n",
    "                )\n",
    "        if nerrors == 0:\n",
    "            utils.caption(\n",
    "                0, \"\\tOK: The used {} enrichment sheets are consistent\".format(check[1])\n",
    "            )\n",
    "\n",
    "    if len(non_match):\n",
    "        utils.caption(\n",
    "            0, \"ERROR: Enrichments have been applied to nodes with non-matching types:\"\n",
    "        )\n",
    "        for x in sorted(non_match)[0:ERR_LIMIT]:\n",
    "            (verb, shouldbe) = non_match[x]\n",
    "            px = T.text(L.d(x, \"word\"), fmt=ev)\n",
    "            utils.caption(\n",
    "                0,\n",
    "                \"ERROR: {}: {} Node {} is not a {} but a {}\".format(\n",
    "                    verb,\n",
    "                    T.sectionFromNode(x),\n",
    "                    x,\n",
    "                    shouldbe,\n",
    "                    F.otype.v(x),\n",
    "                ),\n",
    "                continuation=True,\n",
    "            )\n",
    "        ne = len(non_match)\n",
    "        if ne > ERR_LIMIT:\n",
    "            utils.caption(\n",
    "                0, \" ... AND {} CASES MORE\".format(ne - ERR_LIMIT), continuation=True\n",
    "            )\n",
    "    else:\n",
    "        utils.caption(0, \"\\tOK: all enriched nodes where phrase nodes\")\n",
    "\n",
    "    if len(wrong_node):\n",
    "        utils.caption(0, \"ERROR: Node in filled sheet did not occur in blank sheet:\")\n",
    "        for x in sorted(wrong_node)[0:ERR_LIMIT]:\n",
    "            px = T.text(L.d(x, \"word\"), fmt=ev)\n",
    "            utils.caption(\n",
    "                0,\n",
    "                \"{}: {} node {}\".format(\n",
    "                    wrong_node[x],\n",
    "                    T.sectionFromNode(x),\n",
    "                    x,\n",
    "                ),\n",
    "                continuation=True,\n",
    "            )\n",
    "        ne = len(wrong_node)\n",
    "        if ne > ERR_LIMIT:\n",
    "            utils.caption(\n",
    "                0, \" ... AND {} CASES MORE\".format(ne - ERR_LIMIT), continuation=True\n",
    "            )\n",
    "    else:\n",
    "        utils.caption(0, \"\\tOK: all enriched nodes occurred in the blank sheet\")\n",
    "\n",
    "    if len(dev_results):\n",
    "        utils.caption(\n",
    "            0,\n",
    "            \"\\tOK: there are {} manual correction/enrichment annotations\".format(\n",
    "                len(dev_results)\n",
    "            ),\n",
    "        )\n",
    "        for r in dev_results[0:ERR_LIMIT]:\n",
    "            (x, *vals, f_good, f_corr, s_manual) = r\n",
    "            px = T.text(L.d(x, \"word\"), fmt=ev)\n",
    "            cx = T.text(L.d(L.u(x, \"clause\")[0], \"word\"), fmt=ev)\n",
    "            utils.caption(\n",
    "                0,\n",
    "                \"{:<30} {:>7} => {:<3} {:<3} {}\\n\\t{}\\n\\t\\t{}\".format(\n",
    "                    \"COR\" if f_corr else \"\",\n",
    "                    \"MAN\" if s_manual else \"\",\n",
    "                    \"{} {}:{}\".format(*T.sectionFromNode(x)),\n",
    "                    x,\n",
    "                    \",\".join(vals),\n",
    "                    px,\n",
    "                    cx,\n",
    "                ),\n",
    "                continuation=True,\n",
    "            )\n",
    "        ne = len(dev_results)\n",
    "        if ne > ERR_LIMIT:\n",
    "            utils.caption(\n",
    "                0,\n",
    "                \"... AND {} ANNOTATIONS MORE\".format(ne - ERR_LIMIT),\n",
    "                continuation=True,\n",
    "            )\n",
    "    else:\n",
    "        utils.caption(0, \"\\tthere are no manual correction/enrichment annotations\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-alert",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[44]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(4, \"Processing enrichment sheets ...\")\n",
    "sheetKind = \"enrich_filled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".     16m 49s Processing enrichment sheets ...                                               .\n",
      "..............................................................................................\n",
      "|     16m 49s \tas /Users/dirk/github/etcbc/valence/source/2021/enrich_filled/{verb}.csv\n",
      "|     16m 49s \tblank enrichment sheet for oBR\n",
      "|     16m 49s \tblank enrichment sheet for oFH\n",
      "|     16m 49s \tblank enrichment sheet for oLH\n",
      "|     16m 49s \tblank enrichment sheet for BRa\n",
      "|     16m 49s \tblank enrichment sheet for BWa\n",
      "|     16m 49s \tblank enrichment sheet for CJT\n",
      "|     16m 49s \tblank enrichment sheet for CWB\n",
      "|     16m 49s \tblank enrichment sheet for FJM\n",
      "|     16m 49s \tblank enrichment sheet for HLK\n",
      "|     16m 49s \tblank enrichment sheet for JRD\n",
      "|     16m 49s \tblank enrichment sheet for JYa\n",
      "|     16m 49s \tblank enrichment sheet for NFa\n",
      "|     16m 49s \tblank enrichment sheet for NPL\n",
      "|     16m 49s \tblank enrichment sheet for NTN\n",
      "|     16m 49s \tblank enrichment sheet for NWS\n",
      "|     16m 49s \tblank enrichment sheet for PQD\n",
      "|     16m 49s \tblank enrichment sheet for QRa\n",
      "|     16m 49s \tblank enrichment sheet for SWR\n",
      "|     16m 49s \tOK: The used blank enrichment sheets have legal values\n",
      "|     16m 49s \tOK: The used blank enrichment sheets are consistent\n",
      "|     16m 49s \tOK: The used filled enrichment sheets have legal values\n",
      "|     16m 49s \tOK: The used filled enrichment sheets are consistent\n",
      "|     16m 49s \tOK: all enriched nodes where phrase nodes\n",
      "|     16m 49s \tOK: all enriched nodes occurred in the blank sheet\n",
      "|     16m 49s \tthere are no manual correction/enrichment annotations\n"
     ]
    }
   ],
   "source": [
    "utils.caption(0, \"\\tas {}\".format(vfile(\"{verb}\", sheetKind)[1]))\n",
    "objects_seen = collections.defaultdict(collections.Counter)\n",
    "sheetResults = read_enrich()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-resolution",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[45]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if not SCRIPT:\n",
    "    list(enrichFields.items())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-worth",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[46]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if not SCRIPT:\n",
    "    sheetResults[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the sheet results with the generic results in one single dictionary, keyed by node number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-tsunami",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[47]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".     17m 22s Combine the manual results with the generic results                            .\n",
      "..............................................................................................\n",
      "|     17m 22s \tAnnotations from sheets for 53743 nodes\n",
      "|     17m 22s \tMerging 221353 annotations from generic enrichment\n",
      "|     17m 22s \tResulting in annotations for 221353 nodes\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Combine the manual results with the generic results\")\n",
    "allResults = dict()\n",
    "for (n, *features) in sheetResults:\n",
    "    allResults[n] = features\n",
    "utils.caption(0, \"\\tAnnotations from sheets for {} nodes\".format(len(allResults)))\n",
    "utils.caption(\n",
    "    0, \"\\tMerging {} annotations from generic enrichment\".format(len(enrichFields))\n",
    ")\n",
    "for (n, features) in enrichFields.items():\n",
    "    if n in allResults:\n",
    "        continue\n",
    "    allResults[n] = features + (\"\", \"\", False)\n",
    "utils.caption(0, \"\\tResulting in annotations for {} nodes\".format(len(allResults)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Generate data\n",
    "\n",
    "We write the correction and enrichment data as a data module in text-fabric format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-narrow",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[48]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "newFeatures = list(enrich_fields.keys()) + [\"function\", \"f_correction\", \"s_manual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = dict(\n",
    "    title=\"Correction and enrichment features\",\n",
    "    description=\"Corrections, alternatives and additions to the ETCBC4b encoding of the Hebrew Bible\",\n",
    "    purpose=\"Support the decision process of assigning valence to verbs\",\n",
    "    method=\"Generated blank correction and enrichment spreadsheets with selected clauses\",\n",
    "    steps=\"sheets filled out by researcher; read back in by program; generated new features based on contents\",\n",
    "    author=\"The content and nature of the features are by Janet Dyk, the workflow is by Dirk Roorda\",\n",
    "    coreData=\"BHSA\",\n",
    "    coreVersion=VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaData = {\n",
    "    \"\": description,\n",
    "    \"valence\": {\n",
    "        \"description\": \"verbal valence main classification\",\n",
    "    },\n",
    "    \"predication\": {\n",
    "        \"description\": \"verbal function main classification\",\n",
    "    },\n",
    "    \"grammatical\": {\n",
    "        \"description\": \"constituent role main classification\",\n",
    "    },\n",
    "    \"original\": {\n",
    "        \"description\": \"default value before enrichment logic has been applied\",\n",
    "    },\n",
    "    \"lexical\": {\n",
    "        \"description\": \"additional lexical characteristics\",\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"description\": \"additional semantic characteristics\",\n",
    "    },\n",
    "    \"f_correction\": {\n",
    "        \"description\": \"whether the phrase function has been manually corrected\",\n",
    "    },\n",
    "    \"s_manual\": {\n",
    "        \"description\": \"whether the generated enrichment features have been manually changed\",\n",
    "    },\n",
    "    \"function\": {\n",
    "        \"description\": \"corrected phrase function, only present for phrases that were in a correction sheet\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for f in newFeatures:\n",
    "    metaData[f][\"valueType\"] = \"str\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-border",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[49]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeFeatures = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (node, featureVals) in allResults.items():\n",
    "    for (fName, fVal) in zip(newFeatures, featureVals):\n",
    "        fValRep = fVal\n",
    "        if type(fVal) is bool:\n",
    "            fValRep = \"y\" if fVal else \"\"\n",
    "        nodeFeatures.setdefault(fName, {})[node] = fValRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "RENAMES = [(\"function\", \"cfunction\")]\n",
    "for (oldF, newF) in RENAMES:\n",
    "    for data in (nodeFeatures, metaData):\n",
    "        data[newF] = data[oldF]\n",
    "        del data[oldF]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-pickup",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[50]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".     17m 42s Writing TF enrichment features                                                 .\n",
      "..............................................................................................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.caption(4, \"Writing TF enrichment features\")\n",
    "TF = Fabric(locations=thisTempTf, silent=True)\n",
    "TF.save(nodeFeatures=nodeFeatures, edgeFeatures={}, metaData=metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffs\n",
    "\n",
    "Check differences with previous versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-selling",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[51]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".     17m 47s Check differences with previous version                                        .\n",
      "..............................................................................................\n",
      "|     17m 47s \t9 features to add\n",
      "|     17m 47s \t\tcfunction\n",
      "|     17m 47s \t\tf_correction\n",
      "|     17m 47s \t\tgrammatical\n",
      "|     17m 47s \t\tlexical\n",
      "|     17m 47s \t\toriginal\n",
      "|     17m 47s \t\tpredication\n",
      "|     17m 47s \t\ts_manual\n",
      "|     17m 47s \t\tsemantic\n",
      "|     17m 47s \t\tvalence\n",
      "|     17m 47s \tno features to delete\n",
      "|     17m 47s \t0 features in common\n",
      "|     17m 47s Done\n"
     ]
    }
   ],
   "source": [
    "utils.checkDiffs(thisTempTf, thisTf, only=set(nodeFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliver\n",
    "\n",
    "Copy the new TF features from the temporary location where they have been created to their final destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-culture",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[52]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".     17m 52s Deliver features to /Users/dirk/github/etcbc/valence/tf/2021                   .\n",
      "..............................................................................................\n",
      "|     17m 52s \tvalence\n",
      "|     17m 52s \tpredication\n",
      "|     17m 52s \tgrammatical\n",
      "|     17m 52s \toriginal\n",
      "|     17m 52s \tlexical\n",
      "|     17m 52s \tsemantic\n",
      "|     17m 52s \tf_correction\n",
      "|     17m 52s \ts_manual\n",
      "|     17m 52s \tcfunction\n"
     ]
    }
   ],
   "source": [
    "utils.deliverFeatures(thisTempTf, thisTf, nodeFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-jersey",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[53]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(4, \"Load and compile the new TF features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".     18m 02s Load and compile the new TF features                                           .\n",
      "..............................................................................................\n",
      "This is Text-Fabric 8.5.13\n",
      "Api reference : https://annotation.github.io/text-fabric/tf/cheatsheet.html\n",
      "\n",
      "97 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s Dataset without structure sections in otext:no structure functions in the T-API\n",
      "   |     0.54s T valence              from ~/github/etcbc/valence/tf/2021\n",
      "   |     0.57s T predication          from ~/github/etcbc/valence/tf/2021\n",
      "   |     0.49s T grammatical          from ~/github/etcbc/valence/tf/2021\n",
      "   |     0.32s T original             from ~/github/etcbc/valence/tf/2021\n",
      "   |     0.41s T lexical              from ~/github/etcbc/valence/tf/2021\n",
      "   |     0.36s T semantic             from ~/github/etcbc/valence/tf/2021\n",
      "   |     0.25s T f_correction         from ~/github/etcbc/valence/tf/2021\n",
      "   |     0.25s T s_manual             from ~/github/etcbc/valence/tf/2021\n",
      "   |     0.31s T cfunction            from ~/github/etcbc/valence/tf/2021\n",
      "  7.49s All features loaded/computed - for details use loadLog()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Computed',\n",
       "  'computed-data',\n",
       "  ('C Computed', 'Call AllComputeds', 'Cs ComputedString')),\n",
       " ('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString')),\n",
       " ('Fabric', 'loading', ('TF',)),\n",
       " ('Locality', 'locality', ('L Locality',)),\n",
       " ('Nodes', 'navigating-nodes', ('N Nodes',)),\n",
       " ('Features',\n",
       "  'node-features',\n",
       "  ('F Feature', 'Fall AllFeatures', 'Fs FeatureString')),\n",
       " ('Search', 'search', ('S Search',)),\n",
       " ('Text', 'text', ('T Text',))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF = Fabric(locations=[coreTf, thisTf], modules=[\"\"])\n",
    "api = TF.load(\n",
    "    \"\"\"\n",
    "    lex gloss lex_utf8\n",
    "    sp vs lex rela typ\n",
    "    function\n",
    "\"\"\"\n",
    "    + \" \".join(nodeFeatures)\n",
    ")\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "Take the first 10 phrases and retrieve the corrected and uncorrected function feature.\n",
    "Note that the corrected function feature is only filled in, if it occurs in a clause in which a selected verb occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-agenda",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[54]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time - Time - True\n",
      "Pred - Pred - True\n",
      "Subj - Subj - True\n",
      "Objc - Objc - True\n",
      "Conj -  - True\n",
      "Subj -  - True\n",
      "Pred -  - True\n",
      "PreC -  - True\n",
      "Conj - None - False\n",
      "Subj - None - False\n"
     ]
    }
   ],
   "source": [
    "for i in list(F.otype.s(\"phrase\"))[0:10]:\n",
    "    print(\n",
    "        \"{} - {} - {}\".format(\n",
    "            F.function.v(i),\n",
    "            F.cfunction.v(i),\n",
    "            L.u(i, \"clause\")[0] in clause_verb,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-armor",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[55]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    stop(good=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We put all corrections and enrichments in a single CSV file for checking.\n",
    "\n",
    "We also generate a smaller CSV, with only the data for selected verbs in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-correlation",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[80]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(allResults, \"w\")\n",
    "g = open(selectedResults, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "NALLFIELDS = 17\n",
    "tpl = (\"{};\" * (NALLFIELDS - 1)) + \"{}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.caption(0, \"collecting constituents ...\")\n",
    "f.write(\n",
    "    tpl.format(\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"passage\",\n",
    "        \"verb(s) text\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        \"clause text\",\n",
    "        \"clause node\",\n",
    "    )\n",
    ")\n",
    "f.write(\n",
    "    tpl.format(\n",
    "        \"corrected\",\n",
    "        \"enriched\",\n",
    "        \"passage\",\n",
    "        \"-\",\n",
    "        \"object type\",\n",
    "        \"clause rela\",\n",
    "        \"clause type\",\n",
    "        \"phrase function (old)\",\n",
    "        \"phrase function (new)\",\n",
    "        \"phrase type\",\n",
    "        \"valence\",\n",
    "        \"predication\",\n",
    "        \"grammatical\",\n",
    "        \"original\",\n",
    "        \"lexical\",\n",
    "        \"semantic\",\n",
    "        \"object text\",\n",
    "        \"object node\",\n",
    "    )\n",
    ")\n",
    "i = 0\n",
    "h = 0\n",
    "j = 0\n",
    "c = 0\n",
    "d = 0\n",
    "CHUNK_SIZE = 10000\n",
    "sel_verbs = set(verbs)\n",
    "for cn in sorted(clause_verb):\n",
    "    c += 1\n",
    "    vrbs = sorted(clause_verb[cn])\n",
    "    lex_vrbs = {F.lex.v(verb).rstrip(\"[=\") for verb in vrbs}\n",
    "    selected = len(lex_vrbs & sel_verbs) != 0\n",
    "    if selected:\n",
    "        d += 1\n",
    "        sel_vrbs = [v for v in vrbs if F.lex.v(v).rstrip(\"[=\") in verbs]\n",
    "\n",
    "        g.write(\n",
    "            tpl.format(\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"{} {}:{}\".format(*T.sectionFromNode(cn)),\n",
    "                \" \".join(F.lex.v(verb) for verb in sel_vrbs),\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                T.text(L.d(cn, \"word\"), fmt=\"text-trans-plain\"),\n",
    "                cn,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    f.write(\n",
    "        tpl.format(\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"{} {}:{}\".format(*T.sectionFromNode(cn)),\n",
    "            \" \".join(F.lex.v(verb) for verb in vrbs),\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            T.text(L.d(cn, \"word\"), fmt=\"text-trans-plain\"),\n",
    "            cn,\n",
    "        )\n",
    "    )\n",
    "    for pn in L.d(cn, \"phrase\"):\n",
    "        i += 1\n",
    "        if selected:\n",
    "            h += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            utils.caption(\n",
    "                0,\n",
    "                \"{:>6} selected of {:>6} constituents in {:>5} selected of {:>5} clauses ...\".format(\n",
    "                    h, i, d, c\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        material = tpl.format(\n",
    "            \"COR\" if F.f_correction.v(pn) == \"y\" else \"\",\n",
    "            \"MAN\" if F.s_manual.v(pn) == \"y\" else \"\",\n",
    "            \"{} {}:{}\".format(*T.sectionFromNode(pn)),\n",
    "            \"\",\n",
    "            \"phrase\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            F.function.v(pn),\n",
    "            F.cfunction.v(pn),\n",
    "            F.typ.v(pn),\n",
    "            F.valence.v(pn),\n",
    "            F.predication.v(pn),\n",
    "            F.grammatical.v(pn),\n",
    "            F.original.v(pn),\n",
    "            F.lexical.v(pn),\n",
    "            F.semantic.v(pn),\n",
    "            T.text(L.d(pn, \"word\"), fmt=\"text-trans-plain\"),\n",
    "            pn,\n",
    "        )\n",
    "        f.write(material)\n",
    "        if selected:\n",
    "            g.write(material)\n",
    "    for scn in clause_objects[cn]:\n",
    "        i += 1\n",
    "        if selected:\n",
    "            h += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            utils.caption(0, \"{:>6} constituents in {:>5} clauses ...\".format(i, c))\n",
    "        material = tpl.format(\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"{} {}:{}\".format(*T.sectionFromNode(scn)),\n",
    "            \"\",\n",
    "            \"clause\",\n",
    "            F.rela.v(scn),\n",
    "            F.typ.v(scn),\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            F.valence.v(scn),\n",
    "            F.predication.v(scn),\n",
    "            F.grammatical.v(scn),\n",
    "            F.original.v(scn),\n",
    "            F.lexical.v(scn),\n",
    "            F.semantic.v(scn),\n",
    "            T.text(L.d(scn, \"word\"), fmt=\"text-trans-plain\"),\n",
    "            scn,\n",
    "        )\n",
    "        f.write(material)\n",
    "        if selected:\n",
    "            g.write(material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    29s collecting constituents ...\n",
      "    30s   2439 selected of  10000 constituents in   698 selected of  3065 clauses ...\n",
      "    30s   4980 selected of  20000 constituents in  1444 selected of  6135 clauses ...\n",
      "    31s   8078 selected of  30000 constituents in  2299 selected of  9096 clauses ...\n",
      "    32s  10451 selected of  40000 constituents in  2965 selected of 12043 clauses ...\n",
      "    33s  13045 selected of  50000 constituents in  3714 selected of 15059 clauses ...\n",
      "    34s  15631 selected of  60000 constituents in  4470 selected of 18127 clauses ...\n",
      "    35s  18574 selected of  70000 constituents in  5306 selected of 21091 clauses ...\n",
      "    35s  21265 selected of  80000 constituents in  6133 selected of 24167 clauses ...\n",
      "    36s  24093 selected of  90000 constituents in  6962 selected of 27182 clauses ...\n",
      "    37s  26843 selected of 100000 constituents in  7780 selected of 30269 clauses ...\n",
      "    38s  29681 selected of 110000 constituents in  8674 selected of 33442 clauses ...\n",
      "    39s  31693 selected of 120000 constituents in  9317 selected of 36908 clauses ...\n",
      "    40s  33939 selected of 130000 constituents in 10005 selected of 40260 clauses ...\n",
      "    41s  36358 selected of 140000 constituents in 10736 selected of 43403 clauses ...\n",
      "    41s  39112 selected of 150000 constituents in 11553 selected of 46527 clauses ...\n",
      "    42s  41480 selected of 160000 constituents in 12281 selected of 49785 clauses ...\n",
      "    43s  43089 selected of 170000 constituents in 12801 selected of 53363 clauses ...\n",
      "    44s  44771 selected of 180000 constituents in 13374 selected of 56977 clauses ...\n",
      "    45s  46274 selected of 190000 constituents in 13878 selected of 60445 clauses ...\n",
      "    46s  48402 selected of 200000 constituents in 14534 selected of 63713 clauses ...\n",
      "    46s  50643 selected of 210000 constituents in 15165 selected of 66711 clauses ...\n",
      "    47s  53369 selected of 220000 constituents in 15936 selected of 69702 clauses ...\n",
      "    47s  53802 selected of 221472 constituents in 16053 selected of 70131 clauses done\n"
     ]
    }
   ],
   "source": [
    "f.close()\n",
    "g.close()\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"{:>6} selected of {:>6} constituents in {:>5} selected of {:>5} clauses done\".format(\n",
    "        h, i, d, c\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-battle",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[81]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cmpl\n",
      "True\n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = 671522\n",
    "print(pf_corr.get(x, F.function.v(x)))\n",
    "print(is_lex_local(\"FJM\", x))\n",
    "print(x in rule_cases[\"specific\"][\"phrase\"][(\"FJM\", 2)])\n",
    "print(F.lexical.v(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
