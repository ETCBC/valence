{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/tf-small.png\"/>\n",
    "\n",
    "# Corrections and enrichment\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://github.com/ETCBC/valence/blob/master/notebooks/flowchart.ipynb)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "We also need to enrich constituents surrounding the \n",
    "verb occurrences with higher level features, that can be used\n",
    "as input for the flow chart decisions.\n",
    "\n",
    "Read more in the [wiki](https://github.com/ETCBC/valence/wiki/Workflows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Janet Dyk and Dirk Roorda](https://github.com/ETCBC/valence/wiki/Authors)\n",
    "\n",
    "Last modified 2017-09-13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[References](https://github.com/ETCBC/valence/wiki/References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We carry out the valence project against the Hebrew Text Database of the ETCBC, version 4b.\n",
    "See the description of the [sources](https://github.com/ETCBC/valence/wiki/Sources)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Start the engines. We use the Python package \n",
    "[text-fabric](https://github.com/ETCBC/text-fabric)\n",
    "to process the data of the Hebrew Text Database smoothly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, collections\n",
    "from copy import deepcopy\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.10\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "111 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "ETCBC = f'hebrew/{source}{version}'\n",
    "TF = Fabric(locations='~/github/text-fabric-data-legacy', modules=ETCBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.31s B lex_utf8             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.17s B lex                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.17s B gloss                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.17s B sp                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.15s B vs                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.16s B uvf                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.16s B prs                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.11s B nametype             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.16s B ls                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.12s B function             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.27s B rela                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.29s B typ                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.27s B mother               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s Feature overview: 105 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  8.01s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    lex gloss lex_utf8\n",
    "    sp vs lex uvf prs nametype ls\n",
    "    function rela typ\n",
    "    mother\n",
    "''')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ln_base = 'https://shebanq.ancient-data.org/hebrew/text'\n",
    "ln_tpl = '?book={}&chapter={}&verse={}'\n",
    "ln_tweak = '&version=4b&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x'\n",
    "\n",
    "home_dir = os.path.expanduser('~').replace('\\\\', '/')\n",
    "base_dir = '{}/github/valence/workflow'.format(home_dir)\n",
    "result_dir = '{}/results'.format(base_dir)\n",
    "all_results = '{}/all.csv'.format(result_dir)\n",
    "selected_results = '{}/selected.csv'.format(result_dir)\n",
    "kinds = ('corr_blank', 'corr_filled', 'enrich_blank', 'enrich_filled')\n",
    "kdir = {}\n",
    "for k in kinds:\n",
    "    kd = '{}/{}'.format(base_dir, k)\n",
    "    kdir[k] = kd\n",
    "    if not os.path.exists(kd):\n",
    "        os.makedirs(kd)\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "def vfile(verb, kind):\n",
    "    if kind not in kinds:\n",
    "        error('Unknown kind `{}`'.format(kind))\n",
    "        return None\n",
    "    return '{}/{}_{}{}.csv'.format(kdir[kind], verb.replace('>','a').replace('<', 'o'), source, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain\n",
    "Here is a subset of verbs that interest us.\n",
    "In fact, we are interested in all verbs, but we have subjected the occurrences of these verbs to closer inspection, \n",
    "together with the contexts they occur in.\n",
    "\n",
    "Manual additions in the correction and enrichment workflow can only happen for selected verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs_initial = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())\n",
    "\n",
    "motion_verbs = set('''\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "'''.strip().split())\n",
    "\n",
    "double_object_verbs = set('''\n",
    "    NTN\n",
    "    <FH\n",
    "    FJM\n",
    "'''.strip().split())\n",
    "\n",
    "complex_qal_verbs = set('''\n",
    "    NF>\n",
    "    PQD\n",
    "'''.strip().split())\n",
    "\n",
    "verbs = verbs_initial | motion_verbs | double_object_verbs | complex_qal_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Correction workflow\n",
    "\n",
    "## 1.1 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicate_functions = {\n",
    "    'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_values = dict(\n",
    "    function={F.function.v(p) for p in F.otype.s('phrase')},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_values = dict(\n",
    "    function=dict(\n",
    "        BoundErr='this constituent is part of another constituent and does not merit its own function/type/rela value',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the error_values to the legal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    34s {'function': {'Time', 'Ques', 'EPPr', 'BoundErr', 'Objc', 'PrcS', 'PtcO', 'ModS', 'Voct', 'IntS', 'Intj', 'PreO', 'NCop', 'Supp', 'Loca', 'Subj', 'Nega', 'Conj', 'PrAd', 'Exst', 'Modi', 'ExsS', 'Rela', 'NCoS', 'Adju', 'PreC', 'Frnt', 'Pred', 'PreS', 'Cmpl'}}\n"
     ]
    }
   ],
   "source": [
    "for feature in set(legal_values.keys()) | set(error_values.keys()):\n",
    "    ev = error_values.get(feature, {})\n",
    "    if ev:\n",
    "        lv = legal_values.setdefault(feature, set())\n",
    "        lv |= set(ev.keys())\n",
    "info('{}'.format(legal_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    37s Finding occurrences ...\n",
      "    39s Done\n",
      "All:      1380 verbs with  73679 verb occurrences in 70131 clauses\n",
      "Selected:   18 verbs with  16209 verb occurrences in 16053 clauses\n",
      "    39s <BR   556 occurrences of which   33 outside a predicate phrase\n",
      "    39s <FH  2629 occurrences of which   59 outside a predicate phrase\n",
      "    39s <LH   890 occurrences of which   10 outside a predicate phrase\n",
      "    39s BR>    54 occurrences of which    3 outside a predicate phrase\n",
      "    39s BW>  2570 occurrences of which   27 outside a predicate phrase\n",
      "    39s CJT    85 occurrences of which    1 outside a predicate phrase\n",
      "    39s CWB  1056 occurrences of which   22 outside a predicate phrase\n",
      "    39s FJM   609 occurrences of which    3 outside a predicate phrase\n",
      "    39s HLK  1554 occurrences of which   30 outside a predicate phrase\n",
      "    39s JRD   377 occurrences of which   16 outside a predicate phrase\n",
      "    39s JY>  1069 occurrences of which   32 outside a predicate phrase\n",
      "    39s NF>   656 occurrences of which   52 outside a predicate phrase\n",
      "    39s NPL   445 occurrences of which   11 outside a predicate phrase\n",
      "    39s NTN  2017 occurrences of which   10 outside a predicate phrase\n",
      "    39s NWS   159 occurrences of which    4 outside a predicate phrase\n",
      "    39s PQD   303 occurrences of which   72 outside a predicate phrase\n",
      "    39s QR>   883 occurrences of which   12 outside a predicate phrase\n",
      "    39s SWR   297 occurrences of which    1 outside a predicate phrase\n"
     ]
    }
   ],
   "source": [
    "info('Finding occurrences ...')\n",
    "occs = collections.defaultdict(list)   # dictionary of verb occurrence nodes per verb lexeme\n",
    "npoccs = collections.defaultdict(list) # same, but those not occurring in a \"predicate\"\n",
    "clause_verb = collections.defaultdict(list)    # dictionary of verb occurrence nodes per clause node\n",
    "sel_clause_verb = collections.defaultdict(list)    # dictionary of selected verb occurrence nodes per clause node\n",
    "clause_verb_index = collections.defaultdict(set) # mapping from clauses to its main verb(s)\n",
    "sel_clause_verb_index = collections.defaultdict(set) # mapping from clauses to its main verb(s), for selected verbs\n",
    "verb_clause_index = collections.defaultdict(list) # mapping from verbs to the clauses of which it is main verb\n",
    "sel_verb_clause_index = collections.defaultdict(list) # mapping from selected verbs to the clauses of which it is main verb\n",
    "\n",
    "nw = 0\n",
    "sel_nw = 0\n",
    "for w in F.otype.s('word'):\n",
    "    if F.sp.v(w) != 'verb': continue\n",
    "    lex = F.lex.v(w).rstrip('[=')\n",
    "    nw += 1\n",
    "    pf = F.function.v(L.u(w, 'phrase')[0])\n",
    "    if pf not in predicate_functions:\n",
    "        npoccs[lex].append(w)\n",
    "    occs[lex].append(w)\n",
    "    cn = L.u(w, 'clause')[0]\n",
    "    clause_verb[cn].append(w)\n",
    "    clause_verb_index[cn].add(lex)\n",
    "    verb_clause_index[lex].append(cn)\n",
    "    if lex in verbs:\n",
    "        sel_nw += 1\n",
    "        sel_clause_verb[cn].append(w)\n",
    "        sel_clause_verb_index[cn].add(lex)\n",
    "\n",
    "sel_verb_clause_index = dict((lex, cns) for (lex, cns) in verb_clause_index.items() if lex in verbs)\n",
    "sel_clause_verb\n",
    "\n",
    "info('Done')\n",
    "info('All:      {:>4} verbs with {:>6} verb occurrences in {} clauses'.format(\n",
    "    len(verb_clause_index), nw, len(clause_verb)), tm=False)\n",
    "info('Selected: {:>4} verbs with {:>6} verb occurrences in {} clauses'.format(\n",
    "    len(sel_verb_clause_index), sel_nw, len(sel_clause_verb)), tm=False)\n",
    "\n",
    "for verb in sorted(verbs):\n",
    "    info('{} {:>5} occurrences of which {:>4} outside a predicate phrase'.format(\n",
    "        verb, \n",
    "        len(occs[verb]),\n",
    "        len(npoccs[verb]),\n",
    "        tm=False,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5m 41s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/CWB_etcbc4b.csv\n",
      " 5m 41s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/JYa_etcbc4b.csv\n",
      " 5m 41s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/NPL_etcbc4b.csv\n",
      " 5m 41s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/CJT_etcbc4b.csv\n",
      " 5m 41s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/BRa_etcbc4b.csv\n",
      " 5m 41s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/BWa_etcbc4b.csv\n",
      " 5m 42s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/FJM_etcbc4b.csv\n",
      " 5m 42s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/SWR_etcbc4b.csv\n",
      " 5m 42s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/QRa_etcbc4b.csv\n",
      " 5m 42s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/NWS_etcbc4b.csv\n",
      " 5m 42s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/NTN_etcbc4b.csv\n",
      " 5m 42s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/oLH_etcbc4b.csv\n",
      " 5m 43s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/NFa_etcbc4b.csv\n",
      " 5m 43s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/HLK_etcbc4b.csv\n",
      " 5m 43s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/JRD_etcbc4b.csv\n",
      " 5m 43s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/PQD_etcbc4b.csv\n",
      " 5m 44s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/oFH_etcbc4b.csv\n",
      " 5m 44s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/oBR_etcbc4b.csv\n",
      " 5m 44s 52110  phrases seen 1  time(s)\n",
      " 5m 44s 181    phrases seen 2  time(s)\n",
      " 5m 44s 9      phrases seen 3  time(s)\n",
      " 5m 44s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u(wn, 'clause')[0]\n",
    "        if cln in clauses_seen: continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u(wn, 'verse')[0]\n",
    "        bn = L.u(wn, 'book')[0]\n",
    "        (bookName, ch, vs) = T.sectionFromNode(vn, lang='la')\n",
    "        passage_label = '{} {}:{}'.format(*T.sectionFromNode(vn))\n",
    "        ln = ln_base+(ln_tpl.format(bookName, ch, vs))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.text([wn], fmt='text-trans-plain')\n",
    "        vstem = F.vs.v(wn)\n",
    "        np = '* ' if wn in npoccs[verb] else ''\n",
    "        row = [cln, wn, passage_label, lnx, np+vt, vstem]\n",
    "        phrases = L.d(cln, 'phrase')\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            phrases_seen[pn] += 1\n",
    "            pt = T.text(L.d(pn, 'word'), fmt='text-trans-plain')\n",
    "            pf = F.function.v(pn)\n",
    "            pnp = np if pf in predicate_functions else ''\n",
    "            row.extend((pn, pnp+pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    filename = vfile(verb, 'corr_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    info('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)\n",
    "    \n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    info('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "info('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 1.3 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1h 59m 34s Processing /Users/dirk/github/valence/workflow/corr_filled/oBR_etcbc4b.csv\n",
      " 1h 59m 34s <BR: Found    21 corrections in /Users/dirk/github/valence/workflow/corr_filled/oBR_etcbc4b.csv\n",
      " 1h 59m 34s OK: node numbers in sheet are consistent\n",
      " 1h 59m 34s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 34s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 34s OK: all corrected values are legal\n",
      " 1h 59m 34s Processing /Users/dirk/github/valence/workflow/corr_filled/oFH_etcbc4b.csv\n",
      " 1h 59m 35s <FH: Found   756 corrections in /Users/dirk/github/valence/workflow/corr_filled/oFH_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1h 59m 35s NO file /Users/dirk/github/valence/workflow/corr_filled/oLH_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/BRa_etcbc4b.csv\n",
      " 1h 59m 35s BR>: Found   760 corrections in /Users/dirk/github/valence/workflow/corr_filled/BRa_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/BWa_etcbc4b.csv\n",
      " 1h 59m 35s BW>: Found   815 corrections in /Users/dirk/github/valence/workflow/corr_filled/BWa_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/CJT_etcbc4b.csv\n",
      " 1h 59m 35s CJT: Found   818 corrections in /Users/dirk/github/valence/workflow/corr_filled/CJT_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/CWB_etcbc4b.csv\n",
      " 1h 59m 35s CWB: Found   863 corrections in /Users/dirk/github/valence/workflow/corr_filled/CWB_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/FJM_etcbc4b.csv\n",
      " 1h 59m 35s FJM: Found   985 corrections in /Users/dirk/github/valence/workflow/corr_filled/FJM_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/HLK_etcbc4b.csv\n",
      " 1h 59m 35s HLK: Found  1144 corrections in /Users/dirk/github/valence/workflow/corr_filled/HLK_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1h 59m 35s NO file /Users/dirk/github/valence/workflow/corr_filled/JRD_etcbc4b.csv\n",
      " 1h 59m 35s NO file /Users/dirk/github/valence/workflow/corr_filled/JYa_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/NFa_etcbc4b.csv\n",
      " 1h 59m 35s NF>: Found  1245 corrections in /Users/dirk/github/valence/workflow/corr_filled/NFa_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1h 59m 35s NO file /Users/dirk/github/valence/workflow/corr_filled/NPL_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/NTN_etcbc4b.csv\n",
      " 1h 59m 35s NTN: Found  1390 corrections in /Users/dirk/github/valence/workflow/corr_filled/NTN_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/NWS_etcbc4b.csv\n",
      " 1h 59m 35s NWS: Found  1401 corrections in /Users/dirk/github/valence/workflow/corr_filled/NWS_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/PQD_etcbc4b.csv\n",
      " 1h 59m 35s PQD: Found  1427 corrections in /Users/dirk/github/valence/workflow/corr_filled/PQD_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/QRa_etcbc4b.csv\n",
      " 1h 59m 35s QR>: Found  1430 corrections in /Users/dirk/github/valence/workflow/corr_filled/QRa_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Processing /Users/dirk/github/valence/workflow/corr_filled/SWR_etcbc4b.csv\n",
      " 1h 59m 35s SWR: Found  1455 corrections in /Users/dirk/github/valence/workflow/corr_filled/SWR_etcbc4b.csv\n",
      " 1h 59m 35s OK: node numbers in sheet are consistent\n",
      " 1h 59m 35s OK: Corrected phrases did not receive multiple corrections\n",
      " 1h 59m 35s OK: all corrected nodes where phrase nodes\n",
      " 1h 59m 35s OK: all corrected values are legal\n",
      " 1h 59m 35s Found 1455 corrections in the phrase function\n",
      " 1h 59m 35s 42851  phrases seen 1  time(s)\n",
      " 1h 59m 35s 138    phrases seen 2  time(s)\n",
      " 1h 59m 35s 3      phrases seen 3  time(s)\n",
      " 1h 59m 35s Total phrases seen: 42992\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "pf_corr = {}\n",
    "\n",
    "def read_corr():\n",
    "    function_values = legal_values['function']\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        repeated = collections.defaultdict(list)\n",
    "        non_phrase = set()\n",
    "        illegal_fvalue = set()\n",
    "        nodeNumberErrors = []\n",
    "\n",
    "        filename = vfile(verb, 'corr_filled')\n",
    "        if not os.path.exists(filename):\n",
    "            error('NO file {}'.format(filename))\n",
    "            continue\n",
    "        else:\n",
    "            info('Processing {}'.format(filename))\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for (i, line) in enumerate(f):\n",
    "                fields = line.rstrip().split(';')\n",
    "                cn = int(fields[0])\n",
    "                wn = int(fields[1])\n",
    "                if F.otype.v(cn) != 'clause':\n",
    "                    nodeNumberErrors.append([i, '{} is not a clause node'.format(cn)])\n",
    "                if F.otype.v(wn) != 'word':\n",
    "                    nodeNumberErrors.append([i, '{} is not a word node'.format(wn)])\n",
    "                words = set(L.d(cn, 'word'))\n",
    "                phrases = set(L.d(cn, 'phrase'))\n",
    "                if wn not in words:\n",
    "                    nodeNumberErrors.append([i, '{} is not a word of clause {}'.format(wn, cn)])\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[2+4*i], fields[2+4*i+3])\n",
    "                    if pn != '':\n",
    "                        pn = int(pn)\n",
    "                        if F.otype.v(pn) != 'phrase':\n",
    "                            nodeNumberErrors.append([i, '{} is not a phrase node'.format(pn)])\n",
    "                        if pn not in phrases:\n",
    "                            nodeNumberErrors.append([i, '{} is not a phrase of clause {}'.format(pn, cn)])\n",
    "                        pc = pc.strip()\n",
    "                        phrases_seen[pn] += 1\n",
    "                        if pc != '':\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        info('{}: Found {:>5} corrections in {}'.format(verb, len(pf_corr), filename))\n",
    "        if len(nodeNumberErrors):\n",
    "            for (i, msg) in nodeNumberErrors:\n",
    "                error('Line {:>3}: {}'.format(i+1, msg))\n",
    "        else:\n",
    "            info('OK: node numbers in sheet are consistent')\n",
    "        if len(repeated):\n",
    "            error('ERROR: Some phrases have been corrected multiple times!')\n",
    "            for x in sorted(repeated):\n",
    "                error('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "        else:\n",
    "            info('OK: Corrected phrases did not receive multiple corrections')\n",
    "        if len(non_phrase):\n",
    "            error('ERROR: Corrections have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "        else:\n",
    "            info('OK: all corrected nodes where phrase nodes')\n",
    "        if len(illegal_fvalue):\n",
    "            error('ERROR: Some corrections supply illegal values for phrase function!')\n",
    "            error('`{}`'.format('`, `'.join(illegal_fvalue)))\n",
    "        else:\n",
    "            info('OK: all corrected values are legal')\n",
    "    info('Found {} corrections in the phrase function'.format(len(pf_corr)))\n",
    "        \n",
    "read_corr()\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    info('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "info('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Enrichment workflow\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1h 59m 42s 6 Enrich field specifications OK\n",
      "valence = {adjunct, complement, core}\n",
      "predication = {NA, copula, regular}\n",
      "grammatical = {*, K_object, L_object, NA, NP_direct_object, direct_object, indirect_object, infinitive_object, principal_direct_object, subject}\n",
      "original = {*, K_object, L_object, NA, NP_direct_object, direct_object, indirect_object, infinitive_object, principal_direct_object, subject}\n",
      "lexical = {location, time}\n",
      "semantic = {benefactive, instrument, location, manner, time}\n"
     ]
    }
   ],
   "source": [
    "enrich_field_spec = '''\n",
    "valence\n",
    "    adjunct\n",
    "    complement\n",
    "    core\n",
    "\n",
    "predication\n",
    "    NA\n",
    "    regular\n",
    "    copula\n",
    "\n",
    "grammatical\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "original\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "lexical\n",
    "    location\n",
    "    time\n",
    "\n",
    "semantic\n",
    "    benefactive\n",
    "    time\n",
    "    location\n",
    "    instrument\n",
    "    manner\n",
    "'''\n",
    "enrich_fields = collections.OrderedDict()\n",
    "cur_e = None\n",
    "for line in enrich_field_spec.strip().split('\\n'):\n",
    "    if line.startswith(' '):\n",
    "        enrich_fields.setdefault(cur_e, set()).add(line.strip())\n",
    "    else:\n",
    "        cur_e = line.strip()\n",
    "nef = len(enrich_fields)\n",
    "if None in enrich_fields:\n",
    "    error('Invalid enrich field specification')\n",
    "else:\n",
    "    info('{} Enrich field specifications OK'.format(nef))\n",
    "for ef in enrich_fields:\n",
    "    info('{} = {{{}}}'.format(ef, ', '.join(sorted(enrich_fields[ef]))), tm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enrich_baseline_rules = dict(\n",
    "    phrase='''Adju\tAdjunct\tadjunct\tNA\tNA\t\t\t\n",
    "Cmpl\tComplement\tcomplement\tNA\t*\t\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\t\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\tNA\t\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Exst\tExistence\tcore\tcopula\tNA\t\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\t\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\t\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tNA\t\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\t\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\tNA\t\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\t\tNA\tNA\n",
    "Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\tNA\t\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PreC\tPredicate complement\tcore\tregular\tNA\t\t\t\n",
    "Pred\tPredicate\tcore\tregular\tNA\t\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\t\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\t\tNA\tNA\n",
    "Subj\tSubject\tcore\tNA\tsubject\t\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\tNA\t\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\tNA\t\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\t\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\t\tNA\tNA''',\n",
    "    clause='''Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "InfC\tInfinitive Construct clause\tNA\tNA\t\t\t\t''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1h 59m 47s Enrich baseline rules are OK (204 good)\n"
     ]
    }
   ],
   "source": [
    "transform = collections.OrderedDict((('phrase', {}), ('clause', {})))\n",
    "errors = 0\n",
    "good = 0\n",
    "\n",
    "for kind in ('phrase', 'clause'):\n",
    "    for line in enrich_baseline_rules[kind].split('\\n'):\n",
    "        x = line.split('\\t')\n",
    "        nefields = len(x) - 2\n",
    "        if len(x) - 2 != nef:\n",
    "            error('Wrong number of fields ({} must be {}) in {}:\\n{}'.format(nefields, nef, kind, line))\n",
    "            errors += 1\n",
    "        transform[kind][x[0]] = dict(zip(enrich_fields, x[2:]))\n",
    "    for e in error_values['function']:\n",
    "        transform[kind][e] = dict(zip(enrich_fields, ['']*nef))\n",
    "\n",
    "    for f in transform[kind]:\n",
    "        for e in enrich_fields:\n",
    "            val = transform[kind][f][e]\n",
    "            if val != '' and val != 'NA' and val not in enrich_fields[e]:\n",
    "                error('Defaults for `{}` ({}): wrong `{}` value: \"{}\"'.format(f, kind, e, val))\n",
    "                errors += 1\n",
    "            else: good += 1\n",
    "if errors:\n",
    "    error('There were {} errors ({} good)'.format(errors, good))\n",
    "else:\n",
    "    info('Enrich baseline rules are OK ({} good)'.format(good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prettyprint the baseline rules of enrichment for easier reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func    : valence        predication    grammatical    original       lexical        semantic       \n",
      "[phrase]\n",
      "Adju    : adjunct        NA             NA                                                          \n",
      "BoundErr:                                                                                           \n",
      "Cmpl    : complement     NA             *                                                           \n",
      "Conj    : NA             NA             NA                            NA             NA             \n",
      "EPPr    : NA             copula         NA                                                          \n",
      "ExsS    : core           copula         subject                                                     \n",
      "Exst    : core           copula         NA                                                          \n",
      "Frnt    : NA             NA             NA                            NA             NA             \n",
      "IntS    : core           NA             subject                                                     \n",
      "Intj    : NA             NA             NA                            NA             NA             \n",
      "Loca    : adjunct        NA             NA                            location       location       \n",
      "ModS    : core           NA             subject                                                     \n",
      "Modi    : NA             NA             NA                            NA             NA             \n",
      "NCoS    : core           copula         subject                                                     \n",
      "NCop    : core           copula         NA                                                          \n",
      "Nega    : NA             NA             NA                            NA             NA             \n",
      "Objc    : complement     NA             direct_object                                               \n",
      "PrAd    : adjunct        NA             NA                                                          \n",
      "PrcS    : core           regular        subject                                                     \n",
      "PreC    : core           regular        NA                                                          \n",
      "PreO    : core           regular        direct_object                                               \n",
      "PreS    : core           regular        subject                                                     \n",
      "Pred    : core           regular        NA                                                          \n",
      "PtcO    : core           regular        direct_object                                               \n",
      "Ques    : NA             NA             NA                            NA             NA             \n",
      "Rela    : NA             NA             NA                            NA             NA             \n",
      "Subj    : core           NA             subject                                                     \n",
      "Supp    : adjunct        NA             NA                                           benefactive    \n",
      "Time    : adjunct        NA             NA                            time           time           \n",
      "Unkn    : NA             NA             NA                            NA             NA             \n",
      "Voct    : NA             NA             NA                            NA             NA             \n",
      "[clause]\n",
      "BoundErr:                                                                                           \n",
      "InfC    : NA             NA                                                                         \n",
      "Objc    : complement     NA             direct_object                                               \n"
     ]
    }
   ],
   "source": [
    "ltpl = '{:<8}: '+('{:<15}' * nef)\n",
    "info(ltpl.format('func', *enrich_fields), tm=False)\n",
    "for kind in transform:\n",
    "    info('[{}]'.format(kind), tm=False)\n",
    "    for f in sorted(transform[kind]):\n",
    "        sfs = transform[kind][f]\n",
    "        info(ltpl.format(f, *[sfs[sf] for sf in enrich_fields]), tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Enrichment logic\n",
    "\n",
    "We apply enrichment logic to *all* verbs, not only to selected verbs.\n",
    "But only selected verbs can receive manual enrichment enhancements.\n",
    "\n",
    "For some verbs, selected or not, additional logic specific to that verb can be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Direct objects\n",
    "\n",
    "We have to do some work to identify (multiple) direct objects and indirect objects.\n",
    "\n",
    "[More on direct objects](https://github.com/ETCBC/valence/wiki/Discussion#direct-objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objectfuncs = set('''\n",
    "Objc PreO PtcO\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_obj_preps = set('''\n",
    "K L\n",
    "'''.strip().split())\n",
    "\n",
    "no_prs = set('''\n",
    "absent n/a\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_parts = set('''\n",
    ">NP/ >P/ >PSJM/ >YB</ >ZN/\n",
    "<JN/ <NQ/ <RP/ <YM/ <YM==/\n",
    "BHN/ BHWN/ BVN/\n",
    "CD=/ CD===/ CKM/ CN/\n",
    "DD/\n",
    "GRGRT/ GRM/ GRWN/ GW/ GW=/ GWJH/ GWPH/ GXWN/\n",
    "FPH/\n",
    "JD/ JRK/ JRKH/\n",
    "KRF/ KSL=/ KTP/\n",
    "L</ LCN/ LCWN/ LXJ/\n",
    "M<H/ MPRQT/ MTL<WT/ MTNJM/ MYX/\n",
    "NBLH=/\n",
    "P<M/ PGR/ PH/ PM/ PNH/ PT=/\n",
    "QRSL/\n",
    "R>C/ RGL/\n",
    "XDH/ XLY/ XMC=/ XRY/\n",
    "YW>R/\n",
    "ZRW</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1h 59m 58s Finding direct objects and determining the principal one\n",
      " 2h 00m 02s Done\n",
      " 3560 clauses with  1  principal object\n",
      "66571 clauses with  0  principal objects\n",
      " 3560 clauses with  a  principal object\n",
      "   12 clauses with  2     direct objects\n",
      "23861 clauses with  1     direct object\n",
      "46258 clauses with  0     direct objects\n",
      "23873 clauses with  a     direct object\n",
      "  980 clauses with  1         NP object\n",
      "69151 clauses with  0         NP objects\n",
      "  980 clauses with  a         NP object\n",
      "   30 clauses with  2          L objects\n",
      " 3552 clauses with  1          L object\n",
      "66549 clauses with  0          L objects\n",
      " 3582 clauses with  a          L object\n",
      "  100 clauses with  1          K object\n",
      "70031 clauses with  0          K objects\n",
      "  100 clauses with  a          K object\n",
      "   12 clauses with  2     clause objects\n",
      " 1369 clauses with  1     clause object\n",
      "68750 clauses with  0     clause objects\n",
      " 1381 clauses with  a     clause object\n",
      "    1 clauses with  3 infinitive objects\n",
      "   20 clauses with  2 infinitive objects\n",
      " 1195 clauses with  1 infinitive object\n",
      "68915 clauses with  0 infinitive objects\n",
      " 1216 clauses with  a infinitive object\n"
     ]
    }
   ],
   "source": [
    "info('Finding direct objects and determining the principal one')\n",
    "clause_objects = collections.defaultdict(set)\n",
    "objects = collections.defaultdict(set)\n",
    "objects_count = collections.defaultdict(collections.Counter)\n",
    "object_kinds = (\n",
    "    'principal',\n",
    "    'direct',\n",
    "    'NP',\n",
    "    'L',\n",
    "    'K',\n",
    "    'clause',\n",
    "    'infinitive',\n",
    ")\n",
    "\n",
    "def is_marked(phr):\n",
    "    # simple criterion for determining whether a direct object is marked:\n",
    "    # has it the object marker somewhere?\n",
    "    words = L.d(p, 'word')\n",
    "    has_et = False\n",
    "    for w in words:\n",
    "        if F.lex.v(w) == '>T':\n",
    "            has_et = True\n",
    "            break\n",
    "    return has_et\n",
    "\n",
    "for c in clause_verb:\n",
    "    these_objects = collections.defaultdict(set)\n",
    "    direct_objects_cat = collections.defaultdict(set)\n",
    "\n",
    "    for p in L.d(c, 'phrase'):\n",
    "        pf = pf_corr.get(p, F.function.v(p))  # NB we take the corrected value for phrase function if there is one\n",
    "        if pf in objectfuncs:\n",
    "            direct_objects_cat['p_'+pf].add(p)\n",
    "            these_objects['direct'].add(p)\n",
    "        elif pf == 'Cmpl':\n",
    "            pwords = L.d(p, 'word')\n",
    "            w1 = pwords[0]\n",
    "            w1l = F.lex.v(w1)\n",
    "            w2l = F.lex.v(pwords[1]) if len(pwords) > 1 else None\n",
    "            if w1l in cmpl_as_obj_preps and F.prs.v(w1) in no_prs and not (w1l == 'L' and w2l in body_parts):\n",
    "                if w1l == 'K': these_objects['K'].add(p)\n",
    "                elif w1l == 'L': these_objects['L'].add(p)\n",
    "        \n",
    "    # find clause objects\n",
    "    for ac in L.d(L.u(c, 'sentence')[0], 'clause'):\n",
    "        mothers = list(E.mother.f(ac))\n",
    "        if not (mothers and mothers[0] == c): continue\n",
    "        cr = F.rela.v(ac)\n",
    "        ct = F.typ.v(ac)\n",
    "        if cr in {'Objc'} or ct in {'InfC'}:\n",
    "            clause_objects[c].add(ac)\n",
    "            if cr in {'Objc'}:\n",
    "                label = cr\n",
    "                direct_objects_cat['c_'+label].add(ac)\n",
    "                these_objects['direct'].add(ac)\n",
    "                these_objects['clause'].add(ac)\n",
    "            elif ct in {'InfC'}:\n",
    "                if F.lex.v(L.d(ac, 'word')[0]) == 'L':\n",
    "                    these_objects['infinitive'].add(ac)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # order the objects in the natural ordering\n",
    "    direct_objects_order = sortNodes(these_objects.get('direct', set()))\n",
    "    nobjects = len(direct_objects_order)\n",
    "\n",
    "    # compute the principal object\n",
    "    principal_object = None\n",
    "\n",
    "    for x in [1]:\n",
    "        # just one object \n",
    "        if nobjects == 1:\n",
    "            # we have chosen not to mark a principal object if there is only one object\n",
    "            # the alternative is to mark it if it is a phrase. Uncomment the next 2 lines if you want this\n",
    "            # theobject = list(dobjects_set)[0]\n",
    "            # if F.otype.v(theobject) == 'phrase': principal_object = theobject\n",
    "            break\n",
    "        # rule 1: suffixes and promoted objects\n",
    "        principal_candidates =\\\n",
    "            direct_objects_cat.get('p_PreO', set()) |\\\n",
    "            direct_objects_cat.get('p_PtcO', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            principal_object = sortNodes(principal_candidates)[0]\n",
    "            break\n",
    "        principal_candidates = direct_objects_cat.get('p_Objc', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            if len(principal_candidates) == 1:\n",
    "                principal_object = list(principal_candidates)[0]\n",
    "                break\n",
    "            objects_marked = set()\n",
    "            objects_unmarked = set()\n",
    "            for p in principal_candidates:\n",
    "                if is_marked(p):\n",
    "                    objects_marked.add(p)\n",
    "                else:\n",
    "                    objects_unmarked.add(p)\n",
    "            if len(objects_marked) != 0:\n",
    "                principal_object = sortNodes(objects_marked)[0]\n",
    "                break\n",
    "            if len(objects_unmarked) != 0:\n",
    "                principal_object = sortNodes(objects_unmarked)[0]\n",
    "                break            \n",
    "    if principal_object != None:\n",
    "        these_objects['principal'].add(principal_object)\n",
    "    if len(these_objects['infinitive']) and not len(these_objects['direct']):\n",
    "        # we do not mark an infinitive object if there is no proper direct object around\n",
    "        these_objects['infinitive'] = set()\n",
    "    if len(these_objects['principal']):\n",
    "        these_objects['direct'] -= these_objects['principal']\n",
    "        for x in these_objects['direct'] - these_objects['clause']:\n",
    "            # the NP objects are the non-principal phrase like direct objects\n",
    "            these_objects['NP'].add(x)\n",
    "        these_objects['direct'] -= these_objects['NP']\n",
    "    if len(these_objects['principal']) == 0 and len(these_objects['direct']) and (\n",
    "        len(these_objects['NP']) or\\\n",
    "        len(these_objects['L']) or\\\n",
    "        len(these_objects['K']) or\\\n",
    "        len(these_objects['infinitive'])\n",
    "    ): # promote the direct objects to principal direct objects\n",
    "        these_objects['principal'] = these_objects['direct']\n",
    "        these_objects['direct'] = set()\n",
    "\n",
    "    for kind in object_kinds:\n",
    "        n = len(these_objects.get(kind, set()))\n",
    "        objects_count[kind][n] += 1\n",
    "        if n:\n",
    "            objects[kind] |= these_objects[kind]\n",
    "\n",
    "info('Done')\n",
    "\n",
    "for kind in object_kinds:\n",
    "    total = 0\n",
    "    for (count, n) in sorted(objects_count[kind].items(), key=lambda y: -y[0]):\n",
    "        if count: total += n\n",
    "        info('{:>5} clauses with {:>2} {:>10} object{}'.format(n, count, kind, 's' if count != 1 else ''), tm=False)\n",
    "    info('{:>5} clauses with {:>2} {:>10} object'.format(total, 'a', kind), tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Indirect objects\n",
    "\n",
    "The ETCBC database has not feature that marks indirect objects.\n",
    "We will use computation to determine whether a complement is an indirect object or a locative.\n",
    "This computation is just an approximation.\n",
    "\n",
    "[More on indirect objects](https://github.com/ETCBC/valence/wiki/Discussion#indirect-objects)\n",
    "\n",
    "### The decision\n",
    "\n",
    "We take a decision as follows.\n",
    "Based on indicators $ind$ and $loc$ that are proxies for the degree in which the complement is an indirect object or a locative, we arrive at a decision $L$ (complement is *locative*) or $I$ (complement is *indirect object*) or $C$ (complement is neither *locative* nor *indirect object*) as follows:\n",
    "\n",
    "(1) $ loc > 0 \\wedge ind = 0 \\Rightarrow L $\n",
    "\n",
    "(2) $ loc = 0 \\wedge ind > 0 \\Rightarrow I $\n",
    "\n",
    "(3) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc - 1 > ind \\Rightarrow L$\n",
    "\n",
    "(4) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc + 1 < ind \\Rightarrow I$\n",
    "\n",
    "(5) $ loc > 0 \\wedge ind > 0 \\wedge |ind - loc| <= 1 \\Rightarrow C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complfuncs = set('''\n",
    "Cmpl PreC\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_iobj_preps = set('''\n",
    "L >L\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locative_lexemes = set('''\n",
    ">RY/ >YL/ >XR/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJN/ BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "'''.strip().split())\n",
    "\n",
    "personal_lexemes = set('''\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/ MYRJ/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2h 00m 11s Determinig kind of complements\n",
      " 2h 00m 12s Done\n",
      "Phrases of kind C :  16508\n",
      "Phrases of kind L :  12393\n",
      "Phrases of kind I :   7465\n",
      "Total complements :  36366\n",
      "Total phrases     : 214555\n"
     ]
    }
   ],
   "source": [
    "info('Determinig kind of complements')\n",
    "\n",
    "complements_c = collections.defaultdict(lambda: collections.defaultdict(lambda: []))\n",
    "complements = {}\n",
    "complementk = {}\n",
    "kcomplements = collections.Counter()\n",
    "\n",
    "nphrases = 0\n",
    "ncomplements = 0\n",
    "\n",
    "for c in clause_verb:\n",
    "    for p in L.d(c, 'phrase'):\n",
    "        nphrases += 1\n",
    "        pf = pf_corr.get(p, F.function.v(p))\n",
    "        if pf not in complfuncs: continue\n",
    "        ncomplements += 1\n",
    "        words = L.d(p, 'word')\n",
    "        lexemes = [F.lex.v(w) for w in words]\n",
    "        lexeme_set = set(lexemes)\n",
    "\n",
    "        # measuring locativity\n",
    "        lex_locativity = len(locative_lexemes & lexeme_set)\n",
    "        prep_b = len([x for x in lexeme_set if x == 'B'])\n",
    "        topo = len([x for x in words if F.nametype.v(x) == 'topo'])\n",
    "        h_loc = len([x for x in words if F.uvf.v(x) == 'H'])\n",
    "        body_part = 0\n",
    "        if len(words) > 1 and F.lex.v(words[0]) == 'L' and F.lex.v(words[1]) in body_parts:\n",
    "            body_part = 2\n",
    "        loca = lex_locativity + topo + prep_b + h_loc + body_part\n",
    "\n",
    "        # measuring indirect object\n",
    "        prep_l = len([x for x in words if F.lex.v(x) in cmpl_as_iobj_preps and F.prs.v(x) not in no_prs])\n",
    "        prep_lpr = 0\n",
    "        lwn = len(words)\n",
    "        for (n, wn) in enumerate(words):\n",
    "            if F.lex.v(wn) in cmpl_as_iobj_preps:\n",
    "                if n+1 < lwn:\n",
    "                    nextw = words[n+1]\n",
    "                    if F.lex.v(nextw) in personal_lexemes or F.ls.v(nextw) == 'gntl' or (\n",
    "                        F.sp.v(nextw) == 'nmpr' and F.nametype.v(nextw) == 'pers'):\n",
    "                        prep_lpr += 1                        \n",
    "        indi = prep_l + prep_lpr\n",
    "\n",
    "        # the verdict\n",
    "        ckind = 'C'\n",
    "        if loca == 0 and indi > 0: ckind = 'I'\n",
    "        elif loca > 0 and indi == 0: ckind = 'L'\n",
    "        elif loca > indi + 1: ckind = 'L'\n",
    "        elif loca < indi - 1: ckind = 'I'\n",
    "        complementk[p] = (loca, indi, ckind)\n",
    "        kcomplements[ckind] += 1\n",
    "        complements_c[c][ckind].append(p)\n",
    "        complements[p] = (pf, ckind)\n",
    "\n",
    "info('Done')\n",
    "for (label, n) in sorted(kcomplements.items(), key=lambda y: -y[1]):\n",
    "    info('Phrases of kind {:<2}: {:>6}'.format(label, n), tm=False)\n",
    "info('Total complements : {:>6}'.format(ncomplements), tm=False)\n",
    "info('Total phrases     : {:>6}'.format(nphrases), tm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_L(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len(words) > 0 and F.lex.v(words[0] == 'L')\n",
    "\n",
    "def is_lex_personal(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len(words) > 1 and (F.lex.v(words[1]) in personal_lexemes or F.nametype.v(words[1]) == 'pers')\n",
    "\n",
    "def is_lex_local(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len({F.lex.v(w) for w in words} & locative_lexemes) > 0\n",
    "\n",
    "def has_H_locale(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len({w for w in words if F.uvf.v(w) == 'H'}) > 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Generic logic\n",
    "\n",
    "This is the function that applies the generic rules about (in)direct objects and locatives.\n",
    "It takes a phrase node and a set of new label values, and modifies those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grule_as_str = {\n",
    "    'pdos':   '''direct_object => principal_direct_object''',\n",
    "    'pdos-x': '''non-object => principal_direct_object''',\n",
    "    'ndos':   '''direct_object => NP_direct_object''',\n",
    "    'ndos-x': '''non-object => NP_direct_object''',\n",
    "    'dos':    '''non-object => direct_object''',\n",
    "    'ldos':   '''non-object => L_object''',\n",
    "    'kdos':   '''non-object => K_object''',\n",
    "    'inds-c': '''complement => indirect_object''',\n",
    "    'locs-c': '''complement => location''',\n",
    "    'inds-p': '''predicate complement => indirect_object''',\n",
    "    'locs-p': '''predicate complement => location''',\n",
    "    'cdos':   '''direct-object =(superfluously)=> direct object (clause)''',\n",
    "    'cdos-x': '''non-object => direct object (clause)''',\n",
    "    'idos':   '''infinitive_object =(superfluously)=> infinitive_object (clause)''',\n",
    "    'idos-x': '''infinitive clause => infinitive_object''',\n",
    "}\n",
    "\n",
    "def rule_as_str_g(x, i): return '{}-{}'.format(i, grule_as_str[i])\n",
    "\n",
    "rule_as_str = dict(\n",
    "    generic=rule_as_str_g,\n",
    ")\n",
    "\n",
    "def generic_logic_p(pn, values):\n",
    "    gl = None\n",
    "    if pn in objects['principal']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'pdos'\n",
    "        else:\n",
    "            gl = 'pdos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'principal_direct_object'\n",
    "    elif pn in objects['NP']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'ndos'\n",
    "        else:\n",
    "            gl = 'ndos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'NP_direct_object'\n",
    "    elif pn in objects['direct']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv != 'direct_object':\n",
    "            gl = 'dos'\n",
    "            values['original'] = oldv\n",
    "            values['grammatical'] = 'direct_object'\n",
    "    elif pn in objects['L']:\n",
    "        oldv = values['grammatical']\n",
    "        gl = 'ldos'\n",
    "        values['original'] = oldv\n",
    "        values['grammatical'] = 'L_object'\n",
    "    elif pn in objects['K']:\n",
    "        oldv = values['grammatical']\n",
    "        gl = 'kdos'\n",
    "        values['original'] = oldv\n",
    "        values['grammatical'] = 'K_object'\n",
    "    elif pn in complements:\n",
    "        (pf, ck) = complements[pn]\n",
    "        if ck in {'I', 'L'}:\n",
    "            if pf == 'Cmpl':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 'inds-c'\n",
    "                else:\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 'locs-c'\n",
    "            elif pf == 'PreC':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 'inds-p'\n",
    "                else:\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 'locs-p'\n",
    "    return gl\n",
    "\n",
    "def generic_logic_c(cn, values):\n",
    "    gl = None\n",
    "    if cn in objects['clause']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'cdos'\n",
    "        else:\n",
    "            gl = 'cdos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'direct_object'\n",
    "    elif cn in objects['infinitive']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'infinitive_object':\n",
    "            gl = 'idos'\n",
    "        else:\n",
    "            gl = 'idos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'infinitive_object'\n",
    "    return gl\n",
    "\n",
    "generic_logic = dict(\n",
    "    phrase=generic_logic_p,\n",
    "    clause=generic_logic_c,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Verb specific rules\n",
    "\n",
    "The verb-specific enrichment rules are stored in a dictionary, keyed  by the verb lexeme.\n",
    "The rule itself is a list of items.\n",
    "\n",
    "The last item is a tuple of conditions that need to be fulfilled to apply the rule.\n",
    "\n",
    "A condition can take the shape of\n",
    "\n",
    "* a function, taking a phrase or clause node as argument and returning a boolean value\n",
    "* an ETCBC feature for phrases or clauses : value, \n",
    "  which is true iff that feature has that value for the phrase or clause in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbl_obj_rules = (\n",
    "    (\n",
    "        ('semantic', 'benefactive'), \n",
    "        ('function:Adju', has_L, is_lex_personal),\n",
    "    ),\n",
    "    (\n",
    "        ('lexical', 'location'),\n",
    "        ('function:Cmpl', has_H_locale),\n",
    "    ),\n",
    "    (\n",
    "        ('lexical', 'location'),\n",
    "        ('semantic', 'location'),\n",
    "        ('function:Cmpl', is_lex_local),\n",
    "    ),\n",
    ")\n",
    "enrich_logic = dict(\n",
    "    phrase={\n",
    "        'CJT': dbl_obj_rules,\n",
    "        'FJM': dbl_obj_rules,\n",
    "    },\n",
    "    clause={\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CJT-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "CJT-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "CJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "FJM-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "FJM-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "FJM-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "All 6 rules OK\n"
     ]
    }
   ],
   "source": [
    "rule_index = collections.defaultdict(lambda: [])\n",
    "\n",
    "def rule_as_str_s(vl, i):\n",
    "    (conditions, sfassignments) = rule_index[vl][i]\n",
    "    label = '{}-{}\\n'.format(vl, i+1)\n",
    "    rule = '\\tIF   {}'.format('\\n\\tAND  '.join(\n",
    "        '{:<10} = {:<8}'.format(\n",
    "                *c.split(':')\n",
    "            ) if type(c) is str else '{:<15}'.format(\n",
    "                c.__name__\n",
    "            ) for c in conditions,\n",
    "    ))\n",
    "    ass = []\n",
    "    for (i, sfa) in enumerate(sfassignments):\n",
    "        ass.append('\\t\\t{:<10} => {:<15}\\n'.format(*sfa))\n",
    "    return '{}{}\\n\\tTHEN\\n{}'.format(label, rule, ''.join(ass))\n",
    "\n",
    "rule_as_str['specific'] = rule_as_str_s\n",
    "\n",
    "def check_logic():\n",
    "    errors = 0\n",
    "    nrules = 0\n",
    "    for kind in sorted(enrich_logic):\n",
    "        for vl in sorted(enrich_logic[kind]):\n",
    "            for items in enrich_logic[kind][vl]:\n",
    "                rule_index[vl].append((items[-1], items[0:-1]))\n",
    "            for (i, (conditions, sfassignments)) in enumerate(rule_index[vl]):\n",
    "                info(rule_as_str_s(vl, i), tm=False)\n",
    "                nrules += 1\n",
    "                for (sf, sfval) in sfassignments:\n",
    "                    if sf not in enrich_fields:\n",
    "                        error('{}: \"{}\" not a valid enrich field'.format(kind, sf), tm=False)\n",
    "                        errors += 1\n",
    "                    elif sfval not in enrich_fields[sf]:\n",
    "                        error('{}: `{}`: \"{}\" not a valid enrich field value'.format(kind, sf, sfval), tm=False)\n",
    "                        errors += 1\n",
    "                for c in conditions:\n",
    "                    if type(c) == str:\n",
    "                        x = c.split(':')\n",
    "                        if len(x) != 2:\n",
    "                            error('{}: Wrong feature condition {}'.format(kind, c), tm=False)\n",
    "                            errors += 1\n",
    "                        else:\n",
    "                            (feat, val) = x\n",
    "                            if feat not in legal_values:\n",
    "                                error('{}: Feature `{}` not in use'.format(kind, feat), tm=False)\n",
    "                                errors += 1\n",
    "                            elif val not in legal_values[feat]:\n",
    "                                error('{}: Feature `{}`: not a valid value \"{}\"'.format(kind, feat, val), tm=False)\n",
    "                                errors += 1\n",
    "    if errors:\n",
    "        error('There were {} errors in {} rules'.format(errors, nrules), tm=False)\n",
    "    else:\n",
    "        info('All {} rules OK'.format(nrules), tm=False)\n",
    "\n",
    "check_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rule_cases = collections.defaultdict(lambda: collections.defaultdict(lambda: {}))\n",
    "\n",
    "def apply_logic(kind, vl, n, init_values):\n",
    "    values = deepcopy(init_values)\n",
    "    gr = generic_logic[kind](n, values)\n",
    "    if gr:\n",
    "        rule_cases['generic'][kind].setdefault(('', gr), []).append(n)\n",
    "    verb_rules = enrich_logic[kind].get(vl, [])\n",
    "    for (i, items) in enumerate(verb_rules):\n",
    "        conditions = items[-1]\n",
    "        sfassignments = items[0:-1]\n",
    "\n",
    "        ok = True\n",
    "        for condition in conditions:\n",
    "            if type(condition) is str:\n",
    "                (feature, value) = condition.split(':')\n",
    "                if feature == 'function' and kind == 'phrase':\n",
    "                    fval = pf_corr.get(n, F.function.v(n))\n",
    "                else:\n",
    "                    fval = F.item[feature].v(n)\n",
    "                this_ok =  fval == value\n",
    "            else:\n",
    "                this_ok = condition(vl, n)\n",
    "            if not this_ok:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                values[sf] = sfval\n",
    "            rule_cases['specific'][kind].setdefault((vl, i), []).append(n)\n",
    "    return tuple(values[sf] for sf in enrich_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Generate enrichments\n",
    "\n",
    "First we generate enriched values for all relevant phrases.\n",
    "The generated enrichment values are computed on the basis of generic logic.\n",
    "Additionally, verb-bound logic is applied, if it has been specified.\n",
    "\n",
    "We store the enriched features in a dictionary, first keyed by the type of constituent that\n",
    "receives the enrichments (`phrase` or `clause`), and then by the node number of the constituent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2h 01m 25s Generated enrichment values for 1380 verbs:\n",
      " 2h 01m 25s Enriched values for 221472 nodes\n"
     ]
    }
   ],
   "source": [
    "seen = collections.defaultdict(collections.Counter)\n",
    "enrichFields = dict()\n",
    "\n",
    "def gen_enrich(verb):\n",
    "    clauses_seen = set()\n",
    "\n",
    "    for wn in occs[verb]:\n",
    "        cn = L.u(wn, 'clause')[0]\n",
    "        if cn in clauses_seen:\n",
    "            continue\n",
    "        clauses_seen.add(cn)\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        for pn in L.d(cn, 'phrase'):\n",
    "            seen['phrase'][pn] += 1\n",
    "            pf = pf_corr.get(pn, F.function.v(pn))\n",
    "            enrichFields[pn] = apply_logic('phrase', vl, pn, transform['phrase'][pf])\n",
    "        for scn in clause_objects[cn]:\n",
    "            seen['clause'][scn] += 1\n",
    "            scty = F.typ.v(scn)\n",
    "            scr = F.rela.v(scn)\n",
    "            enrichFields[scn] = apply_logic('clause', vl, scn, transform['clause'][scr if scr == 'Objc' else scty])       \n",
    "\n",
    "for verb in verb_clause_index:\n",
    "    gen_enrich(verb)\n",
    "info('Generated enrichment values for {} verbs:'.format(len(verb_clause_index)))\n",
    "info('Enriched values for {:>5} nodes'.format(len(enrichFields)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generic-phrase rules:\n",
      "2142 x\n",
      "\tndos-direct_object => NP_direct_object\n",
      "\t605443, 606421, 738015, 742447, 799852, 622314, 631236, 634158, 649968, 653775\n",
      "\n",
      "7482 x\n",
      "\tpdos-direct_object => principal_direct_object\n",
      "\t605445, 606422, 738016, 740139, 742446, 799850, 698336, 724586, 829609, 853657\n",
      "\n",
      "1252 x\n",
      "\tlocs-p-predicate complement => location\n",
      "\t737241, 605188, 605556, 606195, 606708, 606994, 607007, 607042, 607368, 607499\n",
      "\n",
      "23668 x\n",
      "\tlocs-c-complement => location\n",
      "\t766959, 605192, 606245, 606264, 607472, 607667, 607671, 607720, 607723, 607726\n",
      "\n",
      "7738 x\n",
      "\tldos-non-object => L_object\n",
      "\t605298, 605301, 605486, 607594, 609325, 609619, 612287, 613250, 614163, 617286\n",
      "\n",
      "11734 x\n",
      "\tinds-c-complement => indirect_object\n",
      "\t605484, 606900, 607603, 607614, 607825, 607831, 609283, 610627, 611058, 612472\n",
      "\n",
      " 856 x\n",
      "\tinds-p-predicate complement => indirect_object\n",
      "\t605590, 608308, 608396, 608780, 609243, 609363, 609628, 609697, 610628, 611059\n",
      "\n",
      " 214 x\n",
      "\tkdos-non-object => K_object\n",
      "\t609790, 627280, 632288, 747861, 764029, 766511, 770073, 771907, 776198, 784198\n",
      "\n",
      " 55086 generic-phrase rule applications\n",
      "generic-clause rules:\n",
      "2608 x\n",
      "\tidos-x-infinitive clause => infinitive_object\n",
      "\t471761, 504853, 513067, 513256, 446672, 446677, 496216, 505203, 427332, 433086\n",
      "\n",
      "2838 x\n",
      "\tcdos-direct-object =(superfluously)=> direct object (clause)\n",
      "\t466081, 476489, 502995, 427578, 434947, 454826, 456930, 467304, 471521, 484795\n",
      "\n",
      "  5446 generic-clause rule applications\n",
      " 60532 generic rule applications\n",
      "specific-phrase rules:\n",
      " 238 x\n",
      "\tFJM-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "\t605599, 608530, 611242, 611636, 611729, 615076, 615425, 616023, 616053, 621182\n",
      "\n",
      "   8 x\n",
      "\tFJM-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "\t632225, 636183, 636237, 766679, 632225, 636183, 636237, 766679\n",
      "\n",
      "  20 x\n",
      "\tCJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "\t605981, 606397, 619339, 630957, 654146, 731941, 776267, 789543, 794186, 797378\n",
      "\n",
      "   2 x\n",
      "\tCJT-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "\t731941, 731941\n",
      "\n",
      "   268 specific-phrase rule applications\n",
      "   268 specific rule applications\n",
      "204657 phrase seen 1  time(s)\n",
      "  9295 phrase seen 2  time(s)\n",
      "   563 phrase seen 3  time(s)\n",
      "    31 phrase seen 4  time(s)\n",
      "     9 phrase seen 5  time(s)\n",
      "214555 phrase seen in total\n",
      "  6729 clause seen 1  time(s)\n",
      "   177 clause seen 2  time(s)\n",
      "     9 clause seen 3  time(s)\n",
      "     2 clause seen 4  time(s)\n",
      "  6917 clause seen in total\n"
     ]
    }
   ],
   "source": [
    "for scope in rule_cases:\n",
    "    totalscope = 0\n",
    "    for kind in rule_cases[scope]:\n",
    "        info('{}-{} rules:'.format(scope, kind), tm=False)\n",
    "        totalkind = 0\n",
    "        for rule_spec in rule_cases[scope][kind]:\n",
    "            cases = rule_cases[scope][kind][rule_spec]\n",
    "            n = len(cases)\n",
    "            totalscope += n\n",
    "            totalkind += n\n",
    "            if scope == 'generic':\n",
    "                info('{:>4} x\\n\\t{}\\n\\t{}\\n'.format(\n",
    "                    n, rule_as_str[scope](*rule_spec), \n",
    "                    ', '.join(str(c) for c in cases[0:10]),\n",
    "                ), tm=False)\n",
    "            else:                \n",
    "                info('{:>4} x\\n\\t{}\\n\\t{}\\n'.format(\n",
    "                    n, rule_as_str[scope](*rule_spec),\n",
    "                    ', '.join(str(c) for c in cases[0:10]),\n",
    "                ), tm=False)\n",
    "        info('{:>6} {}-{} rule applications'.format(totalkind, scope, kind), tm=False)\n",
    "    info('{:>6} {} rule applications'.format(totalscope, scope), tm=False)\n",
    "\n",
    "for kind in seen:\n",
    "    stats = collections.Counter()\n",
    "    for (node, times) in seen[kind].items(): stats[times] += 1\n",
    "    for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "        info('{:>6} {} seen {:<2} time(s)'.format(n, kind, times), tm=False)\n",
    "    info('{:>6} {} seen in total'.format(len(seen[kind]), kind), tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For selected verbs, we write the enrichments to spreadsheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "onode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "verb_lexeme\n",
      "verb_stem\n",
      "verb_occurrence\n",
      "text\n",
      "constituent\n",
      "type\n",
      "rela\n",
      "type\n",
      "function\n",
      "valence\n",
      "predication\n",
      "grammatical\n",
      "original\n",
      "lexical\n",
      "semantic\n"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    onode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "    text\n",
    "    constituent\n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    type\n",
    "    function\n",
    "'''.strip().split()\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    type\n",
    "    rela\n",
    "'''.strip().split()\n",
    "\n",
    "field_names = COMMON_FIELDS + CLAUSE_FIELDS + PHRASE_FIELDS + list(enrich_fields) \n",
    "pfillrows = len(CLAUSE_FIELDS)\n",
    "cfillrows = len(PHRASE_FIELDS)\n",
    "fillrows =  pfillrows + cfillrows + len(enrich_fields)\n",
    "info('\\n'.join(field_names), tm=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated enrichment sheet for verb CWB ( 4325 rows)\n",
      "Generated enrichment sheet for verb JY> ( 4630 rows)\n",
      "Generated enrichment sheet for verb NPL ( 1932 rows)\n",
      "Generated enrichment sheet for verb CJT (  381 rows)\n",
      "Generated enrichment sheet for verb BR> (  221 rows)\n",
      "Generated enrichment sheet for verb BW> (11102 rows)\n",
      "Generated enrichment sheet for verb FJM ( 2915 rows)\n",
      "Generated enrichment sheet for verb SWR ( 1281 rows)\n",
      "Generated enrichment sheet for verb QR> ( 3726 rows)\n",
      "Generated enrichment sheet for verb NWS (  617 rows)\n",
      "Generated enrichment sheet for verb NTN ( 9846 rows)\n",
      "Generated enrichment sheet for verb <LH ( 3893 rows)\n",
      "Generated enrichment sheet for verb NF> ( 2875 rows)\n",
      "Generated enrichment sheet for verb HLK ( 5819 rows)\n",
      "Generated enrichment sheet for verb JRD ( 1593 rows)\n",
      "Generated enrichment sheet for verb PQD ( 1283 rows)\n",
      "Generated enrichment sheet for verb <FH (11323 rows)\n",
      "Generated enrichment sheet for verb <BR ( 2350 rows)\n",
      " 2h 01m 48s Done\n"
     ]
    }
   ],
   "source": [
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cn = L.u(wn, 'clause')[0]\n",
    "        if cn in clauses_seen: continue\n",
    "        clauses_seen.add(cn)\n",
    "        vn = L.u(wn, 'verse')[0]\n",
    "        bn = L.u(wn, 'book')[0]\n",
    "        (book_name, chapter, verse) = T.sectionFromNode(cn, lang='la')\n",
    "        book = T.sectionFromNode(cn)[0]\n",
    "        ln = ln_base+(ln_tpl.format(book_name, chapter, verse))+ln_tweak\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.text([wn], fmt='text-trans-plain')\n",
    "        ct = T.text(L.d(cn, 'word'), fmt='text-trans-plain')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, vl, vstem, vt, ct, '')\n",
    "        rows.append(common_fields + (('',)*fillrows))\n",
    "        for pn in L.d(cn, 'phrase'):\n",
    "            seen['phrase'][pn] += 1\n",
    "            pt = T.text(L.d(pn, 'word'), fmt='text-trans-plain')\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vstem, '', pt, 'phrase')\n",
    "            pty = F.typ.v(pn)\n",
    "            pf = pf_corr.get(pn, F.function.v(pn))\n",
    "            phrase_fields =\\\n",
    "                ('',)*pfillrows +\\\n",
    "                (pty, pf) +\\\n",
    "                enrichFields[pn]\n",
    "            rows.append(common_fields + phrase_fields)\n",
    "        for scn in clause_objects[cn]:\n",
    "            seen['clause'][scn] += 1\n",
    "            sct = T.text(L.d(scn, 'word'), fmt='text-trans-plain')\n",
    "            common_fields = (cn, wn, scn, book, chapter, verse, vl, vstem, '', sct, 'clause')\n",
    "            scty = F.typ.v(scn)\n",
    "            scr = F.rela.v(scn)\n",
    "            clause_fields =\\\n",
    "                (scty, scr) +\\\n",
    "                ('',)*cfillrows +\\\n",
    "                enrichFields[scn]\n",
    "            rows.append(common_fields + clause_fields)\n",
    "\n",
    "    filename = vfile(verb, 'enrich_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    info('Generated enrichment sheet for verb {} ({:>5} rows)'.format(verb, len(rows)), tm=False)\n",
    "    \n",
    "for verb in verbs: gen_sheet_enrich(verb)\n",
    "\n",
    "info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showcase(n):\n",
    "    otype = F.otype.v(n)\n",
    "    att1 = pf_corr.get(n, F.function.v(n)) if otype == 'phrase' else F.rela.v(n)\n",
    "    att2 = F.typ.v(n)\n",
    "    info('''{} ({}-{}) {}\\n{} {}:{}    {}\\n'''.format(\n",
    "        otype, att1, att2,\n",
    "        T.text(L.d(n, 'word'), fmt='text-trans-plain'),\n",
    "        *T.sectionFromNode(n),\n",
    "        T.text(L.d(L.u(n, 'verse')[0], 'word'), fmt='text-trans-plain'),\n",
    "    ), tm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase (Subj-DPrP) >LH \n",
      "Numbers 26:57    W >LH PQWDJ H LWJ L MCPXTM L GRCWN MCPXT H GRCNJ L QHT MCPXT H QHTJ L MRRJ MCPXT H MRRJ \n",
      "\n",
      "clause (Attr-xQt0) >CR XV>TM \n",
      "Deuteronomy 9:18    W >TNPL L PNJ JHWH K  R>CNH >RB<JM JWM W >RB<JM LJLH LXM L> >KLTJ W MJM L> CTJTJ <L KL XV>TKM >CR XV>TM L <FWT H R< B <JNJ JHWH L HK<JSW \n",
      "\n",
      "clause (NA-XYqt) KL HRG QJN CB<TJM JQM \n",
      "Genesis 4:15    W J>MR LW JHWH LKN KL HRG QJN CB<TJM JQM W JFM JHWH L QJN >WT L BLTJ HKWT >TW KL MY>W \n",
      "\n"
     ]
    }
   ],
   "source": [
    "showcase(654844)\n",
    "showcase(445014)\n",
    "showcase(426954)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb BW>: 2570 occurrences. He locales in Cmpl phrases: 157\n",
      "\t256512, 26119, 26128, 146448, 187921, 197139, 272407, 95258, 184351, 398369, 289827, 201254, 24617, 78898, 401460, 100411, 32830, 100414, 198209, 5699, 200259, 100939, 24654, 141903, 112208, 186961, 24659, 196691, 28765, 34401, 298595, 248932, 132199, 162919, 12403, 5748, 146045, 396928, 153217, 134793, 151177, 188043, 97420, 426121, 257166, 136339, 21657, 162971, 200350, 214688, 24741, 257193, 158379, 100528, 25778, 160435, 214708, 4790, 4794, 272570, 139964, 90813, 249021, 38596, 113862, 138449, 8921, 282842, 19167, 20704, 26851, 43236, 145128, 8425, 8938, 170730, 397033, 254704, 154355, 200949, 426231, 176377, 79610, 165627, 206076, 208637, 27392, 269570, 106247, 157448, 26381, 149786, 170783, 211233, 126759, 26415, 27439, 246063, 109364, 172341, 249141, 398135, 64829, 26432, 16705, 4930, 168772, 154965, 132956, 393570, 47461, 157542, 47467, 100207, 37233, 269171, 23416, 411000, 23934, 24449, 78209, 133519, 26000, 191382, 12699, 19356, 24477, 170910, 18345, 157609, 267690, 244661, 256953, 8634, 63420, 167360, 175554, 138695, 110537, 175562, 108491, 111052, 143821, 37325, 192974, 264138, 5587, 99796, 11733, 170964, 20439, 218584, 269286, 25063, 110577, 26100, 184316\n",
      "Verb BW>: 2570 occurrences. He locales in Loca phrases: 14\n",
      "\t93572, 90244, 29638, 284966, 289860, 136746, 257294, 289872, 154355, 154965, 9526, 257017, 284990, 93599\n",
      "Verb BW>: 2570 occurrences. He locales in Adju phrases: 4\n",
      "\t322819, 154355, 154965, 75703\n"
     ]
    }
   ],
   "source": [
    "def check_h(vl, show_results=False):\n",
    "    hl = {}\n",
    "    total = 0\n",
    "    for w in F.otype.s('word'):\n",
    "        if F.sp.v(w) != 'verb' or F.lex.v(w).rstrip('[=/') != vl: continue\n",
    "        total += 1\n",
    "        c = L.u(w, 'clause')[0]\n",
    "        ps = L.d(c, 'phrase')\n",
    "        phs = {p for p in ps if len({w for w in L.d(p, 'word') if F.uvf.v(w) == 'H'}) > 0}\n",
    "        for f in ('Cmpl', 'Adju', 'Loca'):\n",
    "            phc = {p for p in ps if pf_corr.get(p, None) or (pf_corr.get(p, F.function.v(p))) == f}\n",
    "            if len(phc & phs): hl.setdefault(f, set()).add(w)\n",
    "    for f in hl:\n",
    "        info('Verb {}: {} occurrences. He locales in {} phrases: {}'.format(vl, total, f, len(hl[f])), tm=False)\n",
    "        if show_results: info('\\t{}'.format(', '.join(str(x) for x in hl[f])), tm=False)\n",
    "check_h('BW>', show_results=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be handy to generate an informational spreadsheet that shows all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Process the filled in enrichments\n",
    "\n",
    "We read the enrichments, and perform some consistency checks.\n",
    "If the filled-in sheet does not exist, we take the blank sheet, with the default assignment of the new features.\n",
    "If a phrase got conflicting features, because it occurs in sheets for multiple verbs, the values in the filled-in sheet take precedence over the values in the blank sheet. If both occur in a filled in sheet, a warning will be issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_enrich():\n",
    "    of_enriched = {\n",
    "        False: {}, # for enrichments found in blank sheets\n",
    "        True: {}, # for enrichments found in filled sheets\n",
    "    }\n",
    "    repeated = {\n",
    "        False: collections.defaultdict(list), # for blank sheets\n",
    "        True: collections.defaultdict(list), # for filled sheets\n",
    "    }\n",
    "    wrong_value = {\n",
    "        False: collections.defaultdict(list),\n",
    "        True: collections.defaultdict(list),\n",
    "    }\n",
    "\n",
    "    non_match = collections.defaultdict(list)\n",
    "    wrong_node = collections.defaultdict(list)\n",
    "\n",
    "    results = []\n",
    "    dev_results = [] # results that deviate from the filled sheet\n",
    "    \n",
    "    ERR_LIMIT = 10\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        vresults = {\n",
    "            False: {}, # for blank sheets\n",
    "            True: {}, # for filled sheets\n",
    "        }\n",
    "        for check in (\n",
    "            (False, 'blank'), \n",
    "            (True, 'filled'),\n",
    "        ):\n",
    "            is_filled = check[0]\n",
    "            filename = vfile(verb, 'enrich_{}'.format(check[1]))\n",
    "            if not os.path.exists(filename):\n",
    "                error('NO {} enrichments file {}'.format(check[1], filename))\n",
    "                continue\n",
    "            #info('READING {} enrichments file {}'.format(check[1], filename))\n",
    "\n",
    "            with open(filename) as fh:\n",
    "                header = fh.__next__()\n",
    "                for line in fh:\n",
    "                    fields = line.rstrip().split(';')\n",
    "                    on = int(fields[2])\n",
    "                    if on < 0: continue\n",
    "                    kind = fields[10]\n",
    "                    objects_seen[kind][on] += 1\n",
    "                    vvals = tuple(fields[-nef:])\n",
    "                    for (f, v) in zip(enrich_fields, vvals):\n",
    "                        if v != '' and v != 'X' and v != 'NA' and v not in enrich_fields[f]:\n",
    "                            wrong_value[is_filled][on].append((verb, f, v))\n",
    "                    vresults[is_filled][on] = vvals\n",
    "                    if on in of_enriched[is_filled]:\n",
    "                        if on not in repeated[is_filled]:\n",
    "                            repeated[is_filled][on] = [of_enriched[is_filled][on]]\n",
    "                        repeated[is_filled][on].append((verb, vvals))\n",
    "                    else:\n",
    "                        of_enriched[is_filled][on] = (verb, vvals)\n",
    "                    if F.otype.v(on) != kind: \n",
    "                        non_match[on].append((verb, kind))\n",
    "            for on in sorted(vresults[True]):          # check whether the phrase ids are not mangled\n",
    "                if on not in vresults[False]:\n",
    "                    wrong_node[on].append(verb)\n",
    "            for on in sorted(vresults[False]):      # now collect all results, give precedence to filled values\n",
    "                if F.otype.v(on) == 'phrase':\n",
    "                    f_corr = on in pf_corr  # manual correction in phrase function\n",
    "                    f_good = pf_corr.get(on, F.function.v(on)) \n",
    "                else:\n",
    "                    f_corr = ''\n",
    "                    f_good = ''\n",
    "                s_manual = on in vresults[True] and vresults[False][on] != vresults[True][on] # real change\n",
    "\n",
    "                # here we determine which value is going to be put in a feature\n",
    "                # basic rule: if there is an filled-in sheet, take the value from there, else from the blank one\n",
    "                # exception: \n",
    "                # if a value is empty in the filled-in sheet, but not in the blank one, take the non-empty one\n",
    "                #\n",
    "                # Why? Well, sometimes we improve the enrich logic. There may be filled-in sheets based on older\n",
    "                # blank sheets. \n",
    "                # We want to push new values in blank sheets through unfilled in values in the filled sheets.\n",
    "                # If it is intentional to remove a value from the blank sheet, \n",
    "                # you can put an X in the corresponding filled field.\n",
    "                blank_results = vresults[False][on]\n",
    "                these_results = []\n",
    "\n",
    "                for (i, br) in enumerate(blank_results):\n",
    "                    the_value = br\n",
    "                    if s_manual and vresults[True][on][i] != '':\n",
    "                        the_value = vresults[True][on][i]\n",
    "                        if the_value == 'X':\n",
    "                            the_value = ''\n",
    "                    these_results.append(the_value)\n",
    "                these_results = tuple(these_results)\n",
    "                            \n",
    "                # these_results = vresults[True][on] if s_manual else vresults[False][on]\n",
    "                \n",
    "                if f_corr or s_manual:\n",
    "                    dev_results.append((on,)+these_results+(f_good, f_corr, s_manual))\n",
    "                results.append((on,)+these_results+(f_good, f_corr, s_manual))\n",
    "\n",
    "    for check in (\n",
    "        (False, 'blank'), \n",
    "        (True, 'filled'),\n",
    "    ):\n",
    "        if len(wrong_value[check[0]]): #illegal values in sheets\n",
    "            wrongs = wrong_value[check[0]]\n",
    "            for x in sorted(wrongs)[0:ERR_LIMIT]:\n",
    "                px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "                ref_node = L.u(x, 'clause')[0] if F.otype.v(x) != 'clause' else x\n",
    "                cx = T.text(L.d(ref_node, 'word'), fmt='ev')\n",
    "                passage = T.sectionFromNode(x)\n",
    "                error('ERROR: {} Illegal value(s) in {}: {} = {} in {}:'.format(\n",
    "                    passage, check[1], x, px, cx\n",
    "                ), tm=False)\n",
    "                for (verb, f, v) in wrongs[x]:\n",
    "                    error('\\t\"{}\" is an illegal value for \"{}\" in verb {}'.format(\n",
    "                        v, f, verb,\n",
    "                    ), tm=False)\n",
    "            ne = len(wrongs)\n",
    "            if ne > ERR_LIMIT: error('... AND {} CASES MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "        else:\n",
    "            info('OK: The used {} enrichment sheets have legal values'.format(check[1]))\n",
    "\n",
    "        nerrors = 0\n",
    "        if len(repeated[check[0]]): # duplicates in sheets, check consistency\n",
    "            repeats = repeated[check[0]]\n",
    "            for x in sorted(repeats):\n",
    "                overview = collections.defaultdict(list)\n",
    "                for y in repeats[x]: overview[y[1]].append(y[0])\n",
    "                px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "                ref_node = L.u(x, 'clause')[0] if F.otype.v(x) != 'clause' else x\n",
    "                cx = T.text(L.d(ref_node, 'word'), fmt='ev')\n",
    "                passage = T.sectionFromNode(x)\n",
    "                if len(overview) > 1:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        error('ERROR: {} Conflict in {}: {} = {} in {}:'.format(\n",
    "                            passage, check[1], x, px, cx\n",
    "                        ), tm=False)\n",
    "                        for vals in overview:\n",
    "                            error('\\t{:<40} in verb(s) {}'.format(\n",
    "                                ', '.join(vals),\n",
    "                                ', '.join(overview[vals]),\n",
    "                        ), tm=False)\n",
    "                elif False: # for debugging purposes\n",
    "                #else:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        info('{} Agreement in {} {} = {} in {}: {}'.format(\n",
    "                            passage, check[1], x, px, cx, ','.join(list(overview.values())[0]),\n",
    "                        ), tm=False)\n",
    "            ne = nerrors\n",
    "            if ne > ERR_LIMIT: error('... AND {} CASES MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "        if nerrors == 0:\n",
    "            info('OK: The used {} enrichment sheets are consistent'.format(check[1]))\n",
    "\n",
    "    if len(non_match):\n",
    "        error('ERROR: Enrichments have been applied to nodes with non-matching types:')\n",
    "        for x in sorted(non_match)[0:ERR_LIMIT]:\n",
    "            (verb, shouldbe) = non_match[x]\n",
    "            px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "            error('{}: {} Node {} is not a {} but a {}'.format(\n",
    "                verb, T.sectionFromNode(x), x, shouldbe, F.otype.v(x),\n",
    "            ), tm=False)\n",
    "        ne = len(non_phrase)\n",
    "        if ne > ERR_LIMIT: error('... AND {} CASES MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "    else:\n",
    "        info('OK: all enriched nodes where phrase nodes')\n",
    "\n",
    "    if len(wrong_node):\n",
    "        error('ERROR: Node in filled sheet did not occur in blank sheet:')\n",
    "        for x in sorted(wrong_node)[0:ERR_LIMIT]:\n",
    "            px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "            error('{}: {} node {}'.format(\n",
    "                wrong_node[x], T.sectionFromNode(x), x,\n",
    "            ), tm=False)\n",
    "        ne = len(wrong_node)\n",
    "        if ne > ERR_LIMIT: error('... AND {} CASES MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "    else:\n",
    "        info('OK: all enriched nodes occurred in the blank sheet')\n",
    "\n",
    "    if len(dev_results):\n",
    "        info('OK: there are {} manual correction/enrichment annotations'.format(len(dev_results)))\n",
    "        for r in dev_results[0:ERR_LIMIT]:\n",
    "            (x, *vals, f_good, f_corr, s_manual) = r\n",
    "            px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "            cx = T.text(L.d(L.u(x, 'clause')[0], 'word'), fmt='ev')\n",
    "            info('{:<30} {:>7} => {:<3} {:<3} {}\\n\\t{}\\n\\t\\t{}'.format(\n",
    "                'COR' if f_corr else '',\n",
    "                'MAN' if s_manual else'',\n",
    "                '{} {}:{}'.format(*T.sectionFromNode(x)), x, ','.join(vals), px, cx\n",
    "            ), tm=False)\n",
    "        ne = len(dev_results)\n",
    "        if ne > ERR_LIMIT: info('... AND {} ANNOTATIONS MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "    else:\n",
    "        error('WARNING: there are no manual correction/enrichment annotations')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2h 02m 01s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/oBR_etcbc4b.csv\n",
      " 2h 02m 01s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/oFH_etcbc4b.csv\n",
      " 2h 02m 01s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/oLH_etcbc4b.csv\n",
      " 2h 02m 01s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/BRa_etcbc4b.csv\n",
      " 2h 02m 01s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/BWa_etcbc4b.csv\n",
      " 2h 02m 01s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/CJT_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/CWB_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/FJM_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/HLK_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/JRD_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/JYa_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/NFa_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/NPL_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/NTN_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/NWS_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/PQD_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/QRa_etcbc4b.csv\n",
      " 2h 02m 02s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/SWR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2h 02m 02s OK: The used blank enrichment sheets have legal values\n",
      " 2h 02m 02s OK: The used blank enrichment sheets are consistent\n",
      " 2h 02m 02s OK: The used filled enrichment sheets have legal values\n",
      " 2h 02m 02s OK: The used filled enrichment sheets are consistent\n",
      " 2h 02m 02s OK: all enriched nodes where phrase nodes\n",
      " 2h 02m 02s OK: all enriched nodes occurred in the blank sheet\n",
      " 2h 02m 02s OK: there are 1459 manual correction/enrichment annotations\n",
      "COR                                    => Genesis 32:24 616140 complement,NA,*,,,\n",
      "\t17914 17915 17916\n",
      "\t\t17912 17913 17914 17915 17916\n",
      "COR                                    => Exodus 13:12 627423 adjunct,NA,NA,,,\n",
      "\t35983 35984\n",
      "\t\t35978 35979 35980 35981 35982 35983 35984\n",
      "COR                                    => Leviticus 18:21 642320 adjunct,NA,NA,,,\n",
      "\t63643 63644 63645\n",
      "\t\t63641 63642 63643 63644 63645\n",
      "COR                                    => Numbers 20:19 652606 adjunct,NA,NA,,,\n",
      "\t82938 82939\n",
      "\t\t82938 82939 82940\n",
      "COR                                    => Numbers 27:7 654987 adjunct,NA,NA,,,\n",
      "\t87031\n",
      "\t\t87026 87027 87028 87029 87030 87031\n",
      "COR                                    => Numbers 27:8 655002 adjunct,NA,NA,,,\n",
      "\t87050 87051\n",
      "\t\t87046 87047 87048 87049 87050 87051\n",
      "COR                                    => Numbers 32:21 656412 complement,NA,*,,location,location\n",
      "\t90279 90280 90281\n",
      "\t\t90271 90272 90273 90274 90275 90276 90277 90278 90279 90280 90281\n",
      "COR                                    => Numbers 32:30 656510 adjunct,NA,NA,,,\n",
      "\t90457\n",
      "\t\t90453 90454 90455 90456 90457 90458\n",
      "COR                                    => Numbers 32:30 656511 adjunct,NA,NA,,,\n",
      "\t90458\n",
      "\t\t90453 90454 90455 90456 90457 90458\n",
      "COR                                    => Numbers 32:32 656528 adjunct,NA,NA,,,\n",
      "\t90485\n",
      "\t\t90483 90484 90485 90486 90487 90488 90489 90490\n",
      "... AND 1449 ANNOTATIONS MORE\n"
     ]
    }
   ],
   "source": [
    "objects_seen = collections.defaultdict(collections.Counter)\n",
    "sheetResults = read_enrich()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(605134, ('adjunct', 'NA', 'NA', '', 'time', 'time')),\n",
       " (605135, ('core', 'regular', 'NA', '', '', '')),\n",
       " (605136, ('core', 'NA', 'subject', '', '', '')),\n",
       " (605137, ('complement', 'NA', 'direct_object', '', '', '')),\n",
       " (605352, ('NA', 'NA', 'NA', '', 'NA', 'NA')),\n",
       " (605353, ('core', 'regular', 'NA', '', '', '')),\n",
       " (605354, ('core', 'NA', 'subject', '', '', '')),\n",
       " (605355, ('complement', 'NA', 'direct_object', '', '', '')),\n",
       " (605363, ('adjunct', 'NA', 'NA', '', '', '')),\n",
       " (605435, ('NA', 'NA', 'NA', '', 'NA', 'NA'))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enrichFields.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(430194, 'complement', 'NA', 'direct_object', '', '', '', '', '', False),\n",
       " (433827, 'NA', 'NA', '', '', '', '', '', '', False),\n",
       " (436433, 'NA', 'NA', 'infinitive_object', '', '', '', '', '', False),\n",
       " (441354, 'NA', 'NA', '', '', '', '', '', '', False),\n",
       " (441385, 'NA', 'NA', '', '', '', '', '', '', False),\n",
       " (442422, 'NA', 'NA', 'infinitive_object', '', '', '', '', '', False),\n",
       " (442730, 'NA', 'NA', 'infinitive_object', '', '', '', '', '', False),\n",
       " (443452, 'NA', 'NA', '', '', '', '', '', '', False),\n",
       " (444386, 'NA', 'NA', '', '', '', '', '', '', False),\n",
       " (444410, 'NA', 'NA', '', '', '', '', '', '', False)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheetResults[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the sheet results with the generic results in one single dictionary, keyed by node number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2h 02m 24s Annotations from sheets for 53802 nodes\n",
      " 2h 02m 24s Merging 221472 annotations from generic enrichment\n",
      " 2h 02m 24s Resulting in annotations for 221472 nodes\n"
     ]
    }
   ],
   "source": [
    "allResults = dict()\n",
    "for (n, *features) in sheetResults:\n",
    "    allResults[n] = features\n",
    "info('Annotations from sheets for {} nodes'.format(len(allResults)))\n",
    "info('Merging {} annotations from generic enrichment'.format(len(enrichFields)))\n",
    "for (n, features) in enrichFields.items():\n",
    "    if n in allResults: continue\n",
    "    allResults[n] = features + ('', '', False)\n",
    "info('Resulting in annotations for {} nodes'.format(len(allResults)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Generate data\n",
    "\n",
    "We write the correction and enrichment data as a data module in text-fabric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newFeatures = list(enrich_fields.keys())+['function', 'f_correction', 's_manual']\n",
    "\n",
    "provenance = dict(\n",
    "    title='Correction and enrichment features',\n",
    "    description='Corrections, alternatives and additions to the ETCBC4b encoding of the Hebrew Bible',\n",
    "    purpose='Support the decision process of assigning valence to verbs',\n",
    "    method='Generated blank correction and enrichment spreadsheets with selected clauses',\n",
    "    steps='sheets filled out by researcher; read back in by program; generated new features based on contents',\n",
    "    author='The content and nature of the features are by Janet Dyk, the workflow is by Dirk Roorda',\n",
    ")\n",
    "\n",
    "metaData = {\n",
    "    '': provenance,\n",
    "    'valence': {\n",
    "        'description': 'verbal valence main classification',\n",
    "    },\n",
    "    'predication': {\n",
    "        'description': 'verbal function main classification',\n",
    "    },\n",
    "    'grammatical': {\n",
    "        'description': 'constituent role main classification',\n",
    "    },\n",
    "    'original': {\n",
    "        'description': 'default value before enrichment logic has been applied',\n",
    "    },\n",
    "    'lexical': {\n",
    "        'description': 'additional lexical characteristics',\n",
    "    },\n",
    "    'semantic': {\n",
    "        'description': 'additional semantic characteristics',\n",
    "    },\n",
    "    'f_correction': {\n",
    "        'description': 'whether the phrase function has been manually corrected',\n",
    "    },\n",
    "    's_manual': {\n",
    "        'description': 'whether the generated enrichment features have been manually changed',\n",
    "    },\n",
    "    'function': {\n",
    "        'description': 'corrected phrase function, only present for phrases that were in a correction sheet',\n",
    "    },\n",
    "}\n",
    "\n",
    "for f in newFeatures: metaData[f]['valueType'] = 'str'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodeFeatures = dict()\n",
    "\n",
    "for (node, featureVals) in allResults.items():\n",
    "    for (fName, fVal) in zip(newFeatures, featureVals):\n",
    "        fValRep = fVal\n",
    "        if type(fVal) is bool:\n",
    "            fValRep = 'y' if fVal else ''\n",
    "        nodeFeatures.setdefault(fName, {})[node] = fValRep\n",
    "\n",
    "RENAMES = [('function', 'cfunction')]\n",
    "for (oldF, newF) in RENAMES:\n",
    "    for data in (nodeFeatures, metaData):\n",
    "        data[newF] = data[oldF]\n",
    "        del data[oldF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.10\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "121 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "VALENCE = f'tf/{version}'\n",
    "TF = Fabric(locations=['~/github/text-fabric-data-legacy', '~/github/valence'], modules=[ETCBC, VALENCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Exporting 9 node and 0 edge and 0 config features to /Users/dirk/github/valence/tf/4b:\n",
      "   |     0.46s T cfunction            to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.40s T f_correction         to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.42s T grammatical          to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.40s T lexical              to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.41s T original             to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.44s T predication          to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.41s T s_manual             to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.41s T semantic             to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.43s T valence              to /Users/dirk/github/valence/tf/4b\n",
      "  3.79s Exported 9 node features and 0 edge features and 0 config features to /Users/dirk/github/valence/tf/4b\n"
     ]
    }
   ],
   "source": [
    "TF.save(\n",
    "    module=VALENCE,\n",
    "    nodeFeatures=nodeFeatures,\n",
    "    metaData=metaData,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Test the new data\n",
    "We load the new and modified features the Text-Fabric.\n",
    "\n",
    "Note that we draw in the new annotations by specifying a module addressed by VALENCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.10\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "121 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations=['~/github/text-fabric-data-legacy', '~/github/valence'], modules=[ETCBC, VALENCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.23s B lex_utf8             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.23s B lex                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.22s B gloss                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B sp                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B vs                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.24s B rela                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.25s B typ                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.11s B function             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.63s T cfunction            from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.49s T s_manual             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.42s T f_correction         from /Users/dirk/github/valence/tf/4b\n",
      "   |     1.01s T valence              from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.87s T predication          from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.81s T grammatical          from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.40s T original             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.55s T lexical              from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.53s T semantic             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.00s Feature overview: 115 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "    13s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    lex gloss lex_utf8\n",
    "    sp vs lex rela typ\n",
    "    function cfunction s_manual f_correction\n",
    "''' + ' '.join(enrich_fields))\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s M otext                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.04s B otype                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.58s B oslots               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M otext                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.02s B book                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.03s B chapter              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s B verse                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.17s B g_cons               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.21s B g_cons_utf8          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.17s B g_lex                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.22s B g_lex_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s B g_qere_utf8          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.18s B g_word               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.24s B g_word_utf8          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B lex0                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.23s B lex_utf8             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s B qtrailer_utf8        from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.11s B trailer_utf8         from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B __levels__           from otype, oslots\n",
      "   |     0.04s B __order__            from otype, oslots, __levels__\n",
      "   |     0.05s B __rank__             from otype, __order__\n",
      "   |     1.18s B __levUp__            from otype, oslots, __rank__\n",
      "   |     1.10s B __levDown__          from otype, __levUp__, __rank__\n",
      "   |     0.41s B __boundary__         from otype, oslots, __rank__\n",
      "   |     0.02s B __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      "   |     0.23s B lex                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.22s B gloss                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B sp                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B vs                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.24s B rela                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.25s B typ                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.11s B function             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.63s T cfunction            from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.49s T s_manual             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.42s T f_correction         from /Users/dirk/github/valence/tf/4b\n",
      "   |     1.01s T valence              from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.87s T predication          from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.81s T grammatical          from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.40s T original             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.55s T lexical              from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.53s T semantic             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.01s B book@am              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s B book@ar              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s B book@bn              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@da              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@de              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@el              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@en              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@es              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@fa              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@fr              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@he              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@hi              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@id              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@ja              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@ko              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@la              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@nl              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@pa              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@pt              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@ru              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@sw              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@syc             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@tr              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@ur              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@yo              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@zh              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.02s M dist                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s M dist_unit            from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M distributional_parent from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M domain               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M entry                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s M entry_heb            from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M entryid              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M freq_lex             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M freq_occ             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M functional_parent    from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_entry              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_entry_heb          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_nme                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_nme_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_pfm                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_pfm_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_prs                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_prs_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_uvf                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_uvf_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_vbe                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_vbe_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_vbs                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_vbs_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M gn                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M id                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M instruction          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M is_root              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M kind                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M label                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M lan                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M language             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M ls                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M monads               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M mother               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M mother_object_type   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M nametype             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M nme                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M nu                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M number               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M number_in_ch         from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M omap@4-4b            from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M otext                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.02s M pargr                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s M pdp                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s M pfm                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s M phono                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M phono_sep            from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M pos                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M prs                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M ps                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M rank_lex             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M rank_occ             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M root                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M sense                from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.00s M st                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M subpos               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M tab                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M txt                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M uvf                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M vbe                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M vbs                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M vt                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n"
     ]
    }
   ],
   "source": [
    "loadLog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple test\n",
    "Take the first 10 phrases and retrieve the corrected and uncorrected function feature.\n",
    "Note that the corrected function feature is only filled in, if it occurs in a clause in which a selected verb occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time - Time - True\n",
      "Pred - Pred - True\n",
      "Subj - Subj - True\n",
      "Objc - Objc - True\n",
      "Conj -  - True\n",
      "Subj -  - True\n",
      "Pred -  - True\n",
      "PreC -  - True\n",
      "Conj - None - False\n",
      "Subj - None - False\n"
     ]
    }
   ],
   "source": [
    "for i in list(F.otype.s('phrase'))[0:10]: \n",
    "    print('{} - {} - {}'.format(\n",
    "        F.function.v(i), \n",
    "        F.cfunction.v(i),\n",
    "        L.u(i, 'clause')[0] in clause_verb,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We put all corrections and enrichments in a single csv file for checking.\n",
    "\n",
    "We also generate a smaller csv, with only the data for selected verbs in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    29s collecting constituents ...\n",
      "    30s   2439 selected of  10000 constituents in   698 selected of  3065 clauses ...\n",
      "    30s   4980 selected of  20000 constituents in  1444 selected of  6135 clauses ...\n",
      "    31s   8078 selected of  30000 constituents in  2299 selected of  9096 clauses ...\n",
      "    32s  10451 selected of  40000 constituents in  2965 selected of 12043 clauses ...\n",
      "    33s  13045 selected of  50000 constituents in  3714 selected of 15059 clauses ...\n",
      "    34s  15631 selected of  60000 constituents in  4470 selected of 18127 clauses ...\n",
      "    35s  18574 selected of  70000 constituents in  5306 selected of 21091 clauses ...\n",
      "    35s  21265 selected of  80000 constituents in  6133 selected of 24167 clauses ...\n",
      "    36s  24093 selected of  90000 constituents in  6962 selected of 27182 clauses ...\n",
      "    37s  26843 selected of 100000 constituents in  7780 selected of 30269 clauses ...\n",
      "    38s  29681 selected of 110000 constituents in  8674 selected of 33442 clauses ...\n",
      "    39s  31693 selected of 120000 constituents in  9317 selected of 36908 clauses ...\n",
      "    40s  33939 selected of 130000 constituents in 10005 selected of 40260 clauses ...\n",
      "    41s  36358 selected of 140000 constituents in 10736 selected of 43403 clauses ...\n",
      "    41s  39112 selected of 150000 constituents in 11553 selected of 46527 clauses ...\n",
      "    42s  41480 selected of 160000 constituents in 12281 selected of 49785 clauses ...\n",
      "    43s  43089 selected of 170000 constituents in 12801 selected of 53363 clauses ...\n",
      "    44s  44771 selected of 180000 constituents in 13374 selected of 56977 clauses ...\n",
      "    45s  46274 selected of 190000 constituents in 13878 selected of 60445 clauses ...\n",
      "    46s  48402 selected of 200000 constituents in 14534 selected of 63713 clauses ...\n",
      "    46s  50643 selected of 210000 constituents in 15165 selected of 66711 clauses ...\n",
      "    47s  53369 selected of 220000 constituents in 15936 selected of 69702 clauses ...\n",
      "    47s  53802 selected of 221472 constituents in 16053 selected of 70131 clauses done\n"
     ]
    }
   ],
   "source": [
    "f = open(all_results, 'w')\n",
    "g = open(selected_results, 'w')\n",
    "\n",
    "NALLFIELDS = 17\n",
    "tpl = ('{};' * (NALLFIELDS - 1))+'{}\\n'\n",
    "\n",
    "info('collecting constituents ...')\n",
    "f.write(tpl.format(\n",
    "    '-',\n",
    "    '-',\n",
    "    'passage',\n",
    "    'verb(s) text',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    'clause text',\n",
    "    'clause node',\n",
    "))\n",
    "f.write(tpl.format(\n",
    "    'corrected',\n",
    "    'enriched',\n",
    "    'passage',\n",
    "    '-',\n",
    "    'object type',\n",
    "    'clause rela',\n",
    "    'clause type',\n",
    "    'phrase function (old)',\n",
    "    'phrase function (new)',\n",
    "    'phrase type',\n",
    "    'valence',\n",
    "    'predication',\n",
    "    'grammatical',\n",
    "    'original',\n",
    "    'lexical',\n",
    "    'semantic',\n",
    "    'object text',\n",
    "    'object node',\n",
    "))\n",
    "i = 0\n",
    "h = 0\n",
    "j = 0\n",
    "c = 0\n",
    "d = 0\n",
    "CHUNK_SIZE = 10000\n",
    "sel_verbs = set(verbs)\n",
    "for cn in sorted(clause_verb):\n",
    "    c += 1\n",
    "    vrbs = sorted(clause_verb[cn])\n",
    "    lex_vrbs = {F.lex.v(verb).rstrip('[=') for verb in vrbs}\n",
    "    selected = len(lex_vrbs & sel_verbs) != 0\n",
    "    if selected:\n",
    "        d += 1\n",
    "        sel_vrbs = [v for v in vrbs if F.lex.v(v).rstrip('[=') in verbs]\n",
    "        \n",
    "        g.write(tpl.format(\n",
    "            '',\n",
    "            '',\n",
    "            '{} {}:{}'.format(*T.sectionFromNode(cn)),\n",
    "            ' '.join(F.lex.v(verb) for verb in sel_vrbs),\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            T.text(L.d(cn, 'word'), fmt='text-trans-plain'),\n",
    "            cn,\n",
    "        ))\n",
    "\n",
    "    f.write(tpl.format(\n",
    "        '',\n",
    "        '',\n",
    "        '{} {}:{}'.format(*T.sectionFromNode(cn)),\n",
    "        ' '.join(F.lex.v(verb) for verb in vrbs),\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        T.text(L.d(cn, 'word'), fmt='text-trans-plain'),\n",
    "        cn,\n",
    "    ))\n",
    "    for pn in L.d(cn, 'phrase'):\n",
    "        i += 1\n",
    "        if selected: h += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            info('{:>6} selected of {:>6} constituents in {:>5} selected of {:>5} clauses ...'.format(h, i, d, c))\n",
    "            \n",
    "        material = tpl.format(\n",
    "            'COR' if F.f_correction.v(pn) == 'y' else '',\n",
    "            'MAN' if F.s_manual.v(pn) == 'y' else '',\n",
    "            '{} {}:{}'.format(*T.sectionFromNode(pn)),\n",
    "            '',\n",
    "            'phrase',\n",
    "            '',\n",
    "            '',\n",
    "            F.function.v(pn),\n",
    "            F.cfunction.v(pn),\n",
    "            F.typ.v(pn),\n",
    "            F.valence.v(pn),\n",
    "            F.predication.v(pn),\n",
    "            F.grammatical.v(pn),\n",
    "            F.original.v(pn),\n",
    "            F.lexical.v(pn),\n",
    "            F.semantic.v(pn),\n",
    "            T.text(L.d(pn, 'word'), fmt='text-trans-plain'),\n",
    "            pn,\n",
    "        )\n",
    "        f.write(material)\n",
    "        if selected:\n",
    "            g.write(material)\n",
    "    for scn in clause_objects[cn]:\n",
    "        i += 1\n",
    "        if selected: h += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            info('{:>6} constituents in {:>5} clauses ...'.format(i, c))\n",
    "        material = tpl.format(\n",
    "            '',\n",
    "            '',\n",
    "            '{} {}:{}'.format(*T.sectionFromNode(scn)),\n",
    "            '',\n",
    "            'clause',\n",
    "            F.rela.v(scn),\n",
    "            F.typ.v(scn),\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            F.valence.v(scn),\n",
    "            F.predication.v(scn),\n",
    "            F.grammatical.v(scn),\n",
    "            F.original.v(scn),\n",
    "            F.lexical.v(scn),\n",
    "            F.semantic.v(scn),\n",
    "            T.text(L.d(scn, 'word'), fmt='text-trans-plain'),\n",
    "            scn,\n",
    "        )\n",
    "        f.write(material)\n",
    "        if selected:\n",
    "            g.write(material)\n",
    "\n",
    "f.close()\n",
    "g.close()\n",
    "info('{:>6} selected of {:>6} constituents in {:>5} selected of {:>5} clauses done'.format(h, i, d, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cmpl\n",
      "True\n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x  = 671522\n",
    "print(pf_corr.get(x, F.function.v(x)))\n",
    "print(is_lex_local('FJM',x))\n",
    "print(x in rule_cases['specific']['phrase'][('FJM', 2)])\n",
    "print(F.lexical.v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
