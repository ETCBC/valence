{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/tf-small.png\"/>\n",
    "\n",
    "# Complement corrections\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Joint work of Dirk Roorda and Janet Dyk.\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/flowchart.html)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "Because the flowchart assigns meanings to verbs depending on the number and nature of complements found in their context, it is important that the phrases in those clauses are labeled correctly, i.e. that the\n",
    "[function](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/function.html)\n",
    "feature for those phrases have the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) \n",
    "Analysing Valence Patterns in Biblical Hebrew: Theoretical Questions and Analytic Frameworks.\n",
    "*J. of Northwest Semitic Languages, vol. 40 (2014), no. 1, pp. 43-62*.\n",
    "[pdf abstract](http://academic.sun.ac.za/jnsl/Volumes/JNSL%2040%201%20abstracts%20and%20bookreview.pdf)\n",
    "[pdf fulltext (author's copy with deviant page numbering)](https://shebanq.ancient-data.org/static/docs/methods/2014_Dyk_jnsl.pdf)\n",
    "\n",
    "(Janet Dyk 2014)\n",
    "Deportation or Forgiveness in Hosea 1.6? Verb Valence Patterns and Translation Proposals.\n",
    "*The Bible Translator 2014, Vol. 65(3) 235–279*.\n",
    "[pdf](http://tbt.sagepub.com/content/65/3/235.full.pdf?ijkey=VK2CEHvVrvSGA5B&keytype=finite)\n",
    "\n",
    "(Janet Dyk 2014)\n",
    "Traces of Valence Shift in Classical Hebrew.\n",
    "In: *Discourse, Dialogue, and Debate in the Bible: Essays in Honour of Frank Polak*.\n",
    "Ed. Athalya Brenner-Idan.\n",
    "*Sheffield Pheonix Press, 48–65*.\n",
    "[book behind pay-wall](http://www.sheffieldphoenix.com/showbook.asp?bkid=273)\n",
    "\n",
    "(Dirk Roorda et al. 2014-2016)\n",
    "Text-fabric-data-legacy.\n",
    "[Github repository](https://github.com/ETCBC/text-fabric-data-legacy)\n",
    "with text and linguistics of the Hebrew bible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Task\n",
    "In this notebook we do the following tasks:\n",
    "\n",
    "* generate correction sheets for selected verbs,\n",
    "* process the set of filled in correction sheets\n",
    "* generate sheets with computed, new features (based on corrected values, valence related) to be edited manually\n",
    "* transform the set of filled in enrichment sheets into an annotation package\n",
    "\n",
    "Between the first and second task, the sheets will have been filled in by Janet with corrections.\n",
    "Between the third and the fourth task, the sheets will be inspected and improved by Janet.\n",
    "\n",
    "The resulting annotation package offers the corrections as the value of a new feature, also called `function`, but now in the annotation space `JanetDyk` instead of `etcbc4`.\n",
    "The results of the enrichment will be added as new features in that same annotation space.\n",
    "\n",
    "## 1.1 Limitations\n",
    "We restrict ourselves to verb occurrences where the verb is the nucleus of a phrase with function *predicate*. \n",
    "There are also verb occurrences in other kinds of phrases, and these also can have complements. These cases are coded very differently in the database. See for example [Joshua 3:8](https://shebanq.ancient-data.org/hebrew/text?book=Josua&chapter=3&verse=8&version=4b&mr=m&qw=q&tp=txt_tb1&tr=hb&wget=v&qget=v&nget=v). (*and you command the priest carrying the ark* ...).\n",
    "\n",
    "## 1.2 Data\n",
    "We carry out the valence project against the \n",
    "[4b](https://github.com/ETCBC/text-fabric-data-legacy/tree/master/hebrew/etcbc4b)\n",
    "version of the Hebrew Text Database of the \n",
    "[ETCBC](https://www.godgeleerdheid.vu.nl/en/research/institutes-and-centres/eep-talstra-centre-for-bible-and-computer/index.aspx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "\n",
    "Start the engines. We use the Python package \n",
    "[text-fabric](https://github.com/ETCBC/text-fabric)\n",
    "to process the data of the Hebrew Text Database smoothly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, collections\n",
    "from copy import deepcopy\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.10\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "111 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "ETCBC = f'hebrew/{source}{version}'\n",
    "TF = Fabric(locations='~/github/text-fabric-data-legacy', modules=ETCBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.19s B lex_utf8             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B lex                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.15s B gloss                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B sp                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B vs                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.12s B uvf                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.13s B prs                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.08s B nametype             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.12s B ls                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.10s B function             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.21s B rela                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.21s B typ                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.23s B mother               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s Feature overview: 105 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  6.42s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    lex gloss lex_utf8\n",
    "    sp vs lex uvf prs nametype ls\n",
    "    function rela typ\n",
    "    mother\n",
    "''')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ln_base = 'https://shebanq.ancient-data.org/hebrew/text'\n",
    "ln_tpl = '?book={}&chapter={}&verse={}'\n",
    "ln_tweak = '&version=4b&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x'\n",
    "\n",
    "home_dir = os.path.expanduser('~').replace('\\\\', '/')\n",
    "base_dir = '{}/github/valence/workflow'.format(home_dir)\n",
    "result_dir = '{}/results'.format(base_dir)\n",
    "all_results = '{}/all.csv'.format(result_dir)\n",
    "kinds = ('corr_blank', 'corr_filled', 'enrich_blank', 'enrich_filled')\n",
    "kdir = {}\n",
    "for k in kinds:\n",
    "    kd = '{}/{}'.format(base_dir, k)\n",
    "    kdir[k] = kd\n",
    "    if not os.path.exists(kd):\n",
    "        os.makedirs(kd)\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "def vfile(verb, kind):\n",
    "    if kind not in kinds:\n",
    "        error('Unknown kind `{}`'.format(kind))\n",
    "        return None\n",
    "    return '{}/{}_{}{}.csv'.format(kdir[kind], verb.replace('>','a').replace('<', 'o'), source, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Domain\n",
    "Here is a subset of verbs that interest us.\n",
    "In fact, we are interested in all verbs, but we have subjected the occurrences of these verbs to closer inspection, \n",
    "together with the contexts they occur in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs_initial = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())\n",
    "\n",
    "motion_verbs = set('''\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "'''.strip().split())\n",
    "\n",
    "double_object_verbs = set('''\n",
    "    NTN\n",
    "    <FH\n",
    "    FJM\n",
    "'''.strip().split())\n",
    "\n",
    "complex_qal_verbs = set('''\n",
    "    NF>\n",
    "    PQD\n",
    "'''.strip().split())\n",
    "\n",
    "verbs = verbs_initial | motion_verbs | double_object_verbs | complex_qal_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicate_functions = {\n",
    "    'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_values = dict(\n",
    "    function={F.function.v(p) for p in F.otype.s('phrase')},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_values = dict(\n",
    "    function=dict(\n",
    "        BoundErr='this constituent is part of another constituent and does not merit its own function/type/rela value',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the error_values to the legal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.66s {'function': {'PreO', 'ModS', 'Modi', 'Conj', 'PrAd', 'PtcO', 'Intj', 'Loca', 'Adju', 'Time', 'PreS', 'Exst', 'Cmpl', 'Voct', 'Pred', 'Subj', 'IntS', 'NCoS', 'Objc', 'Supp', 'PrcS', 'Rela', 'ExsS', 'NCop', 'PreC', 'BoundErr', 'Frnt', 'Ques', 'Nega', 'EPPr'}}\n"
     ]
    }
   ],
   "source": [
    "for feature in set(legal_values.keys()) | set(error_values.keys()):\n",
    "    ev = error_values.get(feature, {})\n",
    "    if ev:\n",
    "        lv = legal_values.setdefault(feature, set())\n",
    "        lv |= set(ev.keys())\n",
    "info('{}'.format(legal_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.73s Finding occurrences ...\n",
      "  7.40s Done\n",
      "Selected:     16036 verb occurrences in 15880 clauses\n",
      "  7.40s <BR   548 occurrences of which   32 outside a predicate phrase\n",
      "  7.40s <FH  2629 occurrences of which   59 outside a predicate phrase\n",
      "  7.40s <LH   890 occurrences of which   10 outside a predicate phrase\n",
      "  7.40s BR>    48 occurrences of which    3 outside a predicate phrase\n",
      "  7.40s BW>  2570 occurrences of which   27 outside a predicate phrase\n",
      "  7.40s CJT    85 occurrences of which    1 outside a predicate phrase\n",
      "  7.40s CWB  1037 occurrences of which   22 outside a predicate phrase\n",
      "  7.40s FJM   609 occurrences of which    3 outside a predicate phrase\n",
      "  7.41s HLK  1554 occurrences of which   30 outside a predicate phrase\n",
      "  7.41s JRD   377 occurrences of which   16 outside a predicate phrase\n",
      "  7.41s JY>  1069 occurrences of which   32 outside a predicate phrase\n",
      "  7.41s NF>   656 occurrences of which   52 outside a predicate phrase\n",
      "  7.41s NPL   445 occurrences of which   11 outside a predicate phrase\n",
      "  7.41s NTN  2017 occurrences of which   10 outside a predicate phrase\n",
      "  7.41s NWS   159 occurrences of which    4 outside a predicate phrase\n",
      "  7.41s PQD   303 occurrences of which   72 outside a predicate phrase\n",
      "  7.41s QR>   743 occurrences of which   11 outside a predicate phrase\n",
      "  7.41s SWR   297 occurrences of which    1 outside a predicate phrase\n"
     ]
    }
   ],
   "source": [
    "info('Finding occurrences ...')\n",
    "# we restrict our selves to selected verbs and their contexts\n",
    "occs = collections.defaultdict(list)   # dictionary of verb occurrence nodes per verb lexeme\n",
    "npoccs = collections.defaultdict(list) # same, but those not occurring in a \"predicate\"\n",
    "clause_verb = collections.defaultdict(list)    # dictionary of verb occurrence nodes per clause node\n",
    "clause_verb_index = collections.defaultdict(set) # mapping from clauses to its main verb(s)\n",
    "verb_clause_index = collections.defaultdict(list) # mapping from verbs to the clauses of which it is main verb\n",
    "\n",
    "nw = 0\n",
    "nws = 0\n",
    "for w in F.otype.s('word'):\n",
    "    if F.sp.v(w) != 'verb': continue\n",
    "    lex = F.lex.v(w).rstrip('[')\n",
    "    if lex not in verbs: continue\n",
    "    nw += 1\n",
    "    pf = F.function.v(L.u(w, 'phrase')[0])\n",
    "    if pf not in predicate_functions:\n",
    "        npoccs[lex].append(w)\n",
    "    occs[lex].append(w)\n",
    "    cn = L.u(w, 'clause')[0]\n",
    "    clause_verb[cn].append(w)\n",
    "    clause_verb_index[cn].add(lex)\n",
    "    verb_clause_index[lex].append(cn)\n",
    "\n",
    "info('Done')\n",
    "info('Selected:    {:>6} verb occurrences in {} clauses'.format(nw, len(clause_verb)), tm=False)\n",
    "\n",
    "for verb in sorted(verbs):\n",
    "    info('{} {:>5} occurrences of which {:>4} outside a predicate phrase'.format(\n",
    "        verb, \n",
    "        len(occs[verb]),\n",
    "        len(npoccs[verb]),\n",
    "        tm=False,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.63s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/BRa_etcbc4b.csv\n",
      "  7.81s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/QRa_etcbc4b.csv\n",
      "  7.94s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/FJM_etcbc4b.csv\n",
      "  8.11s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/oLH_etcbc4b.csv\n",
      "  8.15s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/NWS_etcbc4b.csv\n",
      "  8.24s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/NPL_etcbc4b.csv\n",
      "  8.66s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/NTN_etcbc4b.csv\n",
      "  9.16s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/BWa_etcbc4b.csv\n",
      "  9.29s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/oBR_etcbc4b.csv\n",
      "  9.77s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/oFH_etcbc4b.csv\n",
      "  9.92s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/NFa_etcbc4b.csv\n",
      "    10s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/JYa_etcbc4b.csv\n",
      "    10s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/HLK_etcbc4b.csv\n",
      "    11s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/JRD_etcbc4b.csv\n",
      "    11s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/SWR_etcbc4b.csv\n",
      "    11s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/CJT_etcbc4b.csv\n",
      "    11s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/CWB_etcbc4b.csv\n",
      "    11s Generated correction sheet for verb /Users/dirk/github/valence/workflow/corr_blank/PQD_etcbc4b.csv\n",
      "    11s 51763  phrases seen 1  time(s)\n",
      "    11s 181    phrases seen 2  time(s)\n",
      "    11s 9      phrases seen 3  time(s)\n",
      "    11s Total phrases seen: 51953\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u(wn, 'clause')[0]\n",
    "        if cln in clauses_seen: continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u(wn, 'verse')[0]\n",
    "        bn = L.u(wn, 'book')[0]\n",
    "        (bookName, ch, vs) = T.sectionFromNode(vn, lang='la')\n",
    "        passage_label = '{} {}:{}'.format(*T.sectionFromNode(vn))\n",
    "        ln = ln_base+(ln_tpl.format(bookName, ch, vs))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.text([wn], fmt='text-trans-plain')\n",
    "        vstem = F.vs.v(wn)\n",
    "        np = '* ' if wn in npoccs[verb] else ''\n",
    "        row = [cln, wn, passage_label, lnx, np+vt, vstem]\n",
    "        phrases = L.d(cln, 'phrase')\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            phrases_seen[pn] += 1\n",
    "            pt = T.text(L.d(pn, 'word'), fmt='text-trans-plain')\n",
    "            pf = F.function.v(pn)\n",
    "            pnp = np if pf in predicate_functions else ''\n",
    "            row.extend((pn, pnp+pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    filename = vfile(verb, 'corr_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    info('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)\n",
    "    \n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    info('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "info('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 4 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11s Processing /Users/dirk/github/valence/workflow/corr_filled/oBR_etcbc4b.csv\n",
      "    11s <BR: Found    15 corrections in /Users/dirk/github/valence/workflow/corr_filled/oBR_etcbc4b.csv\n",
      "    11s OK: Corrected phrases did not receive multiple corrections\n",
      "    11s OK: all corrected nodes where phrase nodes\n",
      "    11s OK: all corrected values are legal\n",
      "    11s Processing /Users/dirk/github/valence/workflow/corr_filled/oFH_etcbc4b.csv\n",
      "    11s <FH: Found   750 corrections in /Users/dirk/github/valence/workflow/corr_filled/oFH_etcbc4b.csv\n",
      "    11s OK: Corrected phrases did not receive multiple corrections\n",
      "    11s OK: all corrected nodes where phrase nodes\n",
      "    11s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    11s NO file /Users/dirk/github/valence/workflow/corr_filled/oLH_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11s Processing /Users/dirk/github/valence/workflow/corr_filled/BRa_etcbc4b.csv\n",
      "    11s BR>: Found   754 corrections in /Users/dirk/github/valence/workflow/corr_filled/BRa_etcbc4b.csv\n",
      "    11s OK: Corrected phrases did not receive multiple corrections\n",
      "    11s OK: all corrected nodes where phrase nodes\n",
      "    11s OK: all corrected values are legal\n",
      "    11s Processing /Users/dirk/github/valence/workflow/corr_filled/BWa_etcbc4b.csv\n",
      "    11s BW>: Found   809 corrections in /Users/dirk/github/valence/workflow/corr_filled/BWa_etcbc4b.csv\n",
      "    11s OK: Corrected phrases did not receive multiple corrections\n",
      "    11s OK: all corrected nodes where phrase nodes\n",
      "    11s OK: all corrected values are legal\n",
      "    11s Processing /Users/dirk/github/valence/workflow/corr_filled/CJT_etcbc4b.csv\n",
      "    11s CJT: Found   812 corrections in /Users/dirk/github/valence/workflow/corr_filled/CJT_etcbc4b.csv\n",
      "    11s OK: Corrected phrases did not receive multiple corrections\n",
      "    11s OK: all corrected nodes where phrase nodes\n",
      "    11s OK: all corrected values are legal\n",
      "    11s Processing /Users/dirk/github/valence/workflow/corr_filled/CWB_etcbc4b.csv\n",
      "    11s CWB: Found   857 corrections in /Users/dirk/github/valence/workflow/corr_filled/CWB_etcbc4b.csv\n",
      "    11s OK: Corrected phrases did not receive multiple corrections\n",
      "    11s OK: all corrected nodes where phrase nodes\n",
      "    11s OK: all corrected values are legal\n",
      "    11s Processing /Users/dirk/github/valence/workflow/corr_filled/FJM_etcbc4b.csv\n",
      "    11s FJM: Found   979 corrections in /Users/dirk/github/valence/workflow/corr_filled/FJM_etcbc4b.csv\n",
      "    11s OK: Corrected phrases did not receive multiple corrections\n",
      "    11s OK: all corrected nodes where phrase nodes\n",
      "    11s OK: all corrected values are legal\n",
      "    11s Processing /Users/dirk/github/valence/workflow/corr_filled/HLK_etcbc4b.csv\n",
      "    12s HLK: Found  1138 corrections in /Users/dirk/github/valence/workflow/corr_filled/HLK_etcbc4b.csv\n",
      "    12s OK: Corrected phrases did not receive multiple corrections\n",
      "    12s OK: all corrected nodes where phrase nodes\n",
      "    12s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    12s NO file /Users/dirk/github/valence/workflow/corr_filled/JRD_etcbc4b.csv\n",
      "    12s NO file /Users/dirk/github/valence/workflow/corr_filled/JYa_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    12s Processing /Users/dirk/github/valence/workflow/corr_filled/NFa_etcbc4b.csv\n",
      "    12s NF>: Found  1239 corrections in /Users/dirk/github/valence/workflow/corr_filled/NFa_etcbc4b.csv\n",
      "    12s OK: Corrected phrases did not receive multiple corrections\n",
      "    12s OK: all corrected nodes where phrase nodes\n",
      "    12s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    12s NO file /Users/dirk/github/valence/workflow/corr_filled/NPL_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    12s Processing /Users/dirk/github/valence/workflow/corr_filled/NTN_etcbc4b.csv\n",
      "    12s NTN: Found  1384 corrections in /Users/dirk/github/valence/workflow/corr_filled/NTN_etcbc4b.csv\n",
      "    12s OK: Corrected phrases did not receive multiple corrections\n",
      "    12s OK: all corrected nodes where phrase nodes\n",
      "    12s OK: all corrected values are legal\n",
      "    12s Processing /Users/dirk/github/valence/workflow/corr_filled/NWS_etcbc4b.csv\n",
      "    12s NWS: Found  1395 corrections in /Users/dirk/github/valence/workflow/corr_filled/NWS_etcbc4b.csv\n",
      "    12s OK: Corrected phrases did not receive multiple corrections\n",
      "    12s OK: all corrected nodes where phrase nodes\n",
      "    12s OK: all corrected values are legal\n",
      "    12s Processing /Users/dirk/github/valence/workflow/corr_filled/PQD_etcbc4b.csv\n",
      "    12s PQD: Found  1421 corrections in /Users/dirk/github/valence/workflow/corr_filled/PQD_etcbc4b.csv\n",
      "    12s OK: Corrected phrases did not receive multiple corrections\n",
      "    12s OK: all corrected nodes where phrase nodes\n",
      "    12s OK: all corrected values are legal\n",
      "    12s Processing /Users/dirk/github/valence/workflow/corr_filled/QRa_etcbc4b.csv\n",
      "    12s QR>: Found  1424 corrections in /Users/dirk/github/valence/workflow/corr_filled/QRa_etcbc4b.csv\n",
      "    12s OK: Corrected phrases did not receive multiple corrections\n",
      "    12s OK: all corrected nodes where phrase nodes\n",
      "    12s OK: all corrected values are legal\n",
      "    12s Processing /Users/dirk/github/valence/workflow/corr_filled/SWR_etcbc4b.csv\n",
      "    12s SWR: Found  1449 corrections in /Users/dirk/github/valence/workflow/corr_filled/SWR_etcbc4b.csv\n",
      "    12s OK: Corrected phrases did not receive multiple corrections\n",
      "    12s OK: all corrected nodes where phrase nodes\n",
      "    12s OK: all corrected values are legal\n",
      "    12s Found 1449 corrections in the phrase function\n",
      "    12s 42851  phrases seen 1  time(s)\n",
      "    12s 138    phrases seen 2  time(s)\n",
      "    12s 3      phrases seen 3  time(s)\n",
      "    12s Total phrases seen: 42992\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "pf_corr = {}\n",
    "\n",
    "def read_corr():\n",
    "    function_values = legal_values['function']\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        repeated = collections.defaultdict(list)\n",
    "        non_phrase = set()\n",
    "        illegal_fvalue = set()\n",
    "\n",
    "        filename = vfile(verb, 'corr_filled')\n",
    "        if not os.path.exists(filename):\n",
    "            error('NO file {}'.format(filename))\n",
    "            continue\n",
    "        else:\n",
    "            info('Processing {}'.format(filename))\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[2+4*i], fields[2+4*i+3])\n",
    "                    if pn != '':\n",
    "                        pc = pc.strip()\n",
    "                        pn = int(pn)\n",
    "                        phrases_seen[pn] += 1\n",
    "                        if pc != '':\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                if F.otype.v(pn) != 'phrase': \n",
    "                                    non_phrase.add(pn)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        info('{}: Found {:>5} corrections in {}'.format(verb, len(pf_corr), filename))\n",
    "        if len(repeated):\n",
    "            error('ERROR: Some phrases have been corrected multiple times!')\n",
    "            for x in sorted(repeated):\n",
    "                error('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "        else:\n",
    "            info('OK: Corrected phrases did not receive multiple corrections')\n",
    "        if len(non_phrase):\n",
    "            error('ERROR: Corrections have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "        else:\n",
    "            info('OK: all corrected nodes where phrase nodes')\n",
    "        if len(illegal_fvalue):\n",
    "            error('ERROR: Some corrections supply illegal values for phrase function!')\n",
    "            error('`{}`'.format('`, `'.join(illegal_fvalue)))\n",
    "        else:\n",
    "            info('OK: all corrected values are legal')\n",
    "    info('Found {} corrections in the phrase function'.format(len(pf_corr)))\n",
    "        \n",
    "read_corr()\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    info('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "info('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Enrichment\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    12s 6 Enrich field specifications OK\n",
      "valence = {adjunct, complement, core}\n",
      "predication = {NA, copula, regular}\n",
      "grammatical = {*, K_object, L_object, NA, NP_direct_object, direct_object, indirect_object, infinitive_object, principal_direct_object, subject}\n",
      "original = {*, K_object, L_object, NA, NP_direct_object, direct_object, indirect_object, infinitive_object, principal_direct_object, subject}\n",
      "lexical = {location, time}\n",
      "semantic = {benefactive, instrument, location, manner, time}\n"
     ]
    }
   ],
   "source": [
    "enrich_field_spec = '''\n",
    "valence\n",
    "    adjunct\n",
    "    complement\n",
    "    core\n",
    "\n",
    "predication\n",
    "    NA\n",
    "    regular\n",
    "    copula\n",
    "\n",
    "grammatical\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "original\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "lexical\n",
    "    location\n",
    "    time\n",
    "\n",
    "semantic\n",
    "    benefactive\n",
    "    time\n",
    "    location\n",
    "    instrument\n",
    "    manner\n",
    "'''\n",
    "enrich_fields = collections.OrderedDict()\n",
    "cur_e = None\n",
    "for line in enrich_field_spec.strip().split('\\n'):\n",
    "    if line.startswith(' '):\n",
    "        enrich_fields.setdefault(cur_e, set()).add(line.strip())\n",
    "    else:\n",
    "        cur_e = line.strip()\n",
    "nef = len(enrich_fields)\n",
    "if None in enrich_fields:\n",
    "    error('Invalid enrich field specification')\n",
    "else:\n",
    "    info('{} Enrich field specifications OK'.format(nef))\n",
    "for ef in enrich_fields:\n",
    "    info('{} = {{{}}}'.format(ef, ', '.join(sorted(enrich_fields[ef]))), tm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enrich_baseline_rules = dict(\n",
    "    phrase='''Adju\tAdjunct\tadjunct\tNA\tNA\t\t\t\n",
    "Cmpl\tComplement\tcomplement\tNA\t*\t\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\t\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\tNA\t\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Exst\tExistence\tcore\tcopula\tNA\t\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\t\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\t\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tNA\t\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\t\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\tNA\t\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\t\tNA\tNA\n",
    "Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\tNA\t\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PreC\tPredicate complement\tcore\tregular\tNA\t\t\t\n",
    "Pred\tPredicate\tcore\tregular\tNA\t\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\t\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\t\tNA\tNA\n",
    "Subj\tSubject\tcore\tNA\tsubject\t\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\tNA\t\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\tNA\t\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\t\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\t\tNA\tNA''',\n",
    "    clause='''Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "InfC\tInfinitive Construct clause\tNA\tNA\t\t\t\t''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    12s Enrich baseline rules are OK (204 good)\n"
     ]
    }
   ],
   "source": [
    "transform = collections.OrderedDict((('phrase', {}), ('clause', {})))\n",
    "errors = 0\n",
    "good = 0\n",
    "\n",
    "for kind in ('phrase', 'clause'):\n",
    "    for line in enrich_baseline_rules[kind].split('\\n'):\n",
    "        x = line.split('\\t')\n",
    "        nefields = len(x) - 2\n",
    "        if len(x) - 2 != nef:\n",
    "            error('Wrong number of fields ({} must be {}) in {}:\\n{}'.format(nefields, nef, kind, line))\n",
    "            errors += 1\n",
    "        transform[kind][x[0]] = dict(zip(enrich_fields, x[2:]))\n",
    "    for e in error_values['function']:\n",
    "        transform[kind][e] = dict(zip(enrich_fields, ['']*nef))\n",
    "\n",
    "    for f in transform[kind]:\n",
    "        for e in enrich_fields:\n",
    "            val = transform[kind][f][e]\n",
    "            if val != '' and val != 'NA' and val not in enrich_fields[e]:\n",
    "                error('Defaults for `{}` ({}): wrong `{}` value: \"{}\"'.format(f, kind, e, val))\n",
    "                errors += 1\n",
    "            else: good += 1\n",
    "if errors:\n",
    "    error('There were {} errors ({} good)'.format(errors, good))\n",
    "else:\n",
    "    info('Enrich baseline rules are OK ({} good)'.format(good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prettyprint the baseline rules of enrichment for easier reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func    : valence        predication    grammatical    original       lexical        semantic       \n",
      "[phrase]\n",
      "Adju    : adjunct        NA             NA                                                          \n",
      "BoundErr:                                                                                           \n",
      "Cmpl    : complement     NA             *                                                           \n",
      "Conj    : NA             NA             NA                            NA             NA             \n",
      "EPPr    : NA             copula         NA                                                          \n",
      "ExsS    : core           copula         subject                                                     \n",
      "Exst    : core           copula         NA                                                          \n",
      "Frnt    : NA             NA             NA                            NA             NA             \n",
      "IntS    : core           NA             subject                                                     \n",
      "Intj    : NA             NA             NA                            NA             NA             \n",
      "Loca    : adjunct        NA             NA                            location       location       \n",
      "ModS    : core           NA             subject                                                     \n",
      "Modi    : NA             NA             NA                            NA             NA             \n",
      "NCoS    : core           copula         subject                                                     \n",
      "NCop    : core           copula         NA                                                          \n",
      "Nega    : NA             NA             NA                            NA             NA             \n",
      "Objc    : complement     NA             direct_object                                               \n",
      "PrAd    : adjunct        NA             NA                                                          \n",
      "PrcS    : core           regular        subject                                                     \n",
      "PreC    : core           regular        NA                                                          \n",
      "PreO    : core           regular        direct_object                                               \n",
      "PreS    : core           regular        subject                                                     \n",
      "Pred    : core           regular        NA                                                          \n",
      "PtcO    : core           regular        direct_object                                               \n",
      "Ques    : NA             NA             NA                            NA             NA             \n",
      "Rela    : NA             NA             NA                            NA             NA             \n",
      "Subj    : core           NA             subject                                                     \n",
      "Supp    : adjunct        NA             NA                                           benefactive    \n",
      "Time    : adjunct        NA             NA                            time           time           \n",
      "Unkn    : NA             NA             NA                            NA             NA             \n",
      "Voct    : NA             NA             NA                            NA             NA             \n",
      "[clause]\n",
      "BoundErr:                                                                                           \n",
      "InfC    : NA             NA                                                                         \n",
      "Objc    : complement     NA             direct_object                                               \n"
     ]
    }
   ],
   "source": [
    "ltpl = '{:<8}: '+('{:<15}' * nef)\n",
    "info(ltpl.format('func', *enrich_fields), tm=False)\n",
    "for kind in transform:\n",
    "    info('[{}]'.format(kind), tm=False)\n",
    "    for f in sorted(transform[kind]):\n",
    "        sfs = transform[kind][f]\n",
    "        info(ltpl.format(f, *[sfs[sf] for sf in enrich_fields]), tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Enrichment logic\n",
    "\n",
    "For certain verbs and certain conditions, we can automatically fill in some of the new features.\n",
    "For example, if the verb is `CJT`, and if an adjunct phrase is personal, starting with `L`, we know that the semantic role is *benefactive*.\n",
    "\n",
    "We will also analyse the direct and indirect objects more precisely and implement heuristics to make a distinction between complements (locative) and indirect objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the direct objects\n",
    "\n",
    "In the target clauses we will find the direct object(s).\n",
    "If there is more than one, we will compute which is the principal one.\n",
    "The others are secundary ones.\n",
    "If there is only one direct object, we do not mark it as principal.\n",
    "\n",
    "An object can be a phrase or a clause.\n",
    "\n",
    "### Clauses as objects\n",
    "We will treat clauses marked as `Objc` by feature `rela` as direct objects.\n",
    "Additionally, we identify clauses marked as `InfC` by feature `typ` as direct objects if they are preceded by the preposition *L* and if there is a direct object phrase elsewhere in the clause.\n",
    "\n",
    "We will not mark all these object clauses as principal direct objects, by rules stated later on.\n",
    "\n",
    "### Implied objects\n",
    "\n",
    "There are many cases where there is a direct object without it being marked as such in the data.\n",
    "Those are cases where there are no objective, unambiguous signals for a direct object.\n",
    "We call them *implied objects*. Examples: \n",
    "\n",
    "* the relativum in relative clauses\n",
    "* complements starting with MN (from) or L (to)\n",
    "* assumed objects: objects that figure in the context, but do not have a concrete presence in the clause under\n",
    "  consideration\n",
    "\n",
    "In the case of implied objects we have to guess.\n",
    "Initially we assume that there are no implied objects.\n",
    "\n",
    "Later, when we inspect individual cases, we can mark principal objects and implied objects manually\n",
    "for those cases where these rules do not suffice.\n",
    "\n",
    "### Finding the principal direct object\n",
    "\n",
    "When there are multiple direct objects, we use the rules formulated by (Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) to determine which one is the principal one. The rules are stated below where we make some remarks about how we apply them to our data.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "When looking for principal direct objects, we restrict ourselves to direct objects at the phrase level, either being complete phrases, or pronominal suffixes within phrases. The following rules express a preference for the principal direct object. In a given context, we select the direct object that is preferred by applying those rules as the principal direct object. We only apply these rules if there are at least two direct objects.\n",
    "If there is only one direct object, it is not marked as principal.\n",
    "\n",
    "#### Rule 1: pronominal suffixes > preferred above marked objects > unmarked objects\n",
    "\n",
    "In a given clause, we collect all phrases with function ``PreO`` or ``PtcO``. \n",
    "If this collection is non-empty, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise, we proceed as follows.\n",
    "\n",
    "We collect all the phrases with function ``Objc``.\n",
    "If this collection is empty, there will not be a principal object.\n",
    "Otherwise, we split it up in marked and unmarked object phrases.\n",
    "\n",
    "An object phrase is *marked* if and only if it contains, somewhere, the object marker ``>T``.\n",
    "If there are marked object phrases, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 2: determined phrases > undetermined phrases\n",
    "\n",
    "We only arrive here if there are multiple ``Objc`` phrases, neither of which is marked.\n",
    "In this case, we take the textually first one (by rule 3) which has the value ``det`` for its feature ``det``, if there is one, and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 3: earlier phrases > later phrases (by textual order)\n",
    "\n",
    "This rule is implicitly applied if one of the rules before yielded more than one candidate for the principal object. Furthermore, we arrive here if the previous rules have not selected any principal direct object, while we do have more than one ``Objc`` phrase.\n",
    "\n",
    "In this case, we pick the textually first ``Objc`` phrase.\n",
    "\n",
    "### Non principal objects\n",
    "\n",
    "In case there is a principal object, we divide the other objects into two kinds:\n",
    "* clause objects\n",
    "* phrase objects\n",
    "\n",
    "We will give the phrase objects the grammatical label `NP_direct_object`.\n",
    "\n",
    "### Complements as LK Objects\n",
    "\n",
    "In some cases, a complement functions as objects, such as in [Genesis 21:13](https://shebanq.ancient-data.org/hebrew/text?nget=v&chapter=21&book=Genesis&qw=n&tp=txt_tb1&version=4b&mr=m) *I make him (into) a people*.\n",
    "\n",
    "Candidates are those complements that: \n",
    "\n",
    "* start with either preposition ``L`` or ``K`` and\n",
    "* the ``L`` or ``K`` in question does not carry a pronominal suffix\n",
    "* should also not be followed by a body part\n",
    "\n",
    "We generated grammatical labels ``L_object`` and ``K_object`` in these cases.\n",
    "The flowchart will make a distinction between ``L_object`` and ``K_object``.\n",
    "\n",
    "An L/K object is never a *principal* direct object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objectfuncs = set('''\n",
    "Objc PreO PtcO\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_obj_preps = set('''\n",
    "K L\n",
    "'''.strip().split())\n",
    "\n",
    "no_prs = set('''\n",
    "absent n/a\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_parts = set('''\n",
    ">NP/ >P/ >PSJM/ >YB</ >ZN/\n",
    "<JN/ <NQ/ <RP/ <YM/ <YM==/\n",
    "BHN/ BHWN/ BVN/\n",
    "CD=/ CD===/ CKM/ CN/\n",
    "DD/\n",
    "GRGRT/ GRM/ GRWN/ GW/ GW=/ GWJH/ GWPH/ GXWN/\n",
    "FPH/\n",
    "JD/ JRK/ JRKH/\n",
    "KRF/ KSL=/ KTP/\n",
    "L</ LCN/ LCWN/ LXJ/\n",
    "M<H/ MPRQT/ MTL<WT/ MTNJM/ MYX/\n",
    "NBLH=/\n",
    "P<M/ PGR/ PH/ PM/ PNH/ PT=/\n",
    "QRSL/\n",
    "R>C/ RGL/\n",
    "XDH/ XLY/ XMC=/ XRY/\n",
    "YW>R/\n",
    "ZRW</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    13s Finding direct objects and determining the principal one\n",
      "    14s Done\n",
      " 1479 clauses with  1 principal  objects\n",
      "14401 clauses with  0 principal  objects\n",
      " 1479 clauses with  a principal  objects\n",
      "    2 clauses with  2 direct     objects\n",
      " 4908 clauses with  1 direct     objects\n",
      "10970 clauses with  0 direct     objects\n",
      " 4910 clauses with  a direct     objects\n",
      "  449 clauses with  1 NP         objects\n",
      "15431 clauses with  0 NP         objects\n",
      "  449 clauses with  a NP         objects\n",
      "   23 clauses with  2 L          objects\n",
      " 1329 clauses with  1 L          objects\n",
      "14528 clauses with  0 L          objects\n",
      " 1352 clauses with  a L          objects\n",
      "   79 clauses with  1 K          objects\n",
      "15801 clauses with  0 K          objects\n",
      "   79 clauses with  a K          objects\n",
      "    2 clauses with  2 clause     objects\n",
      "  180 clauses with  1 clause     objects\n",
      "15698 clauses with  0 clause     objects\n",
      "  182 clauses with  a clause     objects\n",
      "    4 clauses with  2 infinitive objects\n",
      "  341 clauses with  1 infinitive objects\n",
      "15535 clauses with  0 infinitive objects\n",
      "  345 clauses with  a infinitive objects\n",
      "15880 clauses with  a selected verb\n"
     ]
    }
   ],
   "source": [
    "info('Finding direct objects and determining the principal one')\n",
    "clause_objects = collections.defaultdict(set)\n",
    "objects = collections.defaultdict(set)\n",
    "objects_count = collections.defaultdict(collections.Counter)\n",
    "object_kinds = (\n",
    "    'principal',\n",
    "    'direct',\n",
    "    'NP',\n",
    "    'L',\n",
    "    'K',\n",
    "    'clause',\n",
    "    'infinitive',\n",
    ")\n",
    "\n",
    "def is_marked(phr):\n",
    "    # simple criterion for determining whether a direct object is marked:\n",
    "    # has it the object marker somewhere?\n",
    "    words = L.d(p, 'word')\n",
    "    has_et = False\n",
    "    for w in words:\n",
    "        if F.lex.v(w) == '>T':\n",
    "            has_et = True\n",
    "            break\n",
    "    return has_et\n",
    "\n",
    "for c in clause_verb:\n",
    "    these_objects = collections.defaultdict(set)\n",
    "    direct_objects_cat = collections.defaultdict(set)\n",
    "\n",
    "    for p in L.d(c, 'phrase'):\n",
    "        pf = pf_corr.get(p, F.function.v(p))  # NB we take the corrected value for phrase function if there is one\n",
    "        if pf in objectfuncs:\n",
    "            direct_objects_cat['p_'+pf].add(p)\n",
    "            these_objects['direct'].add(p)\n",
    "        elif pf == 'Cmpl':\n",
    "            pwords = L.d(p, 'word')\n",
    "            w1 = pwords[0]\n",
    "            w1l = F.lex.v(w1)\n",
    "            w2l = F.lex.v(pwords[1]) if len(pwords) > 1 else None\n",
    "            if w1l in cmpl_as_obj_preps and F.prs.v(w1) in no_prs and not (w1l == 'L' and w2l in body_parts):\n",
    "                if w1l == 'K': these_objects['K'].add(p)\n",
    "                elif w1l == 'L': these_objects['L'].add(p)\n",
    "        \n",
    "    # find clause objects\n",
    "    for ac in L.d(L.u(c, 'sentence')[0], 'clause'):\n",
    "        mothers = list(E.mother.f(ac))\n",
    "        if not (mothers and mothers[0] == c): continue\n",
    "        cr = F.rela.v(ac)\n",
    "        ct = F.typ.v(ac)\n",
    "        if cr in {'Objc'} or ct in {'InfC'}:\n",
    "            clause_objects[c].add(ac)\n",
    "            if cr in {'Objc'}:\n",
    "                label = cr\n",
    "                direct_objects_cat['c_'+label].add(ac)\n",
    "                these_objects['direct'].add(ac)\n",
    "                these_objects['clause'].add(ac)\n",
    "            elif ct in {'InfC'}:\n",
    "                if F.lex.v(L.d(ac, 'word')[0]) == 'L':\n",
    "                    these_objects['infinitive'].add(ac)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # order the objects in the natural ordering\n",
    "    direct_objects_order = sortNodes(these_objects.get('direct', set()))\n",
    "    nobjects = len(direct_objects_order)\n",
    "\n",
    "    # compute the principal object\n",
    "    principal_object = None\n",
    "\n",
    "    for x in [1]:\n",
    "        # just one object \n",
    "        if nobjects == 1:\n",
    "            # we have chosen not to mark a principal object if there is only one object\n",
    "            # the alternative is to mark it if it is a phrase. Uncomment the next 2 lines if you want this\n",
    "            # theobject = list(dobjects_set)[0]\n",
    "            # if F.otype.v(theobject) == 'phrase': principal_object = theobject\n",
    "            break\n",
    "        # rule 1: suffixes and promoted objects\n",
    "        principal_candidates =\\\n",
    "            direct_objects_cat.get('p_PreO', set()) |\\\n",
    "            direct_objects_cat.get('p_PtcO', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            principal_object = sortNodes(principal_candidates)[0]\n",
    "            break\n",
    "        principal_candidates = direct_objects_cat.get('p_Objc', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            if len(principal_candidates) == 1:\n",
    "                principal_object = list(principal_candidates)[0]\n",
    "                break\n",
    "            objects_marked = set()\n",
    "            objects_unmarked = set()\n",
    "            for p in principal_candidates:\n",
    "                if is_marked(p):\n",
    "                    objects_marked.add(p)\n",
    "                else:\n",
    "                    objects_unmarked.add(p)\n",
    "            if len(objects_marked) != 0:\n",
    "                principal_object = sortNodes(objects_marked)[0]\n",
    "                break\n",
    "            if len(objects_unmarked) != 0:\n",
    "                principal_object = sortNodes(objects_unmarked)[0]\n",
    "                break            \n",
    "    if principal_object != None:\n",
    "        these_objects['principal'].add(principal_object)\n",
    "    if len(these_objects['infinitive']) and not len(these_objects['direct']):\n",
    "        # we do not mark an infinitive object if there is no proper direct object around\n",
    "        these_objects['infinitive'] = set()\n",
    "    if len(these_objects['principal']):\n",
    "        these_objects['direct'] -= these_objects['principal']\n",
    "        for x in these_objects['direct'] - these_objects['clause']:\n",
    "            # the NP objects are the non-principal phrase like direct objects\n",
    "            these_objects['NP'].add(x)\n",
    "        these_objects['direct'] -= these_objects['NP']\n",
    "    if len(these_objects['principal']) == 0 and len(these_objects['direct']) and (\n",
    "        len(these_objects['NP']) or\\\n",
    "        len(these_objects['L']) or\\\n",
    "        len(these_objects['K']) or\\\n",
    "        len(these_objects['infinitive'])\n",
    "    ): # promote the direct objects to principal direct objects\n",
    "        these_objects['principal'] = these_objects['direct']\n",
    "        these_objects['direct'] = set()\n",
    "\n",
    "    for kind in object_kinds:\n",
    "        n = len(these_objects.get(kind, set()))\n",
    "        objects_count[kind][n] += 1\n",
    "        if n:\n",
    "            objects[kind] |= these_objects[kind]\n",
    "\n",
    "info('Done')\n",
    "\n",
    "for kind in object_kinds:\n",
    "    total = 0\n",
    "    for (count, n) in sorted(objects_count[kind].items(), key=lambda y: -y[0]):\n",
    "        if count: total += n\n",
    "        info('{:>5} clauses with {:>2} {:<10} objects'.format(n, count, kind), tm=False)\n",
    "    info('{:>5} clauses with {:>2} {:<10} objects'.format(total, 'a', kind), tm=False)\n",
    "info('{:>5} clauses with {:>2} selected verb'.format(len(clause_verb), 'a'), tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding indirect objects\n",
    "\n",
    "The ETCBC database has not feature that marks indirect objects.\n",
    "We will use computation to determine whether a complement is an indirect object or a locative.\n",
    "This computation is just an approximation.\n",
    "\n",
    "#### Cues for a locative complement\n",
    "\n",
    "* ``# loc lexemes`` how many distinct lexemes with a locative meaning occur in the complement (given by a fixed list)\n",
    "* ``# topo`` how many lexemes with nametype = ``topo`` occur in the complement (nametype is a feature of the lexicon)\n",
    "* ``# prep_b`` how many occurrences of the preposition ``B`` occur in the complement\n",
    "* ``# h_loc`` how many H-locales are carried on words in the complement\n",
    "* ``body_part`` is 2 if the phrase starts with the preposition ``L`` followed by a body part, else 0\n",
    "* ``locativity`` ($loc$) a crude measure of the locativity of the complement, just the sum of ``# loc lexemes``, ``#topo``, ``# prep_b``, ``# h_loc`` and ``body_part``.\n",
    "\n",
    "#### Cues for an indirect object\n",
    "* ``# prep_l`` how many occurrences of the preposition ``L`` or ``>L`` with a pronominal suffix on it occur in the complement\n",
    "* ``# L prop`` how many occurrences of ``L`` or ``>L`` plus proper name or person reference word occur in the complement\n",
    "* ``indirect object`` ($ind$) a crude indicator of whether the complement is an indirect object, just the sum of ``# prep_l`` and ``# L prop`` \n",
    "\n",
    "#### The decision\n",
    "\n",
    "We take a decision as follows.\n",
    "The outcome is $L$ (complement is *locative*) or $I$ (complement is *indirect object*) or $C$ (complement is neither *locative* nor *indirect object*)\n",
    "\n",
    "(1) $ loc > 0 \\wedge ind = 0 \\Rightarrow L $\n",
    "\n",
    "(2) $ loc = 0 \\wedge ind > 0 \\Rightarrow I $\n",
    "\n",
    "(3) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc - 1 > ind \\Rightarrow L$\n",
    "\n",
    "(4) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc + 1 < ind \\Rightarrow I$\n",
    "\n",
    "(5) $ loc > 0 \\wedge ind > 0 \\wedge |ind - loc| <= 1 \\Rightarrow C$\n",
    "\n",
    "In words:\n",
    "\n",
    "* if there are positive signals for L or I and none for the other, we choose the one for which there are positive signals;\n",
    "* if there are positive signals for both L and I, we follow the majority count, but only if the difference is at least two;\n",
    "* in all other cases we leave it at C: not necessarilty locative and not necessarily indirect object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complfuncs = set('''\n",
    "Cmpl PreC\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_iobj_preps = set('''\n",
    "L >L\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locative_lexemes = set('''\n",
    ">RY/ >YL/ >XR/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJN/ BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "'''.strip().split())\n",
    "\n",
    "personal_lexemes = set('''\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/ MYRJ/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    14s Determinig kind of complements\n",
      "    15s Done\n",
      "Phrases of kind C :   4630\n",
      "Phrases of kind L :   4440\n",
      "Phrases of kind I :   2066\n",
      "Total complements :  11136\n",
      "Total phrases     :  51953\n"
     ]
    }
   ],
   "source": [
    "info('Determinig kind of complements')\n",
    "\n",
    "complements_c = collections.defaultdict(lambda: collections.defaultdict(lambda: []))\n",
    "complements = {}\n",
    "complementk = {}\n",
    "kcomplements = collections.Counter()\n",
    "\n",
    "nphrases = 0\n",
    "ncomplements = 0\n",
    "\n",
    "for c in clause_verb:\n",
    "    for p in L.d(c, 'phrase'):\n",
    "        nphrases += 1\n",
    "        pf = pf_corr.get(p, F.function.v(p))\n",
    "        if pf not in complfuncs: continue\n",
    "        ncomplements += 1\n",
    "        words = L.d(p, 'word')\n",
    "        lexemes = [F.lex.v(w) for w in words]\n",
    "        lexeme_set = set(lexemes)\n",
    "\n",
    "        # measuring locativity\n",
    "        lex_locativity = len(locative_lexemes & lexeme_set)\n",
    "        prep_b = len([x for x in lexeme_set if x == 'B'])\n",
    "        topo = len([x for x in words if F.nametype.v(x) == 'topo'])\n",
    "        h_loc = len([x for x in words if F.uvf.v(x) == 'H'])\n",
    "        body_part = 0\n",
    "        if len(words) > 1 and F.lex.v(words[0]) == 'L' and F.lex.v(words[1]) in body_parts:\n",
    "            body_part = 2\n",
    "        loca = lex_locativity + topo + prep_b + h_loc + body_part\n",
    "\n",
    "        # measuring indirect object\n",
    "        prep_l = len([x for x in words if F.lex.v(x) in cmpl_as_iobj_preps and F.prs.v(x) not in no_prs])\n",
    "        prep_lpr = 0\n",
    "        lwn = len(words)\n",
    "        for (n, wn) in enumerate(words):\n",
    "            if F.lex.v(wn) in cmpl_as_iobj_preps:\n",
    "                if n+1 < lwn:\n",
    "                    nextw = words[n+1]\n",
    "                    if F.lex.v(nextw) in personal_lexemes or F.ls.v(nextw) == 'gntl' or (\n",
    "                        F.sp.v(nextw) == 'nmpr' and F.nametype.v(nextw) == 'pers'):\n",
    "                        prep_lpr += 1                        \n",
    "        indi = prep_l + prep_lpr\n",
    "\n",
    "        # the verdict\n",
    "        ckind = 'C'\n",
    "        if loca == 0 and indi > 0: ckind = 'I'\n",
    "        elif loca > 0 and indi == 0: ckind = 'L'\n",
    "        elif loca > indi + 1: ckind = 'L'\n",
    "        elif loca < indi - 1: ckind = 'I'\n",
    "        complementk[p] = (loca, indi, ckind)\n",
    "        kcomplements[ckind] += 1\n",
    "        complements_c[c][ckind].append(p)\n",
    "        complements[p] = (pf, ckind)\n",
    "\n",
    "info('Done')\n",
    "for (label, n) in sorted(kcomplements.items(), key=lambda y: -y[1]):\n",
    "    info('Phrases of kind {:<2}: {:>6}'.format(label, n), tm=False)\n",
    "info('Total complements : {:>6}'.format(ncomplements), tm=False)\n",
    "info('Total phrases     : {:>6}'.format(nphrases), tm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_L(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len(words) > 0 and F.lex.v(words[0] == 'L')\n",
    "\n",
    "def is_lex_personal(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len(words) > 1 and (F.lex.v(words[1]) in personal_lexemes or F.nametype.v(words[1]) == 'pers')\n",
    "\n",
    "def is_lex_local(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len({F.lex.v(w) for w in words} & locative_lexemes) > 0\n",
    "\n",
    "def has_H_locale(vl, pn):\n",
    "    words = L.d(pn, 'word')\n",
    "    return len({w for w in words if F.uvf.v(w) == 'H'}) > 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic logic\n",
    "\n",
    "This is the function that applies the generic rules about (in)direct objects and locatives.\n",
    "It takes a phrase node and a set of new label values, and modifies those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grule_as_str = {\n",
    "    'pdos':   '''direct_object => principal_direct_object''',\n",
    "    'pdos-x': '''non-object => principal_direct_object''',\n",
    "    'ndos':   '''direct_object => NP_direct_object''',\n",
    "    'ndos-x': '''non-object => NP_direct_object''',\n",
    "    'dos':    '''non-object => direct_object''',\n",
    "    'ldos':   '''non-object => L_object''',\n",
    "    'kdos':   '''non-object => K_object''',\n",
    "    'inds-c': '''complement => indirect_object''',\n",
    "    'locs-c': '''complement => location''',\n",
    "    'inds-p': '''predicate complement => indirect_object''',\n",
    "    'locs-p': '''predicate complement => location''',\n",
    "    'cdos':   '''direct-object =(superfluously)=> direct object (clause)''',\n",
    "    'cdos-x': '''non-object => direct object (clause)''',\n",
    "    'idos':   '''infinitive_object =(superfluously)=> infinitive_object (clause)''',\n",
    "    'idos-x': '''infinitive clause => infinitive_object''',\n",
    "}\n",
    "\n",
    "def rule_as_str_g(x, i): return '{}-{}'.format(i, grule_as_str[i])\n",
    "\n",
    "rule_as_str = dict(\n",
    "    generic=rule_as_str_g,\n",
    ")\n",
    "\n",
    "def generic_logic_p(pn, values):\n",
    "    gl = None\n",
    "    if pn in objects['principal']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'pdos'\n",
    "        else:\n",
    "            gl = 'pdos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'principal_direct_object'\n",
    "    elif pn in objects['NP']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'ndos'\n",
    "        else:\n",
    "            gl = 'ndos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'NP_direct_object'\n",
    "    elif pn in objects['direct']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv != 'direct_object':\n",
    "            gl = 'dos'\n",
    "            values['original'] = oldv\n",
    "            values['grammatical'] = 'direct_object'\n",
    "    elif pn in objects['L']:\n",
    "        oldv = values['grammatical']\n",
    "        gl = 'ldos'\n",
    "        values['original'] = oldv\n",
    "        values['grammatical'] = 'L_object'\n",
    "    elif pn in objects['K']:\n",
    "        oldv = values['grammatical']\n",
    "        gl = 'kdos'\n",
    "        values['original'] = oldv\n",
    "        values['grammatical'] = 'K_object'\n",
    "    elif pn in complements:\n",
    "        (pf, ck) = complements[pn]\n",
    "        if ck in {'I', 'L'}:\n",
    "            if pf == 'Cmpl':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 'inds-c'\n",
    "                else:\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 'locs-c'\n",
    "            elif pf == 'PreC':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 'inds-p'\n",
    "                else:\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 'locs-p'\n",
    "    return gl\n",
    "\n",
    "def generic_logic_c(cn, values):\n",
    "    gl = None\n",
    "    if cn in objects['clause']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'cdos'\n",
    "        else:\n",
    "            gl = 'cdos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'direct_object'\n",
    "    elif cn in objects['infinitive']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'infinitive_object':\n",
    "            gl = 'idos'\n",
    "        else:\n",
    "            gl = 'idos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'infinitive_object'\n",
    "    return gl\n",
    "\n",
    "generic_logic = dict(\n",
    "    phrase=generic_logic_p,\n",
    "    clause=generic_logic_c,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Verb specific rules\n",
    "\n",
    "The verb-specific enrichment rules are stored in a dictionary, keyed  by the verb lexeme.\n",
    "The rule itself is a list of items.\n",
    "\n",
    "The last item is a tuple of conditions that need to be fulfilled to apply the rule.\n",
    "\n",
    "A condition can take the shape of\n",
    "\n",
    "* a function, taking a phrase or clause node as argument and returning a boolean value\n",
    "* an ETCBC feature for phrases or clauses : value, \n",
    "  which is true iff that feature has that value for the phrase or clause in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbl_obj_rules = (\n",
    "    (\n",
    "        ('semantic', 'benefactive'), \n",
    "        ('function:Adju', has_L, is_lex_personal),\n",
    "    ),\n",
    "    (\n",
    "        ('lexical', 'location'),\n",
    "        ('function:Cmpl', has_H_locale),\n",
    "    ),\n",
    "    (\n",
    "        ('lexical', 'location'),\n",
    "        ('semantic', 'location'),\n",
    "        ('function:Cmpl', is_lex_local),\n",
    "    ),\n",
    ")\n",
    "enrich_logic = dict(\n",
    "    phrase={\n",
    "        'CJT': dbl_obj_rules,\n",
    "        'FJM': dbl_obj_rules,\n",
    "    },\n",
    "    clause={\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CJT-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "CJT-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "CJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "FJM-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "FJM-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "FJM-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "All 6 rules OK\n"
     ]
    }
   ],
   "source": [
    "rule_index = collections.defaultdict(lambda: [])\n",
    "\n",
    "def rule_as_str_s(vl, i):\n",
    "    (conditions, sfassignments) = rule_index[vl][i]\n",
    "    label = '{}-{}\\n'.format(vl, i+1)\n",
    "    rule = '\\tIF   {}'.format('\\n\\tAND  '.join(\n",
    "        '{:<10} = {:<8}'.format(\n",
    "                *c.split(':')\n",
    "            ) if type(c) is str else '{:<15}'.format(\n",
    "                c.__name__\n",
    "            ) for c in conditions,\n",
    "    ))\n",
    "    ass = []\n",
    "    for (i, sfa) in enumerate(sfassignments):\n",
    "        ass.append('\\t\\t{:<10} => {:<15}\\n'.format(*sfa))\n",
    "    return '{}{}\\n\\tTHEN\\n{}'.format(label, rule, ''.join(ass))\n",
    "\n",
    "rule_as_str['specific'] = rule_as_str_s\n",
    "\n",
    "def check_logic():\n",
    "    errors = 0\n",
    "    nrules = 0\n",
    "    for kind in sorted(enrich_logic):\n",
    "        for vl in sorted(enrich_logic[kind]):\n",
    "            for items in enrich_logic[kind][vl]:\n",
    "                rule_index[vl].append((items[-1], items[0:-1]))\n",
    "            for (i, (conditions, sfassignments)) in enumerate(rule_index[vl]):\n",
    "                info(rule_as_str_s(vl, i), tm=False)\n",
    "                nrules += 1\n",
    "                for (sf, sfval) in sfassignments:\n",
    "                    if sf not in enrich_fields:\n",
    "                        error('{}: \"{}\" not a valid enrich field'.format(kind, sf), tm=False)\n",
    "                        errors += 1\n",
    "                    elif sfval not in enrich_fields[sf]:\n",
    "                        error('{}: `{}`: \"{}\" not a valid enrich field value'.format(kind, sf, sfval), tm=False)\n",
    "                        errors += 1\n",
    "                for c in conditions:\n",
    "                    if type(c) == str:\n",
    "                        x = c.split(':')\n",
    "                        if len(x) != 2:\n",
    "                            error('{}: Wrong feature condition {}'.format(kind, c), tm=False)\n",
    "                            errors += 1\n",
    "                        else:\n",
    "                            (feat, val) = x\n",
    "                            if feat not in legal_values:\n",
    "                                error('{}: Feature `{}` not in use'.format(kind, feat), tm=False)\n",
    "                                errors += 1\n",
    "                            elif val not in legal_values[feat]:\n",
    "                                error('{}: Feature `{}`: not a valid value \"{}\"'.format(kind, feat, val), tm=False)\n",
    "                                errors += 1\n",
    "    if errors:\n",
    "        error('There were {} errors in {} rules'.format(errors, nrules), tm=False)\n",
    "    else:\n",
    "        info('All {} rules OK'.format(nrules), tm=False)\n",
    "\n",
    "check_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rule_cases = collections.defaultdict(lambda: collections.defaultdict(lambda: {}))\n",
    "\n",
    "def apply_logic(kind, vl, n, init_values):\n",
    "    values = deepcopy(init_values)\n",
    "    gr = generic_logic[kind](n, values)\n",
    "    if gr:\n",
    "        rule_cases['generic'][kind].setdefault(('', gr), []).append(n)\n",
    "    verb_rules = enrich_logic[kind].get(vl, [])\n",
    "    for (i, items) in enumerate(verb_rules):\n",
    "        conditions = items[-1]\n",
    "        sfassignments = items[0:-1]\n",
    "\n",
    "        ok = True\n",
    "        for condition in conditions:\n",
    "            if type(condition) is str:\n",
    "                (feature, value) = condition.split(':')\n",
    "                if feature == 'function' and kind == 'phrase':\n",
    "                    fval = pf_corr.get(n, F.function.v(n))\n",
    "                else:\n",
    "                    fval = F.item[feature].v(n)\n",
    "                this_ok =  fval == value\n",
    "            else:\n",
    "                this_ok = condition(vl, n)\n",
    "            if not this_ok:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                values[sf] = sfval\n",
    "            rule_cases['specific'][kind].setdefault((vl, i), []).append(n)\n",
    "    return tuple(values[sf] for sf in enrich_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "onode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "verb_lexeme\n",
      "verb_stem\n",
      "verb_occurrence\n",
      "text\n",
      "constituent\n",
      "type\n",
      "rela\n",
      "type\n",
      "function\n",
      "valence\n",
      "predication\n",
      "grammatical\n",
      "original\n",
      "lexical\n",
      "semantic\n"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    onode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "    text\n",
    "    constituent\n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    type\n",
    "    function\n",
    "'''.strip().split()\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    type\n",
    "    rela\n",
    "'''.strip().split()\n",
    "\n",
    "field_names = COMMON_FIELDS + CLAUSE_FIELDS + PHRASE_FIELDS + list(enrich_fields) \n",
    "pfillrows = len(CLAUSE_FIELDS)\n",
    "cfillrows = len(PHRASE_FIELDS)\n",
    "fillrows =  pfillrows + cfillrows + len(enrich_fields)\n",
    "info('\\n'.join(field_names), tm=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated enrichment sheet for verb BR> (  198 rows)\n",
      "Generated enrichment sheet for verb QR> ( 3332 rows)\n",
      "Generated enrichment sheet for verb FJM ( 2915 rows)\n",
      "Generated enrichment sheet for verb <LH ( 3893 rows)\n",
      "Generated enrichment sheet for verb NWS (  617 rows)\n",
      "Generated enrichment sheet for verb NPL ( 1932 rows)\n",
      "Generated enrichment sheet for verb NTN ( 9846 rows)\n",
      "Generated enrichment sheet for verb BW> (11102 rows)\n",
      "Generated enrichment sheet for verb <BR ( 2320 rows)\n",
      "Generated enrichment sheet for verb <FH (11323 rows)\n",
      "Generated enrichment sheet for verb NF> ( 2875 rows)\n",
      "Generated enrichment sheet for verb JY> ( 4630 rows)\n",
      "Generated enrichment sheet for verb HLK ( 5819 rows)\n",
      "Generated enrichment sheet for verb JRD ( 1593 rows)\n",
      "Generated enrichment sheet for verb SWR ( 1281 rows)\n",
      "Generated enrichment sheet for verb CJT (  381 rows)\n",
      "Generated enrichment sheet for verb CWB ( 4247 rows)\n",
      "Generated enrichment sheet for verb PQD ( 1283 rows)\n",
      "    21s Done\n",
      "generic-phrase rules:\n",
      " 452 x\n",
      "\tndos-direct_object => NP_direct_object\n",
      "\t605441, 738015, 742447, 606050, 606391, 606408, 606429, 606443, 606634, 607902\n",
      "\n",
      "1473 x\n",
      "\tpdos-direct_object => principal_direct_object\n",
      "\t605442, 738016, 740139, 742446, 605171, 605175, 605211, 605235, 605239, 605726\n",
      "\n",
      "  19 x\n",
      "\tlocs-p-predicate complement => location\n",
      "\t737241, 800765, 816577, 773645, 773836, 784988, 809578, 792495, 792848, 804405\n",
      "\n",
      "1388 x\n",
      "\tldos-non-object => L_object\n",
      "\t605170, 605173, 605210, 605234, 605237, 605727, 605764, 608319, 609178, 610564\n",
      "\n",
      "1657 x\n",
      "\tinds-c-complement => indirect_object\n",
      "\t605713, 605718, 609990, 610833, 610898, 611255, 611323, 613033, 613037, 613887\n",
      "\n",
      "  79 x\n",
      "\tkdos-non-object => K_object\n",
      "\t606309, 608530, 616023, 622450, 661560, 715464, 730717, 732933, 736568, 738770\n",
      "\n",
      "4141 x\n",
      "\tlocs-c-complement => location\n",
      "\t606412, 608214, 608393, 611088, 613125, 619443, 622380, 633240, 634167, 634293\n",
      "\n",
      "   4 x\n",
      "\tinds-p-predicate complement => indirect_object\n",
      "\t853848, 691075, 704298, 654845\n",
      "\n",
      "  9213 generic-phrase rule applications\n",
      "generic-clause rules:\n",
      " 350 x\n",
      "\tidos-x-infinitive clause => infinitive_object\n",
      "\t471761, 427066, 429789, 439386, 442716, 450804, 453414, 472282, 472284, 474093\n",
      "\n",
      " 184 x\n",
      "\tcdos-direct-object =(superfluously)=> direct object (clause)\n",
      "\t431302, 438166, 449837, 469304, 472342, 472380, 488585, 488587, 448458, 466593\n",
      "\n",
      "   534 generic-clause rule applications\n",
      "  9747 generic rule applications\n",
      "specific-phrase rules:\n",
      " 108 x\n",
      "\tFJM-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "\t605599, 608530, 611242, 611636, 611729, 614008, 614116, 615076, 615425, 616023\n",
      "\n",
      "   1 x\n",
      "\tFJM-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "\t766679\n",
      "\n",
      "   8 x\n",
      "\tCJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "\t605981, 606397, 619339, 630957, 654146, 776267, 789543, 797378\n",
      "\n",
      "   117 specific-phrase rule applications\n",
      "   117 specific rule applications\n",
      " 51763 phrase seen 1  time(s)\n",
      "   181 phrase seen 2  time(s)\n",
      "     9 phrase seen 3  time(s)\n",
      " 51953 phrase seen in total\n",
      "  1494 clause seen 1  time(s)\n",
      "     3 clause seen 2  time(s)\n",
      "  1497 clause seen in total\n"
     ]
    }
   ],
   "source": [
    "seen = collections.defaultdict(collections.Counter)\n",
    "\n",
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cn = L.u(wn, 'clause')[0]\n",
    "        if cn in clauses_seen: continue\n",
    "        clauses_seen.add(cn)\n",
    "        vn = L.u(wn, 'verse')[0]\n",
    "        bn = L.u(wn, 'book')[0]\n",
    "        (book_name, chapter, verse) = T.sectionFromNode(cn, lang='la')\n",
    "        book = T.sectionFromNode(cn)[0]\n",
    "        ln = ln_base+(ln_tpl.format(book_name, chapter, verse))+ln_tweak\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.text([wn], fmt='text-trans-plain')\n",
    "        ct = T.text(L.d(cn, 'word'), fmt='text-trans-plain')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, vl, vstem, vt, ct, '')\n",
    "        rows.append(common_fields + (('',)*fillrows))\n",
    "        for pn in L.d(cn, 'phrase'):\n",
    "            seen['phrase'][pn] += 1\n",
    "            pt = T.text(L.d(pn, 'word'), fmt='text-trans-plain')\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vstem, '', pt, 'phrase')\n",
    "            pty = F.typ.v(pn)\n",
    "            pf = pf_corr.get(pn, F.function.v(pn))\n",
    "            phrase_fields =\\\n",
    "                ('',)*pfillrows +\\\n",
    "                (pty, pf) +\\\n",
    "                apply_logic('phrase', vl, pn, transform['phrase'][pf])            \n",
    "            rows.append(common_fields + phrase_fields)\n",
    "        for scn in clause_objects[cn]:\n",
    "            seen['clause'][scn] += 1\n",
    "            sct = T.text(L.d(scn, 'word'), fmt='text-trans-plain')\n",
    "            common_fields = (cn, wn, scn, book, chapter, verse, vl, vstem, '', sct, 'clause')\n",
    "            scty = F.typ.v(scn)\n",
    "            scr = F.rela.v(scn)\n",
    "            clause_fields =\\\n",
    "                (scty, scr) +\\\n",
    "                ('',)*cfillrows +\\\n",
    "                apply_logic('clause', vl, scn, transform['clause'][scr if scr == 'Objc' else scty])       \n",
    "            rows.append(common_fields + clause_fields)\n",
    "\n",
    "    filename = vfile(verb, 'enrich_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    info('Generated enrichment sheet for verb {} ({:>5} rows)'.format(verb, len(rows)), tm=False)\n",
    "    \n",
    "for verb in verbs: gen_sheet_enrich(verb)\n",
    "\n",
    "info('Done')\n",
    "for scope in rule_cases:\n",
    "    totalscope = 0\n",
    "    for kind in rule_cases[scope]:\n",
    "        info('{}-{} rules:'.format(scope, kind), tm=False)\n",
    "        totalkind = 0\n",
    "        for rule_spec in rule_cases[scope][kind]:\n",
    "            cases = rule_cases[scope][kind][rule_spec]\n",
    "            n = len(cases)\n",
    "            totalscope += n\n",
    "            totalkind += n\n",
    "            if scope == 'generic':\n",
    "                info('{:>4} x\\n\\t{}\\n\\t{}\\n'.format(\n",
    "                    n, rule_as_str[scope](*rule_spec), \n",
    "                    ', '.join(str(c) for c in cases[0:10]),\n",
    "                ), tm=False)\n",
    "            else:                \n",
    "                info('{:>4} x\\n\\t{}\\n\\t{}\\n'.format(\n",
    "                    n, rule_as_str[scope](*rule_spec),\n",
    "                    ', '.join(str(c) for c in cases[0:10]),\n",
    "                ), tm=False)\n",
    "        info('{:>6} {}-{} rule applications'.format(totalkind, scope, kind), tm=False)\n",
    "    info('{:>6} {} rule applications'.format(totalscope, scope), tm=False)\n",
    "\n",
    "for kind in seen:\n",
    "    stats = collections.Counter()\n",
    "    for (node, times) in seen[kind].items(): stats[times] += 1\n",
    "    for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "        info('{:>6} {} seen {:<2} time(s)'.format(n, kind, times), tm=False)\n",
    "    info('{:>6} {} seen in total'.format(len(seen[kind]), kind), tm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showcase(n):\n",
    "    otype = F.otype.v(n)\n",
    "    att1 = pf_corr.get(n, F.function.v(n)) if otype == 'phrase' else F.rela.v(n)\n",
    "    att2 = F.typ.v(n)\n",
    "    info('''{} ({}-{}) {}\\n{} {}:{}    {}\\n'''.format(\n",
    "        otype, att1, att2,\n",
    "        T.text(L.d(n, 'word'), fmt='text-trans-plain'),\n",
    "        *T.sectionFromNode(n),\n",
    "        T.text(L.d(L.u(n, 'verse')[0], 'word'), fmt='text-trans-plain'),\n",
    "    ), tm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase (Subj-DPrP) >LH \n",
      "Numbers 26:57    W >LH PQWDJ H LWJ L MCPXTM L GRCWN MCPXT H GRCNJ L QHT MCPXT H QHTJ L MRRJ MCPXT H MRRJ \n",
      "\n",
      "clause (Attr-xQt0) >CR XV>TM \n",
      "Deuteronomy 9:18    W >TNPL L PNJ JHWH K  R>CNH >RB<JM JWM W >RB<JM LJLH LXM L> >KLTJ W MJM L> CTJTJ <L KL XV>TKM >CR XV>TM L <FWT H R< B <JNJ JHWH L HK<JSW \n",
      "\n",
      "clause (NA-XYqt) KL HRG QJN CB<TJM JQM \n",
      "Genesis 4:15    W J>MR LW JHWH LKN KL HRG QJN CB<TJM JQM W JFM JHWH L QJN >WT L BLTJ HKWT >TW KL MY>W \n",
      "\n"
     ]
    }
   ],
   "source": [
    "showcase(654844)\n",
    "showcase(445014)\n",
    "showcase(426954)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb BW>: 2570 occurrences. He locales in Cmpl phrases: 157\n",
      "\t256512, 26119, 26128, 146448, 187921, 197139, 272407, 95258, 184351, 398369, 289827, 201254, 24617, 78898, 401460, 100411, 32830, 100414, 198209, 5699, 200259, 100939, 24654, 141903, 112208, 186961, 24659, 196691, 28765, 34401, 298595, 248932, 132199, 162919, 12403, 5748, 146045, 396928, 153217, 134793, 151177, 188043, 97420, 426121, 257166, 136339, 21657, 162971, 200350, 214688, 24741, 257193, 158379, 100528, 25778, 160435, 214708, 4790, 4794, 272570, 139964, 90813, 249021, 38596, 113862, 138449, 8921, 282842, 19167, 20704, 26851, 43236, 145128, 8425, 8938, 170730, 397033, 254704, 154355, 200949, 426231, 176377, 79610, 165627, 206076, 208637, 27392, 269570, 106247, 157448, 26381, 149786, 170783, 211233, 126759, 26415, 27439, 246063, 109364, 172341, 249141, 398135, 64829, 26432, 16705, 4930, 168772, 154965, 132956, 393570, 47461, 157542, 47467, 100207, 37233, 269171, 23416, 411000, 23934, 24449, 78209, 133519, 26000, 191382, 12699, 19356, 24477, 170910, 18345, 157609, 267690, 244661, 256953, 8634, 63420, 167360, 175554, 138695, 110537, 175562, 108491, 111052, 143821, 37325, 192974, 264138, 5587, 99796, 11733, 170964, 20439, 218584, 269286, 25063, 110577, 26100, 184316\n",
      "Verb BW>: 2570 occurrences. He locales in Loca phrases: 12\n",
      "\t93572, 90244, 29638, 284966, 289860, 136746, 257294, 289872, 9526, 257017, 284990, 93599\n",
      "Verb BW>: 2570 occurrences. He locales in Adju phrases: 2\n",
      "\t322819, 75703\n"
     ]
    }
   ],
   "source": [
    "def check_h(vl, show_results=False):\n",
    "    hl = {}\n",
    "    total = 0\n",
    "    for w in F.otype.s('word'):\n",
    "        if F.sp.v(w) != 'verb' or F.lex.v(w).rstrip('[=/') != vl: continue\n",
    "        total += 1\n",
    "        c = L.u(w, 'clause')[0]\n",
    "        ps = L.d(c, 'phrase')\n",
    "        phs = {p for p in ps if len({w for w in L.d(p, 'word') if F.uvf.v(w) == 'H'}) > 0}\n",
    "        for f in ('Cmpl', 'Adju', 'Loca'):\n",
    "            phc = {p for p in ps if pf_corr.get(p, None) or (pf_corr.get(p, F.function.v(p))) == f}\n",
    "            if len(phc & phs): hl.setdefault(f, set()).add(w)\n",
    "    for f in hl:\n",
    "        info('Verb {}: {} occurrences. He locales in {} phrases: {}'.format(vl, total, f, len(hl[f])), tm=False)\n",
    "        if show_results: info('\\t{}'.format(', '.join(str(x) for x in hl[f])), tm=False)\n",
    "check_h('BW>', show_results=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be handy to generate an informational spreadsheet that shows all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Process the enrichments\n",
    "\n",
    "We read the enrichments, perform some consistency checks, and produce an annotation package.\n",
    "If the filled-in sheet does not exist, we take the blank sheet, with the default assignment of the new features.\n",
    "If a phrase got conflicting features, because it occurs in sheets for multiple verbs, the values in the filled-in sheet take precedence over the values in the blank sheet. If both occur in a filled in sheet, a warning will be issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_enrich():\n",
    "    of_enriched = {\n",
    "        False: {}, # for enrichments found in blank sheets\n",
    "        True: {}, # for enrichments found in filled sheets\n",
    "    }\n",
    "    repeated = {\n",
    "        False: collections.defaultdict(list), # for blank sheets\n",
    "        True: collections.defaultdict(list), # for filled sheets\n",
    "    }\n",
    "    wrong_value = {\n",
    "        False: collections.defaultdict(list),\n",
    "        True: collections.defaultdict(list),\n",
    "    }\n",
    "\n",
    "    non_match = collections.defaultdict(list)\n",
    "    wrong_node = collections.defaultdict(list)\n",
    "\n",
    "    results = []\n",
    "    dev_results = [] # results that deviate from the filled sheet\n",
    "    \n",
    "    ERR_LIMIT = 10\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        vresults = {\n",
    "            False: {}, # for blank sheets\n",
    "            True: {}, # for filled sheets\n",
    "        }\n",
    "        for check in (\n",
    "            (False, 'blank'), \n",
    "            (True, 'filled'),\n",
    "        ):\n",
    "            is_filled = check[0]\n",
    "            filename = vfile(verb, 'enrich_{}'.format(check[1]))\n",
    "            if not os.path.exists(filename):\n",
    "                error('NO {} enrichments file {}'.format(check[1], filename))\n",
    "                continue\n",
    "            #info('READING {} enrichments file {}'.format(check[1], filename))\n",
    "\n",
    "            with open(filename) as fh:\n",
    "                header = fh.__next__()\n",
    "                for line in fh:\n",
    "                    fields = line.rstrip().split(';')\n",
    "                    on = int(fields[2])\n",
    "                    if on < 0: continue\n",
    "                    kind = fields[10]\n",
    "                    objects_seen[kind][on] += 1\n",
    "                    vvals = tuple(fields[-nef:])\n",
    "                    for (f, v) in zip(enrich_fields, vvals):\n",
    "                        if v != '' and v != 'X' and v != 'NA' and v not in enrich_fields[f]:\n",
    "                            wrong_value[is_filled][on].append((verb, f, v))\n",
    "                    vresults[is_filled][on] = vvals\n",
    "                    if on in of_enriched[is_filled]:\n",
    "                        if on not in repeated[is_filled]:\n",
    "                            repeated[is_filled][on] = [of_enriched[is_filled][on]]\n",
    "                        repeated[is_filled][on].append((verb, vvals))\n",
    "                    else:\n",
    "                        of_enriched[is_filled][on] = (verb, vvals)\n",
    "                    if F.otype.v(on) != kind: \n",
    "                        non_match[on].append((verb, kind))\n",
    "            for on in sorted(vresults[True]):          # check whether the phrase ids are not mangled\n",
    "                if on not in vresults[False]:\n",
    "                    wrong_node[on].append(verb)\n",
    "            for on in sorted(vresults[False]):      # now collect all results, give precedence to filled values\n",
    "                if F.otype.v(on) == 'phrase':\n",
    "                    f_corr = on in pf_corr  # manual correction in phrase function\n",
    "                    f_good = pf_corr.get(on, F.function.v(on)) \n",
    "                else:\n",
    "                    f_corr = ''\n",
    "                    f_good = ''\n",
    "                s_manual = on in vresults[True] and vresults[False][on] != vresults[True][on] # real change\n",
    "\n",
    "                # here we determine which value is going to be put in a feature\n",
    "                # basic rule: if there is an filled-in sheet, take the value from there, else from the blank one\n",
    "                # exception: \n",
    "                # if a value is empty in the filled-in sheet, but not in the blank one, take the non-empty one\n",
    "                #\n",
    "                # Why? Well, sometimes we improve the enrich logic. There may be filled-in sheets based on older\n",
    "                # blank sheets. \n",
    "                # We want to push new values in blank sheets through unfilled in values in the filled sheets.\n",
    "                # If it is intentional to remove a value from the blank sheet, \n",
    "                # you can put an X in the corresponding filled field.\n",
    "                blank_results = vresults[False][on]\n",
    "                these_results = []\n",
    "\n",
    "                for (i, br) in enumerate(blank_results):\n",
    "                    the_value = br\n",
    "                    if s_manual and vresults[True][on][i] != '':\n",
    "                        the_value = vresults[True][on][i]\n",
    "                        if the_value == 'X':\n",
    "                            the_value = ''\n",
    "                    these_results.append(the_value)\n",
    "                these_results = tuple(these_results)\n",
    "                            \n",
    "                # these_results = vresults[True][on] if s_manual else vresults[False][on]\n",
    "                \n",
    "                if f_corr or s_manual:\n",
    "                    dev_results.append((on,)+these_results+(f_good, f_corr, s_manual))\n",
    "                results.append((on,)+these_results+(f_good, f_corr, s_manual))\n",
    "\n",
    "    for check in (\n",
    "        (False, 'blank'), \n",
    "        (True, 'filled'),\n",
    "    ):\n",
    "        if len(wrong_value[check[0]]): #illegal values in sheets\n",
    "            wrongs = wrong_value[check[0]]\n",
    "            for x in sorted(wrongs)[0:ERR_LIMIT]:\n",
    "                px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "                ref_node = L.u(x, 'clause')[0] if F.otype.v(x) != 'clause' else x\n",
    "                cx = T.text(L.d(ref_node, 'word'), fmt='ev')\n",
    "                passage = T.sectionFromNode(x)\n",
    "                error('ERROR: {} Illegal value(s) in {}: {} = {} in {}:'.format(\n",
    "                    passage, check[1], x, px, cx\n",
    "                ), tm=False)\n",
    "                for (verb, f, v) in wrongs[x]:\n",
    "                    error('\\t\"{}\" is an illegal value for \"{}\" in verb {}'.format(\n",
    "                        v, f, verb,\n",
    "                    ), tm=False)\n",
    "            ne = len(wrongs)\n",
    "            if ne > ERR_LIMIT: error('... AND {} CASES MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "        else:\n",
    "            info('OK: The used {} enrichment sheets have legal values'.format(check[1]))\n",
    "\n",
    "        nerrors = 0\n",
    "        if len(repeated[check[0]]): # duplicates in sheets, check consistency\n",
    "            repeats = repeated[check[0]]\n",
    "            for x in sorted(repeats):\n",
    "                overview = collections.defaultdict(list)\n",
    "                for y in repeats[x]: overview[y[1]].append(y[0])\n",
    "                px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "                ref_node = L.u(x, 'clause')[0] if F.otype.v(x) != 'clause' else x\n",
    "                cx = T.text(L.d(ref_node, 'word'), fmt='ev')\n",
    "                passage = T.sectionFromNode(x)\n",
    "                if len(overview) > 1:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        error('ERROR: {} Conflict in {}: {} = {} in {}:'.format(\n",
    "                            passage, check[1], x, px, cx\n",
    "                        ), tm=False)\n",
    "                        for vals in overview:\n",
    "                            error('\\t{:<40} in verb(s) {}'.format(\n",
    "                                ', '.join(vals),\n",
    "                                ', '.join(overview[vals]),\n",
    "                        ), tm=False)\n",
    "                elif False: # for debugging purposes\n",
    "                #else:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        info('{} Agreement in {} {} = {} in {}: {}'.format(\n",
    "                            passage, check[1], x, px, cx, ','.join(list(overview.values())[0]),\n",
    "                        ), tm=False)\n",
    "            ne = nerrors\n",
    "            if ne > ERR_LIMIT: error('... AND {} CASES MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "        if nerrors == 0:\n",
    "            info('OK: The used {} enrichment sheets are consistent'.format(check[1]))\n",
    "\n",
    "    if len(non_match):\n",
    "        error('ERROR: Enrichments have been applied to nodes with non-matching types:')\n",
    "        for x in sorted(non_match)[0:ERR_LIMIT]:\n",
    "            (verb, shouldbe) = non_match[x]\n",
    "            px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "            error('{}: {} Node {} is not a {} but a {}'.format(\n",
    "                verb, T.sectionFromNode(x), x, shouldbe, F.otype.v(x),\n",
    "            ), tm=False)\n",
    "        ne = len(non_phrase)\n",
    "        if ne > ERR_LIMIT: error('... AND {} CASES MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "    else:\n",
    "        info('OK: all enriched nodes where phrase nodes')\n",
    "\n",
    "    if len(wrong_node):\n",
    "        error('ERROR: Node in filled sheet did not occur in blank sheet:')\n",
    "        for x in sorted(wrong_node)[0:ERR_LIMIT]:\n",
    "            px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "            error('{}: {} node {}'.format(\n",
    "                wrong_node[x], T.sectionFromNode(x), x,\n",
    "            ), tm=False)\n",
    "        ne = len(wrong_node)\n",
    "        if ne > ERR_LIMIT: error('... AND {} CASES MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "    else:\n",
    "        info('OK: all enriched nodes occurred in the blank sheet')\n",
    "\n",
    "    if len(dev_results):\n",
    "        info('OK: there are {} manual correction/enrichment annotations'.format(len(dev_results)))\n",
    "        for r in dev_results[0:ERR_LIMIT]:\n",
    "            (x, *vals, f_good, f_corr, s_manual) = r\n",
    "            px = T.text(L.d(x, 'word'), fmt='ev')\n",
    "            cx = T.text(L.d(L.u(x, 'clause')[0], 'word'), fmt='ev')\n",
    "            info('{:<30} {:>7} => {:<3} {:<3} {}\\n\\t{}\\n\\t\\t{}'.format(\n",
    "                'COR' if f_corr else '',\n",
    "                'MAN' if s_manual else'',\n",
    "                '{} {}:{}'.format(*T.sectionFromNode(x)), x, ','.join(vals), px, cx\n",
    "            ), tm=False)\n",
    "        ne = len(dev_results)\n",
    "        if ne > ERR_LIMIT: info('... AND {} ANNOTATIONS MORE'.format(ne - ERR_LIMIT), tm=False)\n",
    "    else:\n",
    "        error('WARNING: there are no manual correction/enrichment annotations')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newFeatures = list(enrich_fields.keys())+['function', 'f_correction', 's_manual']\n",
    "\n",
    "provenance = dict(\n",
    "    title='Correction and enrichment features',\n",
    "    description='Corrections, alternatives and additions to the ETCBC4b encoding of the Hebrew Bible',\n",
    "    purpose='Support the decision process of assigning valence to verbs',\n",
    "    method='Generated blank correction and enrichment spreadsheets with selected clauses',\n",
    "    steps='sheets filled out by researcher; read back in by program; generated new features based on contents',\n",
    "    author='The content and nature of the features are by Janet Dyk, the workflow is by Dirk Roorda',\n",
    ")\n",
    "\n",
    "metaData = {\n",
    "    '': provenance,\n",
    "    'valence': {\n",
    "        'description': 'verbal valence main classification',\n",
    "    },\n",
    "    'predication': {\n",
    "        'description': 'verbal function main classification',\n",
    "    },\n",
    "    'grammatical': {\n",
    "        'description': 'constituent role main classification',\n",
    "    },\n",
    "    'original': {\n",
    "        'description': 'default value before enrichment logic has been applied',\n",
    "    },\n",
    "    'lexical': {\n",
    "        'description': 'additional lexical characteristics',\n",
    "    },\n",
    "    'semantic': {\n",
    "        'description': 'additional semantic characteristics',\n",
    "    },\n",
    "    'f_correction': {\n",
    "        'description': 'whether the phrase function has been manually corrected',\n",
    "    },\n",
    "    's_manual': {\n",
    "        'description': 'whether the generated enrichment features have been manually changed',\n",
    "    },\n",
    "    'function': {\n",
    "        'description': 'corrected phrase function, only present for phrases that were in a correction sheet',\n",
    "    },\n",
    "}\n",
    "\n",
    "for f in newFeatures: metaData[f]['valueType'] = 'str'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    43s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/oBR_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/oFH_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/oLH_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/BRa_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/BWa_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/CJT_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/CWB_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/FJM_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/HLK_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/JRD_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/JYa_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/NFa_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/NPL_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/NTN_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/NWS_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/PQD_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/QRa_etcbc4b.csv\n",
      "    44s NO filled enrichments file /Users/dirk/github/valence/workflow/enrich_filled/SWR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    44s OK: The used blank enrichment sheets have legal values\n",
      "    44s OK: The used blank enrichment sheets are consistent\n",
      "    44s OK: The used filled enrichment sheets have legal values\n",
      "    44s OK: The used filled enrichment sheets are consistent\n",
      "    44s OK: all enriched nodes where phrase nodes\n",
      "    44s OK: all enriched nodes occurred in the blank sheet\n",
      "    44s OK: there are 1211 manual correction/enrichment annotations\n",
      "COR                                    => Genesis 32:24 616138 complement,NA,*,,,\n",
      "\t17912\n",
      "\t\t17912 17913 17914 17915 17916\n",
      "COR                                    => Exodus 13:12 627421 adjunct,NA,NA,,,\n",
      "\t35979\n",
      "\t\t35978 35979 35980 35981 35982 35983 35984\n",
      "COR                                    => Numbers 27:7 654985 adjunct,NA,NA,,,\n",
      "\t87027\n",
      "\t\t87026 87027 87028 87029 87030 87031\n",
      "COR                                    => Numbers 27:8 655000 adjunct,NA,NA,,,\n",
      "\t87047\n",
      "\t\t87046 87047 87048 87049 87050 87051\n",
      "COR                                    => Joshua 4:3 670209 adjunct,NA,NA,,,\n",
      "\t114436\n",
      "\t\t114435 114436 114437 114438\n",
      "COR                                    => 1_Kings 6:21 709465 ,,,,,\n",
      "\t180496\n",
      "\t\t180496 180497 180498 180499 180500 180501 180502 180503 180504\n",
      "COR                                    => Isaiah 10:28 730052 complement,NA,*,,,\n",
      "\t215998\n",
      "\t\t215998 215999 216000\n",
      "COR                                    => Isaiah 28:15 733150 core,regular,NA,,,\n",
      "\t221065\n",
      "\t\t221064 221065\n",
      "COR                                    => Jeremiah 32:35 753353 adjunct,NA,NA,,,\n",
      "\t252273 252274\n",
      "\t\t252273 252274 252275 252276 252277 252278 252279 252280 252281 252282\n",
      "COR                                    => Ezekiel 47:3 775496 complement,NA,*,,,\n",
      "\t289735\n",
      "\t\t289735 289736 289737 289738 289739 289740 289741\n",
      "... AND 1201 ANNOTATIONS MORE\n"
     ]
    }
   ],
   "source": [
    "objects_seen = collections.defaultdict(collections.Counter)\n",
    "\n",
    "nodeFeatures = dict()\n",
    "\n",
    "for result in read_enrich():\n",
    "    for (fName, fVal) in zip(newFeatures, result[1:]):\n",
    "        fValRep = fVal\n",
    "        if type(fVal) is bool:\n",
    "            fValRep = 'y' if fVal else ''\n",
    "        nodeFeatures.setdefault(fName, {})[result[0]] = fValRep\n",
    "\n",
    "RENAMES = [('function', 'cfunction')]\n",
    "for (oldF, newF) in RENAMES:\n",
    "    for data in (nodeFeatures, metaData):\n",
    "        data[newF] = data[oldF]\n",
    "        del data[oldF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.10\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "111 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "VALENCE = f'tf/{version}'\n",
    "TF = Fabric(locations=['~/github/text-fabric-data-legacy', '~/github/valence'], modules=[ETCBC, VALENCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Exporting 9 node and 0 edge and 0 config features to /Users/dirk/github/valence/tf/4b:\n",
      "   |     0.14s T cfunction            to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.10s T f_correction         to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.12s T grammatical          to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.10s T lexical              to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.09s T original             to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.10s T predication          to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.10s T s_manual             to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.10s T semantic             to /Users/dirk/github/valence/tf/4b\n",
      "   |     0.10s T valence              to /Users/dirk/github/valence/tf/4b\n",
      "  0.97s Exported 9 node features and 0 edge features and 0 config features to /Users/dirk/github/valence/tf/4b\n"
     ]
    }
   ],
   "source": [
    "TF.save(\n",
    "    module=VALENCE,\n",
    "    nodeFeatures=nodeFeatures,\n",
    "    metaData=metaData,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1m 10s 42851  phrases seen 1  time(s)\n",
      " 1m 10s 138    phrases seen 2  time(s)\n",
      " 1m 10s 3      phrases seen 3  time(s)\n",
      " 1m 10s Total phrases seen: 42992\n"
     ]
    }
   ],
   "source": [
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    info('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "info('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 VALENCE data module\n",
    "We load the new and modified features the Text-Fabric.\n",
    "\n",
    "Note that we draw in the new annotations by specifying a module addressed by VALENCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.10\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "120 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations=['~/github/text-fabric-data-legacy', '~/github/valence'], modules=[ETCBC, VALENCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.19s B lex_utf8             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.13s B lex                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.15s B gloss                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B sp                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.13s B vs                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.22s B rela                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.22s B typ                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.08s B function             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.21s T cfunction            from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.12s T s_manual             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.12s T f_correction         from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.21s T valence              from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.21s T predication          from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.21s T grammatical          from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.12s T original             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.14s T lexical              from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.14s T semantic             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.00s Feature overview: 114 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  6.68s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    lex gloss lex_utf8\n",
    "    sp vs lex rela typ\n",
    "    function cfunction s_manual f_correction\n",
    "''' + ' '.join(enrich_fields))\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s M otext                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.02s B otype                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.48s B oslots               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M otext                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.01s B book                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B chapter              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B verse                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.13s B g_cons               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.18s B g_cons_utf8          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B g_lex                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.19s B g_lex_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B g_qere_utf8          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.15s B g_word               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.20s B g_word_utf8          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.12s B lex0                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.19s B lex_utf8             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B qtrailer_utf8        from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.10s B trailer_utf8         from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B __levels__           from otype, oslots\n",
      "   |     0.03s B __order__            from otype, oslots, __levels__\n",
      "   |     0.03s B __rank__             from otype, __order__\n",
      "   |     1.00s B __levUp__            from otype, oslots, __rank__\n",
      "   |     0.77s B __levDown__          from otype, __levUp__, __rank__\n",
      "   |     0.31s B __boundary__         from otype, oslots, __rank__\n",
      "   |     0.01s B __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      "   |     0.13s B lex                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.15s B gloss                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.14s B sp                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.13s B vs                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.22s B rela                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.22s B typ                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.08s B function             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.21s T cfunction            from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.12s T s_manual             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.12s T f_correction         from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.21s T valence              from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.21s T predication          from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.21s T grammatical          from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.12s T original             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.14s T lexical              from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.14s T semantic             from /Users/dirk/github/valence/tf/4b\n",
      "   |     0.00s B book@am              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@ar              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@bn              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@da              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@de              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@el              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@en              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@es              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@fa              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@fr              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@he              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@hi              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@id              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@ja              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@ko              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@la              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@nl              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@pa              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@pt              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@ru              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@sw              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@syc             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@tr              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@ur              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@yo              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s B book@zh              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M dist                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M dist_unit            from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M distributional_parent from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M domain               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M entry                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M entry_heb            from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M entryid              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M freq_lex             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M freq_occ             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M functional_parent    from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_entry              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_entry_heb          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_nme                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_nme_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_pfm                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_pfm_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_prs                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_prs_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_uvf                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_uvf_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_vbe                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_vbe_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_vbs                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M g_vbs_utf8           from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M gn                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M id                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M instruction          from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M is_root              from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M kind                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M label                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M lan                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M language             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M ls                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M monads               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M mother               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M mother_object_type   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M nametype             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M nme                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M nu                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M number               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M number_in_ch         from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M omap@4-4b            from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M otext                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M pargr                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M pdp                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M pfm                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M phono                from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M phono_sep            from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M pos                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M prs                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M ps                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M rank_lex             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M rank_occ             from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M root                 from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M st                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M subpos               from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M tab                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M txt                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M uvf                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M vbe                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M vbs                  from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n",
      "   |     0.00s M vt                   from /Users/dirk/github/text-fabric-data-legacy/hebrew/etcbc4b\n"
     ]
    }
   ],
   "source": [
    "loadLog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple test\n",
    "Take the first 10 phrases and retrieve the corrected and uncorrected function feature.\n",
    "Note that the corrected function feature is only filled in, if it occurs in a clause in which a selected verb occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time - Time - True\n",
      "Pred - Pred - True\n",
      "Subj - Subj - True\n",
      "Objc - Objc - True\n",
      "Conj - None - False\n",
      "Subj - None - False\n",
      "Pred - None - False\n",
      "PreC - None - False\n",
      "Conj - None - False\n",
      "Subj - None - False\n"
     ]
    }
   ],
   "source": [
    "for i in list(F.otype.s('phrase'))[0:10]: \n",
    "    print('{} - {} - {}'.format(\n",
    "        F.function.v(i), \n",
    "        F.cfunction.v(i),\n",
    "        L.u(i, 'clause')[0] in clause_verb,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We put all corrections and enrichments in a single csv file for checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    21s collecting constituents ...\n",
      "    22s  10000 constituents in  2823 clauses ...\n",
      "    22s  20000 constituents in  5707 clauses ...\n",
      "    23s  30000 constituents in  8713 clauses ...\n",
      "    24s  40000 constituents in 11768 clauses ...\n",
      "    25s  50000 constituents in 14914 clauses ...\n",
      "    25s  53450 constituents in 15880 clauses done\n"
     ]
    }
   ],
   "source": [
    "f = open(all_results, 'w')\n",
    "NALLFIELDS = 17\n",
    "tpl = ('{};' * (NALLFIELDS - 1))+'{}\\n'\n",
    "\n",
    "info('collecting constituents ...')\n",
    "f.write(tpl.format(\n",
    "    '-',\n",
    "    '-',\n",
    "    'passage',\n",
    "    'verb(s) text',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    'clause text',\n",
    "    'clause node',\n",
    "))\n",
    "f.write(tpl.format(\n",
    "    'corrected',\n",
    "    'enriched',\n",
    "    'passage',\n",
    "    '-',\n",
    "    'object type',\n",
    "    'clause rela',\n",
    "    'clause type',\n",
    "    'phrase function (old)',\n",
    "    'phrase function (new)',\n",
    "    'phrase type',\n",
    "    'valence',\n",
    "    'predication',\n",
    "    'grammatical',\n",
    "    'original',\n",
    "    'lexical',\n",
    "    'semantic',\n",
    "    'object text',\n",
    "    'object node',\n",
    "))\n",
    "i = 0\n",
    "j = 0\n",
    "c = 0\n",
    "CHUNK_SIZE = 10000\n",
    "for cn in sorted(clause_verb):\n",
    "    c += 1\n",
    "    vrbs = sorted(clause_verb[cn])\n",
    "    f.write(tpl.format(\n",
    "        '',\n",
    "        '',\n",
    "        '{} {}:{}'.format(*T.sectionFromNode(cn)),\n",
    "        ' '.join(F.lex.v(verb) for verb in vrbs),\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        T.text(L.d(cn, 'word'), fmt='text-trans-plain'),\n",
    "        cn,\n",
    "    ))\n",
    "    for pn in L.d(cn, 'phrase'):\n",
    "        i += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            info('{:>6} constituents in {:>5} clauses ...'.format(i, c))\n",
    "        f.write(tpl.format(\n",
    "            'COR' if F.f_correction.v(pn) == 'y' else '',\n",
    "            'MAN' if F.s_manual.v(pn) == 'y' else '',\n",
    "            '{} {}:{}'.format(*T.sectionFromNode(pn)),\n",
    "            '',\n",
    "            'phrase',\n",
    "            '',\n",
    "            '',\n",
    "            F.function.v(pn),\n",
    "            F.cfunction.v(pn),\n",
    "            F.typ.v(pn),\n",
    "            F.valence.v(pn),\n",
    "            F.predication.v(pn),\n",
    "            F.grammatical.v(pn),\n",
    "            F.original.v(pn),\n",
    "            F.lexical.v(pn),\n",
    "            F.semantic.v(pn),\n",
    "            T.text(L.d(pn, 'word'), fmt='text-trans-plain'),\n",
    "            pn,\n",
    "        ))\n",
    "    for scn in clause_objects[cn]:\n",
    "        i += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            info('{:>6} constituents in {:>5} clauses ...'.format(i, c))\n",
    "        f.write(tpl.format(\n",
    "            '',\n",
    "            '',\n",
    "            '{} {}:{}'.format(*T.sectionFromNode(scn)),\n",
    "            '',\n",
    "            'clause',\n",
    "            F.rela.v(scn),\n",
    "            F.typ.v(scn),\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            F.valence.v(scn),\n",
    "            F.predication.v(scn),\n",
    "            F.grammatical.v(scn),\n",
    "            F.original.v(scn),\n",
    "            F.lexical.v(scn),\n",
    "            F.semantic.v(scn),\n",
    "            T.text(L.d(scn, 'word'), fmt='text-trans-plain'),\n",
    "            scn,\n",
    "        ))\n",
    "\n",
    "f.close()\n",
    "info('{:>6} constituents in {:>5} clauses done'.format(i, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cmpl\n",
      "True\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x  = 671522\n",
    "print(pf_corr.get(x, F.function.v(x)))\n",
    "print(is_lex_local('FJM',x))\n",
    "print(x in rule_cases['specific']['phrase'][('FJM', 2)])\n",
    "print(F.lexical.v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
